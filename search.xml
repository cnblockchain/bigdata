<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kafka在zk中的信息详解]]></title>
    <url>%2F2017%2F08%2F30%2FKafka%E5%9C%A8zk%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[问题： Kafka的哪些信息记录在Zookeeper中 Consumer Group消费的每个Partition的Offset信息存放在什么位置 Topic的每个Partition存放在哪个Broker上的信息存放在哪里 Producer跟Zookeeper究竟有没有关系？]]></content>
      <categories>
        <category>Kafka</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka宕机演练]]></title>
    <url>%2F2017%2F08%2F30%2FKafka%E5%AE%95%E6%9C%BA%E6%BC%94%E7%BB%83%2F</url>
    <content type="text"><![CDATA[备注：kafka全部停机的情况下，故障处理须严格按照以下步骤在hcontrol界面停止kafka，模拟故障过程1.在stdp节点停止全部的sdtp服务，列表如下： IP 地址 监测端口 用途 10.11.94.23 9100/9200/9300/9400/9500/9600 2/3G数据接收 10.11.94.31 9100/9200/9300/9400/9500/9600 LTE数据接收 10.11.94.39 9100/9200/9300/9400/9500/9600 LTE数据接收 10.11.94.46 9100/9200/9300/9400/9500/9600 LTE数据接收 10.11.94.53 9100/9200/9300/9400/9500/9600 2/3G数据接收 10.11.94.61 9100/9200/9300/9400/9500/9600 LTE数据接收 登录sdtp服务器执行如下命令停止sdtp：1supervisorctl stop all 2.在hcontrol界面启动kafka3.选举kafka leader节点123export JAVA_HOME=/usr/jdk64/jdk1.7.0_67cd /cmss/bch/bc1.3.4/kafka/./kafka-preferred-replica-election.sh --zookeeper 10.11.94.38:2181/kafka1 &gt; 1.txt 4.检查kafka采集源有没有恢复分别执行如下命令，看看有没有数据输出1234./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic r_gn_http | more./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic r_lte_s1_mme | more./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic nokia_wsnotify1 | more./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic esb_wsnotify --from-beginnin |more 5.重启启动crossdata并确认ws+mq的拓扑crossdata部署在10.11.94.195与10.11.94.188两个节点上，我们需要先停止10.11.94.188节点上停止crossdata123su - crossdata cd scheduler-1.3.0-SNAPSHOT-dist/bin ./stop_scheduler.sh 10.11.94.195节点，重启crossdata1234su - crossdata cd scheduler-1.3.0-SNAPSHOT-dist/bin ./stop_scheduler.sh./start_scheduler.sh 出现这样的如图所示字样代表启动成功然后需要逐一的检查每个拓扑的状态，确保全部无异常报错，拓扑如下表格 MQ+FTP WS+FTP cmnet_asia nokia_webService cmnet_haohan MRO_common 3rd_all aaa_all data_greenet 检查方法见运维手册 6.检查storm的所有的拓扑，确保所有的拓扑都有数据，如果拓扑有问题，重新提交该拓扑 东信 亚信 csfb-topology BUSI_ENGINE drpc-topology LOCATION_INFORMATION ott-topology ott-topology volte-delay-topology volte-topology 7.启动停止的stdp服务登录sdtp服务器执行如下命令停止sdtp：1supervisorctl start all]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka基本概念]]></title>
    <url>%2F2017%2F08%2F30%2FKafka%E5%9F%BA%E6%9C%AC%E8%AF%A5%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[简介Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消费。Kafka作为一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 支持通过Kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。 Kafka系统角色如图所示，kafka是由如下的组件所组成： Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。一个Broker上可以有一个Topic的多个Partition，每个Partition的Lead随机存在于某一个Broker，这样实现了Topic的读写的负载均衡 topic： 可以理解为一个MQ消息队列的名字 Partition：为了实现扩展性，一个非常大的topic可以分布到多个 broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息 都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体 （多个partition间）的顺序。也就是说，一个topic在集群中可以有多个partition，那么分区的策略是什么？(消息发送到哪个分区上，有两种基本的策略，一是采用Key Hash算法，一是采用Round Robin算法) Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。 Producer ：消息生产者，就是向kafka broker发消息的客户端（Push）。 Consumer ：消息消费者，向kafka broker取消息的客户端(Pull) Consumer Group(CG）：消息系统有两类，一是广播，二是订阅发布。广播是把消息发送给所有的消费者；发布订阅是把消息只发送给一个订阅者。Kafka通过Consumer Group组合实现了这两种机制： 实现一个topic消息广播（发给所有的consumer）和单播（发给任意一个consumer）。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还 可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。典型的应用场景是，多个Consumer来读取一个Topic(理想情况下是一个Consumer读取Topic的一个Partition）,那么可以让这些Consumer属于同一个Consumer Group即可实现消息的多Consumer并行处理，原理是Kafka将一个消息发布出去后，ConsumerGroup中的Consumers可以通过Round Robin的方式进行消费(Consumers之间的负载均衡使用Zookeeper来实现) 消息多播的实现：为一个Topic指定多个Consumer Group，每个Consumer Group指定一个Consumer，那么由于消息会发送给所有的Consumer Group，那么所有的Consumer都会消费这个消息 消息单播的实现：为一个Topic指定一个Consumer Group，这个Consumer Group指定多个Consumer，那么由于消息发送给这个Consumer Group时只有一个Consumer消费，这就实现了一个消息只被一个Consumer消费的效果 #总结：Topic、Partition和Replica的关系：]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Yarn-cluster与Yarn-client]]></title>
    <url>%2F2017%2F08%2F29%2FSpark-Yarn-cluster%E4%B8%8EYarn-client%2F</url>
    <content type="text"><![CDATA[摘要在Spark中，有Yarn-Client和Yarn-Cluster两种模式可以运行在Yarn上，通常Yarn-cluster适用于生产环境，而Yarn-client更适用于交互，调试模式，以下是它们的区别 Spark插拨式资源管理Spark支持Yarn,Mesos,Standalone三种集群部署模式，它们的共同点：Master服务(Yarn ResourceManager,Mesos master,Spark standalone)来决定哪些应用可以运行以及在哪什么时候运行，Slave服务(Yarn NodeManger)运行在每个节点上，节点上实际运行着Executor进程，此外还监控着它们的运行状态以及资源的消耗 Spark On Yarn的优势 Spark支持资源动态共享，运行于Yarn的框架都共享一个集中配置好的资源池 可以很方便的利用Yarn的资源调度特性来做分类·，隔离以及优先级控制负载，拥有更灵活的调度策略 Yarn可以自由地选择executor数量 Yarn是唯一支持Spark安全的集群管理器，使用Yarn，Spark可以运行于Kerberized Hadoop之上，在它们进程之间进行安全认证 Yarn-cluster VS Yarn-client当在Spark On Yarn模式下，每个Spark Executor作为一个Yarn container在运行，同时支持多个任务在同一个container中运行，极大地节省了任务的启动时间 Appliaction Master为了更好的理解这两种模式的区别先了解下Yarn的Application Master概念，在Yarn中，每个application都有一个Application Master进程，它是Appliaction启动的第一个容器，它负责从ResourceManager中申请资源，分配资源，同时通知NodeManager来为Application启动container，Application Master避免了需要一个活动的client来维持，启动Applicatin的client可以随时退出，而由Yarn管理的进程继续在集群中运行 Yarn-cluster在Yarn-cluster模式下，driver运行在Appliaction Master上，Appliaction Master进程同时负责驱动Application和从Yarn中申请资源，该进程运行在Yarn container内，所以启动Application Master的client可以立即关闭而不必持续到Application的生命周期，下图是yarn-cluster模式 Yarn-cluster模式下作业执行流程：客户端 根据yarnConf来初始化yarnClient，并启动yarnClient 创建客户端Application，并获取Application的ID，进一步判断集群中的资源是否满足executor和ApplicationMaster申请的资源，如果不满足则抛出IllegalArgumentException； 设置资源、环境变量：其中包括了设置Application的Staging目录、准备本地资源（jar文件、log4j.properties）、设置Application其中的环境变量、创建Container启动的Context等； 设置Application提交的Context，包括设置应用的名字、队列、AM的申请的Container、标记该作业的类型为Spark； 申请Memory，并最终通过yarnClient.submitApplication向ResourceManager提交该Application。当作业提交到YARN上之后，客户端就没事了，甚至在终端关掉那个进程也没事，因为整个作业运行在YARN集群上进行，运行的结果将会保存到HDFS或者日志中。 YARN集群 运行ApplicationMaster的run方法； 设置好相关的环境变量。 创建amClient，并启动； 在Spark UI启动之前设置Spark UI的AmIpFilter； 在startUserClass函数专门启动了一个线程（名称为Driver的线程）来启动用户提交的Application，也就是启动了Driver。在Driver中将会初始化SparkContext； 等待SparkContext初始化完成，最多等待spark.yarn.applicationMaster.waitTries次数（默认为10），如果等待了的次数超过了配置的，程序将会退出；否则用SparkContext初始化yarnAllocator； 当SparkContext、Driver初始化完成的时候，通过amClient向ResourceManager注册ApplicationMaster 分配并启动Executeors。在启动Executeors之前，先要通过yarnAllocator获取到numExecutors个Container，然后在Container中启动Executeors。如果在启动Executeors的过程中失败的次数达到了maxNumExecutorFailures的次数，那么这个Application将失败，将Application Status标明为FAILED，并将关闭SparkContext。其实，启动Executeors是通过ExecutorRunnable实现的，而ExecutorRunnable内部是启动CoarseGrainedExecutorBackend的。 最后，Task将在CoarseGrainedExecutorBackend里面运行，然后运行状况会通过Akka通知CoarseGrainedScheduler，直到作业运行完成。 Yarn-client在Yarn-client中，Application Master仅仅从Yarn中申请资源给Executor，之后client会跟container通信进行作业的调度，下图是Yarn-client模式 Yarn-client模式下作业执行流程： 客户端生成作业信息提交给ResourceManager(RM) RM在本地NodeManager启动container并将Application Master(AM)分配给该NodeManager(NM) NM接收到RM的分配，启动Application Master并初始化作业，此时这个NM就称为Driver Application向RM申请资源，分配资源同时通知其他NodeManager启动相应的Executor Executor向本地启动的Application Master注册汇报并完成相应的任务 下表是Spark Standalone与Spark On Yarn模式下的比较 从深层次的含义讲，yarn-cluster和yarn-client模式的区别其实就是Application Master进程的区别，yarn-cluster模式下，driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。然而yarn-cluster模式不适合运行交互类型的作业。而yarn-client模式下，Application Master仅仅向YARN请求executor，client会和请求的container通信来调度他们工作，也就是说Client不能离开。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[map 和 flatMap]]></title>
    <url>%2F2017%2F08%2F29%2Fmap-%E5%92%8C-flatMap%2F</url>
    <content type="text"><![CDATA[#基本概念map 和 flatMap 这两兄弟转文负责将 “一个数组转化为另外一个数组”。1let testArray = ["test1","test1234","","test56"] Mapmap函数能够被数组调用，它接受一个闭包作为参数，作用于数组中的每个元素，闭包返回一个变换后的元素，接着将所有这些变换后的元素组成一个新的数组。 12345678let anotherArray = testArray.map &#123; (string:String) -&gt; Int? in let length = string.characters.count guard length &gt; 0 else &#123; return nil &#125; return string.characters.count&#125; print(anotherArray) //[Optional(5), Optional(8), nil, Optional(6)] FlatMapflatMap很像map函数，但是它摒弃了那些值为nil的元素。另外一个与map函数不同之处在于：倘若元素值不为nil情况下，flapMap能够将可选类型(optional)转换为非可选类型(non-optionals)。 123456789let anotherArray2 = testArray.flatMap &#123; (string:String) -&gt; Int? in let length = string.characters.count guard length &gt; 0 else &#123; return nil &#125; return string.characters.count&#125; print(anotherArray2) //[5, 8, 6]]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark概念]]></title>
    <url>%2F2017%2F08%2F28%2FSpark%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[什么是SparkApache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架，Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。 Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。 Spark的架构Spark架构采用了分布式计算中的Master-Slave模型。 Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。 Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行，如图1-4所示Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进行控制。 在一个Spark应用的执行过程中，Driver和Worker是两个重要角色。 Driver程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发，而多个Worker用来管理计算节点和创建Executor并行处理任务。 在执行阶段，Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进行处理。 Spark的整体流程为：Client提交应用，Master找到一个Worker启动Driver，Driver向Master或者资源管理器申请资源，之后将应用转化为RDD Graph，再由DAGScheduler将RDD Graph转化为Stage的有向无环图提交给TaskScheduler，由TaskScheduler提交任务给Executor执行。 在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。 下面详细介绍Spark的架构中的基本组件。 ClusterManager：在Standalone模式中即为Master（主节点），控制整个集群，监控Worker。 在YARN模式中为资源管理器。 Worker：从节点，负责控制计算节点，启动Executor或Driver。 在YARN模式中为NodeManager，负责计算节点的控制。 Driver：运行Application的main（）函数并创建SparkContext。 Executor：执行器，在worker node上执行任务的组件、 用于启动线程池运行任务。 每个Application拥有独立的一组Executors。 SparkContext：整个应用的上下文，控制应用的生命周期。 RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph。 DAG Scheduler：根据作业（Job）构建基于Stage的DAG，并提交Stage给TaskScheduler。 TaskScheduler：将任务（Task）分发给Executor执行。 SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。SparkEnv内创建并包含如下一些重要组件的引用。 MapOutPutTracker：负责Shuffle元信息的存储。 BroadcastManager：负责广播变量的控制与元信息的存储。 BlockManager：负责存储管理、 创建和查找块。 MetricsSystem：监控运行时性能指标信息。 SparkConf：负责存储配置信息。 Spark的运行逻辑如图所示，在Spark应用中，整个执行流程在逻辑上会形成有向无环图（DAG）。Action算子触发之后，将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。 Spark的调度方式与MapReduce有所不同。 Spark根据RDD之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数执行流水线。图中的A、 B、C、 D、 E、 F分别代表不同的RDD，RDD内的方框代表分区。 数据从HDFS输入Spark，形成RDD A和RDD C， RDD A上执行map与flat map操作，转换为RDD B RDD C上执行map操作，转换为RDD D RDD D在执行reduce by key的操作转换为RDD E RDD B和RDD E执行join操作，转换为F，而在B和E连接转化为F的过程中又会执行Shuffle， 最后RDD F通过函数saveAsSequenceFile输出并保存到HDFS中。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop操作手册]]></title>
    <url>%2F2017%2F08%2F21%2Fhadoop%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[HDFS命令用户命令classpath命令说明：打印需要得到Hadoop的jar和所需要的lib包路径。命令使用：hdfs classpath dfs命令说明：在支持Hadoop文件系统上运行文件命令。命令使用：1hdfs dfs [COMMAND [COMMAND_OPTIONS]] appendToFile 描述：把本地文件系统中的单个src或是多个srcs复制到目标文件系统中。并且从stdin中读入并且附加至目标文件系统。使用：1hadoop fs -appendToFile &lt;localsrc&gt;...&lt;dst&gt; 举例：12hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile cat描述：将参数所指示的文件的内容输出到stdout。使用：1hadoop fs -cat URI [URI ...] 举例：1hadoop fs -cat /file1 hdfs://nn2.example.com/file2 2）hadoop fs -cat file:///file3 /user/hadoop/file4 checksum描述：返回文件的校验信息。使用：1hadoop fs -checksum URI 举例：11）hadoop fs -checksum hdfs://nn1.example.com/file1 2）hadoop fs -checksum file:///etc/hosts chgrp描述：改变文件所属的用户组。使用这一命令的用户必须是文件的所属用户，或者是超级用户。使用：1hadoop fs -chgrp [-R] GROUP URI [URI ...] -R 这一操作对整个目录结构递归执行 chmod描述：改变文件的权限。使用这一命令的用户必须是文件的所属用户，或者是超级用户。使用：1hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...] -R，这一操作对整个目录结构递归执行 chown描述：改变文件的所属用户。使用这一命令的用户必须是文件在命令变更之前的所属用户，或者是超级用户。使用：1hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] -R，这一操作对整个目录结构递归执行 copyFromLocal描述：与-put命令相似，但是要限定源文件路径为本地文件系统。使用：1hadoop fs -copyFromLocal &lt;localsrc&gt; URI -f，将覆盖目标，若是此文件已经存在的话。 copyToLocal描述：与-get命令相似，但是要限定目标文件路径为本地文件系统。使用：1hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt; count描述：统计匹配对应路径下的目录数，文件数，字节数（文件大小）。使用： 12345678910hadoop fs -count [-q] [-h] [-v] &lt;paths&gt;``` -count的输出格式为DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME &gt; -q的输出格式为QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME &gt; -h，选项显示可读的格式大小。 &gt; -v，选项显示标题行。 &gt; 举例 1）hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file22）hadoop fs -count -q hdfs://nn1.example.com/file13）hadoop fs -count -q -h hdfs://nn1.example.com/file14）hdfs dfs -count -q -h -v hdfs://nn1.example.com/file1123456### cp描述：文件复制从源到目的地。该命令允许多个来源，但这种情况下，目标必须是一个目录。使用：``` bashhadoop fs -cp [-f] [-p | -p[topax]] URI [URI ...] &lt;dest&gt; -f，将覆盖目标，若是此文件已经存在的话。-p，保留文件属性（timestamps, ownership, permission，ACL，XATTR），如果-p没有arg，则保留（timestamps, ownership, permission) 举例： 12hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir createSnapshot描述：创建一个snapshot。使用： 1hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt; renameSnapshot描述：重命名一个snapshot。使用：1hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt; deleteSnapshot描述：删除一个snapshot。使用：1hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; df描述：显示HDFS可用空间。使用：1hadoop fs -df [-h] URI [URI ...] 1） -h,会将文件大小的数值用方便阅读的形式表示，比如用64.0M 代替67108864。 2）hadoop dfs -df /user/hadoop/dir1du描述：如果参数为目录，显示该目录下所有目录+文件的大小；如果参数为单个文件，则显示文件大小。使用：hadoop fs -du [-s] [-h] URI [URI …]1）-s,指输出所有文件大小的累加和，而不是每个文件的大小。 2）-h,会将文件大小的数值用方便阅读的形式表示，比如用64.0M 代替67108864。举例：1）hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1dus描述：显示文件的大小。这个命令等价于hadoop dfs -du -s。使用：hadoop fs -dus expunge描述：清空回收站。使用：hadoop fs -expungeget描述：将文件或目录从HDFS中的src拷贝到本地文件系统localdst。CRC 校验失败的文件可通过-ignorecrc 选项拷贝。文件和CRC 校验和可以通过-crc 选项拷贝。使用：hadoop fs -get [-ignorecrc] [-crc] 举例：1）hadoop fs -get /user/hadoop/file localfile 2）hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfilegetfacl描述：显示文件和目录的访问控制列表（ACL）。如果一个目录有一个默认的ACL，然后setfacl的也显示默认的ACL。1）-R，列出所有文件及子目录的访问控制列表（ACL）。 2）path，列出文件或目录。使用：hadoop fs -getfacl [-R] 举例：1）hadoop fs -getfacl /file 2）hadoop fs -getfacl -R /dirgetfattr描述：显示一个文件或目录的扩展属性名和值（如果有的话）。使用：hadoop fs -getfattr [-R] -n name | -d [-e en] 1）-R，这一操作对整个目录结构递归执行。 2）–n name,以名字匹配的扩展属性值。 3） -d：以路径匹配。 4）-encode：检索后进行编码，有效的编码为“text”, “hex”和 “base64”，编码为text括在双引号double quotes (“)，编码为“hex”和 “base64”其前缀分别为 0x和0s。 5）path:文件或目录。举例：1）hadoop fs -getfattr -d /file 2）hadoop fs -getfattr -R -n user.myAttr /dirgetmerge描述：取一个源目录和目标文件作为输入，并会连接src的文件到目标本地文件。1）-nl，可以设置为允许在每个文件的末尾添加一个换行符（LF）。使用：hadoop fs -getmerge [-nl] 举例：1）hadoop fs -getmerge -nl /src /opt/output.txt 2）hadoop fs -getmerge -nl /src/file1.txt /src/file2.txt /output.txthelp描述：返回命令的用法介绍。使用：hadoop fs -helpls描述：类似于Linux的ls命令。1）对于一个文件，返回文件状态以如下格式列出：文件权限，副本数，用户ID，组ID，文件大小，最近修改日期，最近修改时间，文件名 2）对于目录，返回目录下的第一层子目录和文件，与Unix 中ls 命令的结果类似；结果以如下状态列出：文件权限，用户ID，组ID，最近修改日期，最近修改时间，文件名。使用：hadoop fs -ls [-d] [-h] [-R] 1） -R，这一操作对整个目录结构递归执行。 2） -d：目录被列为纯文本文件。 3） -h,会将文件大小的数值用方便阅读的形式表示，比如用64.0M 代替67108864。举例：1）hadoop fs -ls /user/hadoop/file1lsr描述：递归版本的ls。类似于命令hadoop fs -ls -R。使用：hadoop fs -lsr mkdir描述：需要的URI路径作为参数，并创建目录。使用：hadoop fs -mkdir [-p] 1）-p, 该命令的行为与Unix 中mkdir -p 的行为十分相似。这一路径上的父目录如果不存在，则创建该父目录。举例：1）hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 2）hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dirmoveFromLocal描述：与put 命令类似，将本地文件或目录localsrc上传到HDFS中的dst路径。但是源文件localsrc 拷贝之后自身被删除。使用：hadoop fs -moveFromLocal moveToLocal描述：“尚未实现”。使用：hadoop fs -moveToLocal [-crc] mv描述：将文件从源路径移动到目标路径（移动之后源文件删除）。目标路径为目录的情况下，源路径可以有多个。跨文件系统的移动（本地到HDFS 或者反过来）是不允许的。使用：hadoop fs -mv URI [URI …] 举例：1）hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 2）hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1put描述：将单个src或是多个srcs从本地文件系统复制到目标文件系统。从stdin中读取输入，然后写入目标文件系统。使用：hadoop fs -put … 举例：1）hadoop fs -put localfile /user/hadoop/hadoopfile 2）hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir 3）hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile 4）hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.rm描述：删除指定的文件，只删除非空目录和文件。使用：hadoop fs -rm [-f] [-r |-R] [-skipTrash] URI [URI …]1）-f，如果这个文件不存在将不会显示诊断信息或修改退出状态以反映一个错误。 2）-R，此操作对整个目录结构递归执行。-r选项等同于-R。 3）-skipTrash，删除指定文件而不是放入垃圾回收站。举例：1）hadoop fs -rm hdfs://nn.example.com/file /user/hadoop/emptydirrmdir描述：删除一个目录。使用：hadoop fs -rmdir [–ignore-fail-on-non-empty] URI [URI …]–ignore-fail-on-non-empty，即使用通配符时，如果目录仍包含的文件不会失败。举例：1）hadoop fs -rmdir /user/hadoop/emptydirrmr描述：递归版本的删除。与命令hadoop fs -rm -r类似。使用：hadoop fs -rmr [-skipTrash] URI [URI …]setfacl描述：设置文件和目录的访问控制列表（ACL）。使用：hadoop fs -setfacl [-R] [-b |-k -m |-x ] |[–set ]1）-b,基本ACL条目全部删除。对于用户，组和其他的条目将保留与权限位的兼容性。 2）-k, 删除默认的ACL。 3）-R,此操作对整个目录结构递归执行。 4）-m，修改ACL。新条目将添加到ACL，并且现有条目将被保留。 5）-x，删除指定的ACL条目。其他ACL条目将被保留。 6）–set, 完全取代ACL，丢弃所有现有条目。该acl_spec必须包括用户，组条目，以及其他与权限位的兼容性。 7）acl_spec，逗号分隔的ACL条目列表。 8）path，文件或目录进行修改。举例：1）hadoop fs -setfacl -m user:hadoop:rw- /file 2）hadoop fs -setfacl -x user:hadoop /file 3）hadoop fs -setfacl -b /file 4）hadoop fs -setfacl -k /dir 5）hadoop fs -setfacl –set user::rw-,user:hadoop:rw-,group::r–,other::r– /file 6）hadoop fs -setfacl -R -m user:hadoop:r-x /dir 7）hadoop fs -setfacl -m default:user:hadoop:r-x /dirsetfattr描述：设置一个文件或目录的扩展属性名和值。使用：hadoop fs -setfattr -n name [-v value] | -x name 1）-b，基本ACL条目全部删除。对于用户，组和其他的条目将保留与权限位的兼容性。 2）-n name,扩展属性名称。 3）-v value：扩展属性值。该值有三种不同的编码方法。如果参数双引号括起来，则该值是引号内的字符串；如果参数是0x或0X前缀，那么它被当作一个十六进制数；如果参数是0或0s开头，那么它被视为一个base64编码。 4）-x name,删除扩展属性。 5）path,文件或目录。举例：1）hadoop fs -setfattr -n user.myAttr -v myValue /file 2）hadoop fs -setfattr -n user.noValue /file 3）hadoop fs -setfattr -x user.myAttr /filesetrep描述：更改文件的副本系数。如果路径为一个目录，则命令递归的修改目录树下的所有文件的副本系数。使用：hadoop fs -setrep [-R] [-w] 1）–w,标志请求命令等待复制完成。这可能潜在地需要很长的时间。 2）-R,标志被接受的向后兼容性。它没有任何效果。举例：hadoop fs -setrep -w 3 /user/hadoop/dir1stat描述：用于返回指定路径的统计信息。使用：hadoop fs -stat [format] …举例：hadoop fs -stat “%F %u:%g %b %y %n” /filetail描述：将文件尾部1KB的内容输出到stdout。使用：hadoop fs -tail [-f] URI-f,随着文件的增长，输出附加数据，和UNIX中一致。举例：hadoop fs -tail pathnametest描述：检查命令，可以检查文件是否存在、文件的大小等。使用：hadoop fs -test -[defsz] URI1）-e，检查文件是否存在，如果存在则返回0。 2）-z，检查文件是否是0字节，如果是则返回0。 3）-d，如果路径是一个目录，则返回1，否则返回0。 4）-f，如果路径是一个文件，返回0。 5）-s，如果路径不为空，则返回0。举例：hadoop fs -test -e filenametext描述：取一个源文件，并把源文件按照text的格式输出。允许的格式为zip和TextRecordInputStream。举例：hadoop fs -text touchz描述：创建0长度的文件。使用：hadoop fs -touchz URI [URI …]举例：hadoop fs -touchz pathnametruncate描述：截断匹配指定的文件模式到指定长度的所有文件。使用：hadoop fs -truncate [-w] 1）-w,标志请求块恢复命令等待完成。举例：1）hadoop fs -truncate 55 /user/hadoop/file1 /user/hadoop/file2 2）hadoop fs -truncate -w 127 hdfs://nn1.example.com/user/hadoop/file1usage描述：返回单个命令的帮助。使用：hadoop fs -usage command(3) fetchdt命令说明：从NameNode获取授权令牌。命令使用：hdfs fetchdt [–webservice ] 参数说明：]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume操作手册]]></title>
    <url>%2F2017%2F08%2F21%2Fflume%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[Flume操作指南主要从设置和配置两个方面说明，若要看Flume各个组件的更详细的配置，可参照Flume官网。 http://flume.apache.org/FlumeUserGuide.html 设置设置一个AgentFlume Agent配置存在本地的配置文件中。它是一个Java Properties文件格式的文本文件。一个配置文件中可以配置一个或多个Agent。配置文件包含了一个Agent中每一个Source、Sink和Channel的属性，以及它们如何连接起来形成数据流。 配置各自的组件每一个组件（Source、Sink和Channel）都有name、type以及跟特性type相关联的其他属性集。例如，一个Avro Source需要一个hostname或者是IP地址以及一个端口号去接收数据。一个Memory Channel的最大队列数（“capacity”），一个HDFS Sink需要知道文件系统的URI，创建文件的路径以及文件循环的频率（“hdfs.rollInternal”）等等。一个组件的所有这些属性都需要被设置在这个Flume Agent所以来的配置文件中。 把各个组件串起来Flume Agent需要知道加载什么样的组件，以及如何将这些组件按照循序串起来形成数据流。这就需要列出Agent中每一个Source、Sink和Channel的名字，然后为每一个Source和Sink指定连接的Channel。例如，一个Agent从一个叫avroWeb的Avro Source通过一个叫做file-channel的File Channel传输Event到一个叫hdfs-cluster1的HDFS Sink中。这个配置文件就需要包含这些组件的名字，并且file-channel作为avroWeb和hdfs-cluster1共享的Channel。 启动一个AgentAgent通过一个叫做flume-ng的shell脚本启动，flume-ng位于Flume分发包的bin目录下。你需要在命令行指定Agent的名字、配置文件目录以及对应的配置文件：1$ bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template 一个简单的例子， 这里我们举一个配置文件例子，描述单节点Flume部署。这个配置文件用户生成Event并且把他们顺序地打印到console上。123456789101112# example.conf: A single-node Flume configuration # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 这个配置文件定义了一个单独的叫做a1的Agent。a1有一个在44444端口上监听数据的Source，一个把Event暂存到Memory的Channel以及一个把Event数据打印到console的Sink。这个配置文件命名了各种各样的组件，然后描述了他们的类型和配置参数。一个给定的配置文件可能定义了多个命名的Agent；当一个给定的Flume进程要被启动时，一个标志会传进去来告诉它到底哪个命名的Agent要被启动。基于这个配置文件，我们可以通过以下方式启动Flume：1$ bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console 注意在一个完整的部署中，我们通常会多包含一个配置：–conf=。目录包含一个叫做flume-env.sh的shell脚本和一个log4j配置文件。这个例子中我们传入了一个Java选项强制Flume把日志记录在console上，并且不使用定制的环境脚本。从另一个shell终端，我们可以telnet 44444端口然后发送一个Event：1$ telnet localhost 44444 Trying 127.0.0.1... Connected to localhost.localdomain (127.0.0.1). Escape character is '^]'. Hello world! &lt;ENTER&gt; OK 之前的Flume启动终端上就会用日志形式输出传入的Event。116/06/19 15:32:19 INFO source.NetcatSource: Source starting 16/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444] 16/06/19 15:32:34 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D Hello world!. &#125; 祝贺你，你已经成功配置并部署了一个Flume Agent！下边的文章更详细的讲述了Agent 的配置。 数据采集Flume支持很多从不同数据源采集数据的机制 RPCFlume分发包中的一个Avro客户端可以通过Avro RPC机制将一个给定的文件发送给Flume Avro Source。1$ bin/flume-ng avro-client -H localhost -p 41414 -F /usr/logs/log.10 上边的命令会发送/usr/logs/log.10的内容发送到监听那个端口的Flume Source上。 执行命令Flume分发包中有一个exec Source可一个执行一个给定的命令，并且消费的输出。一个输出的一行。例如以’\r’或’\n’或两者结尾的文本。注：Flume不支持tail作为一个Source。不过你可以封装tail命令到一个exec Source中从而流化文件。 网络流Flume支持一下的机制去从常用的日志流中读取数据，例如: Avro Thrift Syslog Netcat 配置多Agent流为了使数据跨多个Agent和hop传输，前一个Agent的Sink和当前Agent的Source需要时Avro类型的，并且Sink指向Source的hostname和port。 合并日志收集中一个非常常见的场景是大量的日志生产客户端向少量的和存储系统关联的消费端Agent发送数据。例如，从成百上千个Web服务器收集日志然后发送到十几个写入HDFS集群的Agent。这可以通过配置多个第一层的带有Avro Sink的Agent，并且所Avro Sink都指向一个单的Agent的Avro Source上。第二层Agent的Source合并接收过来的数据到单独的Channel中，然后由Sink消费并发送到目的地。 拆分流（多路输出）Flume支持多路输出Event到一个或多个目的地。可以通过定义一个流多路输出器，它来复制或选择性路由一个Event到一个或多个Channel中。上边的例子显示了一个叫“foo”的Agent的一个Source扇出数据流到三个不同的Channel中。这个扇出可以是复制或者多路输出。在复制流的情况下，每一个Event被发送到三个Channel中。在多路输出的情况下，一个Event根据它的属性和预定义值的匹配情况被传送到可用Channel一个子集中，例如，如果一个Event的属性叫做txnType，并且被设置为“customer”，然后它应该被传送到channel1和channel3中，如果被设置为“vendor”，它应该被传送到channel2中，否则channel3中。这些映射都可以在配置文件中进行配置。 配置前边的文章已经介绍过了，Flume Agent配置是从一个具有分层属性的Java属性文件格式的文件中读取的。 定义数据流要在一个Flume Agent中定义数据流，你需要通过一个Channel将Source和Sink连接起来。你需要列出给定Agent的Source、Sink和Channel。一个Source可以指定多个Channel，但是一个Sink只能指定一个Channel。格式如下：1234# list the sources, sinks and channels for the agent &lt;Agent&gt;.sources = &lt;Source&gt; &lt;Agent&gt;.sinks = &lt;Sink&gt; &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; # set channel for source &lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ... # set channel for sink &lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt; 例如，一个叫做agent_foo的Agent从一个外部的Avro客户端读取数据，然后通过Memory Channel将数据发送到HDFS上。配置文件如下：1234# list the sources, sinks and channels for the agent agent_foo.sources = avro-appserver-src-1 agent_foo.sinks = hdfs-sink-1 agent_foo.channels = mem-channel-1 # set channel for source agent_foo.sources.avro-appserver-src-1.channels = mem-channel-1 # set channel for sink agent_foo.sinks.hdfs-sink-1.channel = mem-channel-1 这个配置将会使Event通过一个叫mem-channel-1的Memory Channel从avro-AppSrv-source流向hdfs-Cluster1-sink。当Agent使用此配置文件启动的时候，它就会实例化这个数据流。 配置各自的组件定义数据流之后，你需要设置每一个Source、Sink和Channel的属性。属性位于每个组件类型配置的层次命名空间下。1234# properties for sources &lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt; # properties for channels &lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt; # properties for sinks &lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt; Flume的每一个组件都需要设置“type”属性，以便理解到底需要的是那种组件对象。每一个Source、Sink和Channel类型都有它自己的属性集。这些属性都需要根据需要设置。就像前边的一个通过mem-channel-1的Memory Channel从avro-AppSrv-source流向hdfs-Cluster1-sink的数据流的例子，下边是这些组件配置的例子：123456789agent_foo.sources = avro-AppSrv-source agent_foo.sinks = hdfs-Cluster1-sink agent_foo.channels = mem-channel-1 # set channel for sources, sinks # properties of avro-AppSrv-source agent_foo.sources.avro-AppSrv-source.type = avro agent_foo.sources.avro-AppSrv-source.bind = localhost agent_foo.sources.avro-AppSrv-source.port = 10000 # properties of mem-channel-1 agent_foo.channels.mem-channel-1.type = memory agent_foo.channels.mem-channel-1.capacity = 1000 agent_foo.channels.mem-channel-1.transactionCapacity = 100 # properties of hdfs-Cluster1-sink agent_foo.sinks.hdfs-Cluster1-sink.type = hdfs agent_foo.sinks.hdfs-Cluster1-sink.hdfs.path = hdfs://namenode/flume/webdata 3.一个Agent中添加多个数据流一个简单的Flume Agent可以包含多个独立的数据流配置。你可以在配置中列出多个Source、Sink和Channel。这些组件能够互联起来形成数据流： 1234# list the sources, sinks and channels for the agent &lt;Agent&gt;.sources = &lt;Source1&gt; &lt;Source2&gt; &lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink2&gt; &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; 之后你就可以连接Source和Sink到它们各自的Channel上，从而形成不同的数据流。例如，如果你需要在一个Agent中设置两个数据流，一个从外部Avro客户端流向外部HDFS，另一个从tail的输出流向Avro Sink，下边就是实现这么个数据流的配置：123456# list the sources, sinks and channels in the agent agent_foo.sources = avro-AppSrv-source1 exec-tail-source2 agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2 agent_foo.channels = mem-channel-1 file-channel-2 # flow #1 configuration agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1 # flow #2 configuration agent_foo.sources.exec-tail-source2.channels = file-channel-2 agent_foo.sinks.avro-forward-sink2.channel = file-channel-2 4.配置一个多Agent数据流为设置一个多层的数据流，你需要在第一跳上使用一个Avro/Thrift Sink指向下一跳的Avro/Thrift Source。这就会使第一个Flume Agent传输Event到下一个Flume Agent。例如，如果你在使用Avro客户端周期性的发送文件（每个Event包含一个文件）到本地Flume Agent，然后本地Agent又把Event传输到另一个和最终存储系统挂载的Flume Agent上。看下边的例子Weblog Agent配置：12345# list sources, sinks and channels in the agent agent_foo.sources = avro-AppSrv-source agent_foo.sinks = avro-forward-sink agent_foo.channels = file-channel # define the flow agent_foo.sources.avro-AppSrv-source.channels = file-channel agent_foo.sinks.avro-forward-sink.channel = file-channel # avro sink properties agent_foo.sources.avro-forward-sink.type = avro agent_foo.sources.avro-forward-sink.hostname = 10.1.1.100 agent_foo.sources.avro-forward-sink.port = 10000 # configure other pieces HDFS Agent配置：1234# list sources, sinks and channels in the agent agent_foo.sources = avro-collection-source agent_foo.sinks = hdfs-sink agent_foo.channels = mem-channel # define the flow agent_foo.sources.avro-collection-source.channels = mem-channel agent_foo.sinks.hdfs-sink.channel = mem-channel # avro sink properties agent_foo.sources.avro-collection-source.type = avro agent_foo.sources.avro-collection-source.bind = 10.1.1.100 agent_foo.sources.avro-collection-source.port = 10000 # configure other pieces 这里我们连接Weblog Agent的avro-forward-sink到HDFS Agent的avro-collection-source。这就会使来自外部appserver Source的Event最终存储到HDFS中。 扇出数据流前边的章节已经讨论过，Flume支持从一个Source扇出数据到多个Channel中。有两种扇出模式：复制（replicating）和多路（multiplexing）。在复制流中，Event被发送到所有配置Channel中。在多路流中，Event被发送到一个特定的子集。为扇出数据流，你需要为这个Source指定一个Channel列表以及扇出的策略。这是通过添加一个Channel “selector”（可以是复制或者多路）实现的。然后如果多路的模式，还需进一步指定选择规则。如果不指定“selector”，默认是复制。123456# List the sources, sinks and channels for the agent &lt;Agent&gt;.sources = &lt;Source1&gt; &lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink2&gt; &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; # set list of channels for source (separated by space) &lt;Agent&gt;.sources.&lt;Source1&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; # set channel for sinks &lt;Agent&gt;.sinks.&lt;Sink1&gt;.channel = &lt;Channel1&gt; &lt;Agent&gt;.sinks.&lt;Sink2&gt;.channel = &lt;Channel2&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = replicating 多路选择还有一些其他的配置来使数据流分叉。这就需要指定一个Event属性到一个Channel集合的映射。选择器会检查Event Header上配置的每一个属性。如果它匹配了指定的值，那么这个Event就会被发送到跟这个值映射的所有Channel上。如果没有任何匹配，这个Event就会被发送到发送到默认配置的Channel集合上：12# Mapping for multiplexing selector &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = multiplexing &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.header = &lt;someHeader&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value1&gt; = &lt;Channel1&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value2&gt; = &lt;Channel1&gt; &lt;Channel2&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value3&gt; = &lt;Channel2&gt; #... &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.default = &lt;Channel2&gt; 为每个值的映射的Channel集合允许重叠。下边的例子是一个单一流，它多路输出到两个路径上。名叫agent_foo的Agent有一个单一的Avro Source和两个Channel连接到两个Sink上：123456789# list the sources, sinks and channels in the agent agent_foo.sources = avro-AppSrv-source1 agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2 agent_foo.channels = mem-channel-1 file-channel-2 # set channels for source agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 file-channel-2 # set channel for sinks agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1 agent_foo.sinks.avro-forward-sink2.channel = file-channel-2 # channel selector configuration agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing agent_foo.sources.avro-AppSrv-source1.selector.header = State agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1 agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1 多路选择器检查Event中一个叫做“State”的Header属性。如果它的值是“CA”，这个Event就会被发送到mem-channel-1，如果它是“AZ”，这个Event就会被发送到file-channel-2，或者如果它是“NY”，那么这个Event就会被发送到这两个Channel上。如果“State” Header属性没有设置或者没有匹配上以上3个的任何一个，这个Event就被发送到mem-channel-1上，它是默认的。多路选择器也支持可选的Channel。为了为Header指定可选的Channel，“optional”配置参数需要像下边的方式一样使用：1234567# channel selector configuration agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing agent_foo.sources.avro-AppSrv-source1.selector.header = State agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1 agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.optional.CA = mem-channel-1 file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1 多路选择器将会首先尝试向必需的Channel上写入，即使这些必需的Channel中有任何一个没有成功消费Event，这整个事务将会失败。事务将会在所有的必需的Channel上重试，一旦多有必需的Channel都成功的消费了Event，多路选择器才会尝试向可选的Channel上写入。并且可选的Channel中有任何消费Event失败的，Flume也会简单忽略它并且不会重试。如果一个特殊Header的可选Channel集合和必选Channel集合有重叠的，那么这些Channel就被认为是必选的，那自然在这些Channel的失败会导致所有Channel的重试。例如，上边的例子中，“CA” Header对应的mem-channel-1就被认为是必选的Channel，尽管它同时被标记为必选和可选的，对这个Channel的写入失败将会导致跟这个选择器关联的所有Channel上重试。注意如果一个Header并没有配置任何必选的Channel，那么这个Event将会被写入默认的Channel上，并且将会尝试写入到跟这个Header关联的可选Channel上。如果指定了可选的Channel，而没有指定必选的Channel，依然会导致Event被发送到默认的Channel上。如果没有Channel被指定为默认的并且也没有必选的，选择器会尝试将Event写入到可选的Channel中。这种情况下，任何的失败都会被简单忽略并且不在重试。]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase权限控制]]></title>
    <url>%2F2017%2F08%2F21%2Fhbase%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[工作原理HBase 权限控制（简称ACL）,权限控制可对用户组以及用户进行授权，HBase 权限控制通过 AccessController 协处理器实现。 权限分类HBase 权限分为五种：READ(R),WRITE(R),EXECUTE(X),CREATE(C),ADMIN(A)。 R:READ权限，读取给定范围的数据，如scan,get等 W:WRITE权限，如put，delete等 X:EXECUTE, 如执行协处理器endpoint C:CREATE权限, 可以建表，删除表，BULKLOAD 需要CREATE权限 A：Admin 权限,可以执行集群操作，如Merge，split,balance,snapshot,assign/unassign/offline,move,list_procedures,abort_procedure 需要注意的是：desc/list/flush /split/disable/enable 需要CREATE或者ADMIN权限listnamespace/getnamespace 需要ADMIN权限 权限范围HBase 可对Global，namespace,table,ColumnFamily,Cell 级别进行权限控制。 Global，全局级别，表示所有命名空间，所有表。 Namespace，命名空间级别权限，可对某命名空间进行权限控制。 Table，表级别权限，可对某张表进行权限控制 ColumnFamily，列簇级别权限，可对某个表中的某个列簇进行权限控制 Cell ，Cell 级别权限，可对某个表的某个列簇中的某个Cell进行权限控制。 权限配置配置如下：1234567891011121314151617181920&lt;property&gt;&lt;name&gt;hbase.superuser&lt;/name&gt;&lt;value&gt;hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.security.authorization&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt; 配置好如上属性，重启HBase 生效。注意如上是针对权限的配置，如果存在其它协处理器，多个协处理器之间使用逗号分隔。 hbase权限命令HBase 权限控制命令有grant，revoke，user_permissiongrant 授予权限，revoke 收回权限，user_permission 查看权限 授予权限grant命令用于授予权限grant &lt;user&gt;, &lt;permissions&gt; [, &lt;@namespace&gt; [, &lt;table&gt; [, &lt;column family&gt; [, &lt;column qualifier&gt;]]]用户和用户组授权语法一直，不同的是用户组使用前缀@，namespace和table语法相同，不同的是namespace需要使用前缀@示例：12345hbase&gt; grant 'user1', 'RWXCA' //授予user1用户 READ,WRITE,EXECUTE,CREATE,ADMIN权限,global级别。hbase&gt; grant '@hadoop', 'RWXCA' //授予hadoop用户组READ,WRITE,EXECUTE,CREATE,ADMIN权限,global级别。hbase&gt; grant 'user1', 'RWXCA', '@ns1' //授予命名空间ns1的READ,WRITE,EXECUTE,CREATE,ADMIN权限给user1用户,namespace级别hbase&gt; grant 'user1', 'RW', 't1', 'F' //授予表t1列簇F READ,WRITE权限给user1用户，列簇级别hbase&gt; grant 'user1', 'RW', 'ns1:t1', 'F', 'col1' //授予命名空间ns1下表t1列簇F，column为col1 READ,WRITE权限给user1用户，Cell级别 收回权限revoke 命令用于收回权限，语法与grant 类似，语法如下revoke &lt;user&gt; [, &lt;@namespace&gt; [, &lt;table&gt; [, &lt;column family&gt; [, &lt;column qualifier&gt;]]]]示例：12345hbase&gt; revoke 'user1'hbase&gt; revoke '@hadoop'hbase&gt; revoke 'user1', '@ns1'hbase&gt; revoke 'user1', 't1', 'F', 'col1'hbase&gt; revoke 'user1', 'ns1:t1', 'F', 'col1' 查看权限user_permission 命令用于查看权限，语法如下：user_permission &lt;table&gt;注：namespace请使用@前缀示例：123456hbase&gt; user_permission //查看所有权限列表hbase&gt; user_permission '@ns1' //查看命名空间ns1权限hbase&gt; user_permission 'table1' //查看命名空间table1权限hbase&gt; user_permission 'namespace1:table1' //查看namespace1中的table1权限hbase&gt; user_permission '.*' //支持正则表达是查看权限hbase&gt; user_permission '^[A-C].*' //支持正则表达是查看]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[klist命令]]></title>
    <url>%2F2017%2F08%2F17%2Fklist%2F</url>
    <content type="text"><![CDATA[用途显示 Kerberos 凭证高速缓存或密钥表的内容。 语法klist [[ -c] [ -f] [ -e] [ -s] [ -a] [ -n]] [ -k [ -t] [ -K]] [ name] 描述klist 命令显示 Kerberos 凭证高速缓存或密钥表的内容。 选项 描述 -a 显示所有在凭证高速缓存中的票据，包括过期的票据。如果不指定该标志，不列出过期的票据。仅当列出凭证高速缓存时该标志有效。 -c 列出凭证高速缓存中票据。如果 -c 或 -k 标志都不指定，这个是缺省的。该标志和 -k 标志是互斥的。 -e 显示为会话密钥和票据的加密类型。 -f 用以下缩写显示票据的标志 name 指定凭证高速缓存或密钥表的名称。如果不指定一个文件名则用缺省的凭证高速缓存或密钥表。如果不指定表示高速缓存名称的文件名或密钥表名，klist 显示在缺省凭证高速缓存或密钥表文件的凭证。如果设置了 KRB5CCNAME 环境变量，它的值就用来命名缺省凭证（票据）高速缓存。 -k 列出在密钥表中的条目。该标志和 -c 标志互斥。 -K 为每个密钥表条目显示加密密钥值。仅当列出一个密钥表时该标志有效。 -n 显示数字因特网地址而不是主机名。没有 -n 的缺省情况是显示主机名。该命令与 -a 标志连用。 -s 禁止命令输出但是如果一个有效的票据授权票据（ticket-granting ticket）在凭证高速缓存中被发现，那么设置退出状态为 0。仅当列出凭证高速缓存时该标志有效。 -t 为密钥表条目显示时间戳。仅当列出一个密钥表时该标志有效。 示例 要列出在缺省凭证高速缓存中的所有条目，请输入：bash klist 要列出在 etc/krb5/my_keytab 密钥表中所有条目还有时间戳，请输入:klist -t -k etc/krb5/my_keytab]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
        <tag>klist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kinit命令]]></title>
    <url>%2F2017%2F08%2F17%2Fkinit%2F</url>
    <content type="text"><![CDATA[用途获得或更新 Kerberos 票据授权票据（ticket-granting ticket）。 语法1kinit [ -l lifetime ] [ -r renewable_life ] [ -f ] [ -p ] [ -A ] [ -s start_time ] [ -S target_service ] [ -k [ -t keytab_file ] ] [ -R ] [ -v ] [ -u ] [ -c cachename ] [ principal ] 描述kinit 命令获得或更新 Kerberos 票据授权票据。如果不在命令行上指定一个票据标志，那么使用由在 Kerberos 配置文件（kdc.conf ）中的 [kdcdefault] 和 [realms] 指定的密钥分发中心（KDC）选项.如果不更新一个存在的票据，该命令重新初始化凭证高速缓存并将包含从 KDC 接受的新的票据授权票据。如果不在命令行上指定 Principal 名并且指定 -s 标志，Principal 名从凭证高速缓存中获取。新的凭证高速缓存成为缺省的高速缓存，除非用 -c 标志指定高速缓存的名称。-l、-r 和 -s 标志的票据 Time 值被表达为 ndnhnmns 其中： n 代表一个数字 d 代表天 h 代表小时 m 代表分钟 s 代表秒,必须以这种顺序指定各个部分，但可省略任何部分，例如 4h5m 代表 4 小时 5 分钟，1d2s 代表 1 天 2 秒。 标志描述 选项 描述 -A 指定这个票据包含一个客户机地址的列表。如果不指定这个选项，这个票据将包含本地主机地址。当一个初始票据包含一个地址列表时，它仅可从地址列表中的一个地址中使用。 -c cachename 指定要用的凭证高速缓存的名称。如果该标志没被指定，应用缺省凭证高速缓存。如果 KRB5CCNAME 环境变量被设置，它的值被用来命名缺省票据高速缓存。高速缓存的任何存在的内容可由 kinit 破坏 -f 指定票据是可转发的。为转发票据，该标志必须被指定。 -k 指定从密钥表获得票据主体的密钥。如果不指定该标志，将提示您为票据主体输入密码。 -l lifetime 指定票据结束时间间隔。在到期失效后，不能再用此票据，除非票据被更新。这个间隔缺省时间是 10 小时。 -p 指定这个票据是可代理的。要使票据可代理，该标志必须被指定。principal 指定票据的主体。如果主体不在命令行中指定，那么主体从凭证高速缓存获得。 -r renewable_life 为可更新的票据指定更新时间间隔。在间隔到期后，票据不能被更新。更新时间必须大于结束时间。如果该标志不指定，那么这个票据是不可更新的，尽管如果请求的票据的生命期超出最大票据生命期仍能生成一个可更新的票据。 -R 指定更新一个存在的票据。当更新一个存在的票据时,可能没指定其他标志。 -s start_time 为一个迟后的票据指定一个请求，从 start_time 开始有效。 -S target_service 当得到初始票据时指定一个备用服务名来用。 -t keytab_file 指定密钥表名。如果没指定该标志并且 -k 标志被指定，用缺省的密钥表。-t 标志意味着 -k 标志。 -v 指定在高速缓存中的票据授权票据应被传到 kdc 来确认。如果票据在它的请求的时间范围内，高速缓存用确认过的票据替换。 -u 指定 kinit 命令创建进程的唯一凭证高速缓存文件。如果 kinit 命令成功，那么凭证高速缓存文件名将包含一个唯一编号（进程认证组或 PAG）。在 AIX® V5.3 和更高版本中，将从操作系统服务生成 PAG。KRB5CCNAME 环境变量设置为此凭证高速缓存文件，而 kinit 命令执行新的 shell。 示例 要获得一个生命期为 10 小时五天内可更新的票据授权票据，请输入：kinit -l 10h -r 5d my_principal 要更新一个存在票据，请输入：kinit -R]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
        <tag>kinit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrips]]></title>
    <url>%2F2017%2F08%2F17%2Fscrips%2F</url>
    <content type="text"><![CDATA[1.123456789101112#! /bin/bashDAY=`/bin/date "+%Y%m%d%H" -d "-1 hours"`echo `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/cpu.logls /home/bcstream/monitor-storm/monitor*.log.$DAY* | xargs cat &gt; file.log a=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|head -1` b=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|tail -1` c=`cat file.log |sort -n|wc -l` d=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|awk '&#123;a+=$1&#125;'END'&#123;print a&#125;'`echo "max cpu_usage:"$b &gt;&gt; /root/statistics/cpu.logecho "min cpu_usage:"$a &gt;&gt; /root/statistics/cpu.logecho "average cpu_usage is:" `awk 'BEGIN&#123;printf "%.2f\n", '$d'/'$c'&#125;'` &gt;&gt; /root/statistics/cpu.logrm -rf file.log 2.12345678910111213141516#! /bin/bashDAY=`/bin/date "+%Y%m%d%H" -d "-1 hours"`echo `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/cpu.logls /home/bcstream/monitor-storm/monitor*.log.$DAY* | xargs cat &gt; file.log a=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|head -1` b=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|tail -1` c=`cat file.log |sort -n|wc -l` d=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|awk '&#123;a+=$1&#125;'END'&#123;print a&#125;'`echo "max cpu_usage:"$b &gt;&gt; /root/statistics/cpu.logecho "min cpu_usage:"$a &gt;&gt; /root/statistics/cpu.logecho "average cpu_usage is:" `awk 'BEGIN&#123;printf "%.2f\n", '$d'/'$c'&#125;'` &gt;&gt; /root/statistics/cpu.logrm -rf file.log[root@YP-TYHJ-APOLLO4200-5031 statistics]# cat kafka.sh#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/kafka.logcat /home/bcstream/srv/logs/writer/writer.log.`/bin/date "+%Y-%m-%d-%H"`-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;' &gt;&gt; /root/statistics/kafka.log 3.12345678910111213#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/kafka.logcat /home/bcstream/srv/logs/writer/writer.log.`/bin/date "+%Y-%m-%d-%H"`-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;' &gt;&gt; /root/statistics/kafka.log[root@YP-TYHJ-APOLLO4200-5031 statistics]# cat storm.sh#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/storm.logDAY=`/bin/date "+%Y-%m-%d-%H"`COUNT=`cat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |wc -l`for((i=1;i&lt;=$COUNT;i++)); do cat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |awk -F [\:] '&#123;print $2&#125;' | awk -F [\-] '&#123;print $3&#125;' |sed -n "$i p" &gt;&gt; /root/statistics/storm.logcat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |awk -F [\;] '&#123;print $4&#125;' |sed -n "$i p" &gt;&gt; /root/statistics/storm.logdone 4.12345#!/bin/bashLOG_DIR=/home/bcstream/monitorLOG_RETAIN_DAY=1find $LOG_DIR -type f -mtime +$LOG_RETAIN_DAY -name "*.complete" -exec rm &#123;&#125; \; &gt;/dev/null 5.12345678910#! /bin/bashDAY=$(date +%Y%m%d%H)HOST=$(hostname --fqdn)LOG_DIR=/home/bcstream/monitorif [ ! -d $LOG_DIR ]; then mkdir $LOG_DIRfiLOG=$LOG_DIR/monitor_$HOST.log.$DAYcpuUsage=`sar -C 1 1|grep "Average" | awk '&#123;print $3&#125;'`echo "$(date +"%Y%m%d %H:%M")|$cpuUsage"&gt;&gt; $LOG 6.12345678910111213141516171819#/bin/bashLOG_DIR=/home/bcstream/monitorCOORDINATE_HOST=10.11.94.52COORDINATE_DIR=/home/bcstream/monitor-stormpasswd=Tyhj@1118for file in `ls $LOG_DIR | grep -v complete`do echo $file expect -c " spawn scp $LOG_DIR/$file bcstream@$COORDINATE_HOST:$COORDINATE_DIR expect &#123; \"assword\" &#123;set timeout 300; send \"$passwd\r\";&#125; \"yes/no\" &#123;send \"yes\r\"; exp_continue;&#125; &#125;expect eof"mv $LOG_DIR/$file $LOG_DIR/$file.completedone 7.12345#! /bin/basha=`cat /home/bcstream/srv/logs/writer/writer.log.2017-08-17-14-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;'`b=`cat /home/bcstream/srv/logs/writer/writer.log.2017-08-17-14-10.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;'`c=$(($b-$a))echo "Kafka statistics:"$c]]></content>
      <categories>
        <category>Bash</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Dynamic Resource Allocation 动态资源分配]]></title>
    <url>%2F2017%2F08%2F16%2FSpark-Dynamic-Resource-Allocation%2F</url>
    <content type="text"><![CDATA[http://lxw1234.com/archives/2015/12/593.htm Spark中，所谓资源单位一般指的是executors，和Yarn中的Containers一样，在Spark On Yarn模式下，通常使用–num-executors来指定Application使用的executors数量，而–executor-memory和–executor-cores分别用来指定每个executor所使用的内存和虚拟CPU核数。相信很多朋友至今在提交Spark应用程序时候都使用该方式来指定资源。 假设有这样的场景，如果使用Hive，多个用户同时使用hive-cli做数据开发和分析，只有当用户提交执行了Hive SQL时候，才会向YARN申请资源，执行任务，如果不提交执行，无非就是停留在Hive-cli命令行，也就是个JVM而已，并不会浪费YARN的资源。现在想用Spark-SQL代替Hive来做数据开发和分析，也是多用户同时使用，如果按照之前的方式，以yarn-client模式运行spark-sql命令行，在启动时候指定–num-executors 10，那么每个用户启动时候都使用了10个YARN的资源（Container），这10个资源就会一直被占用着，只有当用户退出spark-sql命令行时才会释放。 spark-sql On Yarn，能不能像Hive一样，执行SQL的时候才去申请资源，不执行的时候就释放掉资源呢，其实从Spark1.2之后，对于On Yarn模式，已经支持动态资源分配（Dynamic Resource Allocation），这样，就可以根据Application的负载（Task情况），动态的增加和减少executors，这种策略非常适合在YARN上使用spark-sql做数据开发和分析，以及将spark-sql作为长服务来使用的场景。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop FairScheduler介绍]]></title>
    <url>%2F2017%2F08%2F15%2FHadoop-FairScheduler%2F</url>
    <content type="text"><![CDATA[翻译自：https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/FairScheduler.html 介绍在一个公司内部的Hadoop Yarn集群，肯定会被多个业务、多个用户同时使用，共享Yarn的资源，如果不做资源的管理与规划，那么整个Yarn的资源很容易被某一个用户提交的Application占满，其它任务只能等待，这种当然很不合理，我们希望每个业务都有属于自己的特定资源来运行MapReduce任务，Hadoop中提供的公平调度器–Fair Scheduler，就可以满足这种需求。Fair Scheduler将整个Yarn的可用资源划分成多个资源池，每个资源池中可以配置最小和最大的可用资源（内存和CPU）、最大可同时运行Application数量、权重、以及可以提交和管理Application的用户等。 FairScheduler是一个资源分配方式，在整个时间线上，所有的applications平均的获取资源。Hadoop NextGen能够调度多种类型的资源。默认情况下，FairScheduler只是对内存资源做公平的调度(分配)。当集群中只有一个application运行时，那么此application占用整个集群资源。当其他的applications提交后，那些释放的资源将会被分配给新的applications，所以每个applicaiton最终都能获取几乎一样多的资源。不像Hadoop默认的Scheduler(CapacityScheduler)，CapacityScheduler将applications以queue的方式组成，它可以让short applications在何时的时间内完成，而不会starving那些长期运行的applications，它也是一个合理的方式在多个用户之间共享集群。最终，Fair共享也可以与application priorities一起工作—–“priorities”作为权重来使用，以决定每个application需要获取资源的量。 Scheduler将applications以queues的方式组织，在这些queues之间公平的共享资源。默认，所有的users共享一个queue，名称为“default”。如果application在请求资源时指定了queue，那么请求将会被提交到指定的queue中；仍然可以通过配置，根据请求中包含的user名称来分配queue。在每个queue内部，调度策略是在运行中的applicaitons之间共享资源。默认是基于内存的公平共享，不过FIFO和multi-resource with Dominant Resource Fairness也能够配置。Queues可以分级来划分资源，配置权重以特定的比例共享集群资源。 FairScheduler允许为queues分配担保性的最小的共享资源量，这对保证某些用户、groups或者applications总能获取充足的资源是有帮助的。当一个queue中包含applications时，它至少能够获取最小量的共享资源，当queue不在需要时，那些过剩的资源将会被拆分给其他的运行中的application。这就让Scheduler在有效利用资源是，保证了queue的capacity。 为了了解queue之间如何共享资源，设想两个用户A和B，各自有自己的queue（图4-4）。A开启了一个job，由于B没有请求，所以它获得了所有的资源。然后在A的job还在运行时，B开启了一个job，一段时间后，每一个job使用了一半的资源，以我们早些时候看到的方式。现在，如果B开启了第二个job，它会同B的其它job一起分享资源，所以B的每一个job占有四分之一个资源，而A继续占有一半的资源。结果是资源的用户之间平分。 FairScheudler在默认情况下允许所有的application运行，但是这也可以通过配置文件来限制每个用户下和每个queue下运行applications的个数。这对限制一个用户一次提交大量applications是有用的，或者通过限制running applications个数来提升性能，因为大量的running applicaiton会导致创建大量的中间数据或者过多的上下文切换。限制applications不会导致随后的提交失败，只是在Scheduler queue中等待，直到先前的application结束。 Hierarchical queuesFairScheduler支持分层的queues。所有的queues继承自“root” queue。有效的资源在root子节点中，以典型的公平调度的方式分布；子节点再将分配给自己的资源以相同的方式分配给自己的子节点。applications只能在queue的叶子节点上调度。可以通过FairScheduler相关的配置文件，Queues可以被指定作为其他queues的子节点。 Queue的名字，以其父节点path作为开始，以“.”作为分割符。比如名称为“queue1”的queue作为root的子节点，那么应该表示为“root.queue1”，名称为“queue2”的queue为“parent1”的子节点，那么应该表示为“root.parent1.queue2”。当指明一个queue时，root部分是可选的，比如“queue1”就是指queue1，而queue2指“parent1.queue2”。 Installation为了使用FairScheduler，首先需要在yarn-site.xml配置： 1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt; 配置定制FairScheduler涉及到2个文件。首先，scheduler有关的选项可以在yarn-site.xml中配置。此外，多数情况，用户需要创建一个“allocation”文件来列举存在的queues和它们相应的weights和capacities。这个“allocation”文件每隔10秒钟加载一次，更新的配置可以更快的生效。 yarn-site.xml中的配置 yarn.scheduler.fair.allocation.file： “allocation”文件的位置，“allocation”文件是一个用来描述queue以及它们的属性的配置文件。这个文件必须为格式严格的xml文件。如果为相对路径，那么将会在classpath下查找此文件(conf目录下)。默认值为“fair-scheduler.xml”。 yarn.scheduler.fair.user-as-default-queue：是否将与allocation有关的username作为默认的queue name，当queue name没有指定的时候。如果设置成false(且没有指定queue name) 或者没有设定，所有的jobs将共享“default” queue。默认值为true。 yarn.scheduler.fair.preemption：是否使用“preemption”(优先权，抢占)，默认为fasle，在此版本中此功能为测试性的。 yarn.scheduler.fair.assignmultiple：是在允许在一个心跳中，发送多个container分配信息。默认值为false。 yarn.scheduler.fair.max.assign：如果assignmultuple为true，那么在一次心跳中，最多发送分配container的个数。默认为-1，无限制。 yarn.scheduler.fair.locality.threshold.node：一个float值，在0~1之间，表示在等待获取满足node-local条件的containers时，最多放弃不满足node-local的container的机会次数，放弃的nodes个数为集群的大小的比例。默认值为-1.0表示不放弃任何调度的机会。 yarn.scheduler.fair.locality.threashod.rack：同上，满足rack-local。 yarn.scheduler.fair.sizebaseweight：是否根据application的大小(job的个数)作为权重。默认为false，如果为true，那么复杂的application将获取更多的资源。 分配文件格式分配文件必须是XML格式。这个格式包含5类元素： 队列元素：表示队列。队列元素可以选择一个可选属性”type”，当设置为’parent’可以将其设置为父队列。这个当我们想创建一个父队列而不配置任何子队列时是有用的。每个队列元素可能包含下列属性： minResources:队列最小资源所有权，是以”X mb,Y vcores”形式。对于单一资源公平策略，vcores值被忽略。如果一个队列的最小共享不满足，则将在同一个父之前的任何其他队列中提供可用的资源。在单一资源公平策略下，如果一个队列的内存使用率低于其最小内存共享，则被认为是不满足的。在主导资源公平下，一个队列如果它的使用占主导地位的资源相对于集群容量是低于其最低份额的，那么其没认为是不满意的。如果在这个情况下，多个队列是不满足的，资源到队列是相关资源实例用和最小值中间最小的比例。注意这是可能的，一个队列低于其最小值，可能不会立即获得其最小值当其提交一个应用程序时，因为已经运行的作业可能会使用到这些资源。 maxResources：队列被允许的最大资源，以”X mb,Y vcores”的形式。对于单一资源的公平策略，Vcores值被忽略。如果其总使用量超过这个限制，一个队列将永远不会被分配到一个容器。 maxRunningApps：限制同时从队列中运行的应用程序数目。 maxAMShare：限制可用于运行应用程序主程序的队列的公平份额的分数。此属性只能用于子队列。例如，如果设置为1.0f，然后AMs在子队列可以达到100%的内存和CPU的公平分享。对-1.0f价值将禁用此功能，amShare不会被检查。默认值是0.5f weight：非比例的于其他队列共享集群，权重默认值是1，并且一个权重是2的队列应得到作为一个队列权重越2倍多的资源。 schedulingPolicy：为任何队列设置调度策略。这个允许的值是”fifo”,”fair”,”drf”或任何org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy.延伸类。默认是”fair”。FIFO：先按照优先级高低调度，如果优先级相同，则按照提交时间先后顺序调度，如果提交时间相同，则按照（队列或者应用程序）名称大小（字符串比较）调度；FAIR：按照内存资源使用量比率调度，即按照used_memory/minShare大小调度（核心思想是按照该调度算法决定调度顺序，但还需考虑一些边界情况）；DRF：借鉴了Mesos中的设计策略，按照主资源公平调度算法进行调度，具体已经在Apache Mesos调度器机制进行了介绍。 aclSubmitApps：可以向队列中提交应用程序的用户和/或组的列表。 aclAdministerApps：可以管理队列的用户和/或组的列表。目前唯一的能管理的动作是杀死一个应用程序。 minSharePreemptionTimeout：从其他队列试图强占容器资源之前，在最小共享下的秒数。如果没设置，队列将继承其父队列的值。 fairSharePreemptionTimeout：从其他队列试图强占容器资源之前，在公平共享阈值下的秒数。如果没设置，队列将继承其父队列的值。 fairSharePreemptionThreshold：队列的公平共享优先权阈值。如果队列等待fairSharePreemptionTimeout没有接受到fairSharePreemptionThreshold* fairShare相应的资源，其可以从其他队列强占容器资源。如果没设置，队列将继承其父队列的值。 用户元素:它代表了管理个人用户行为的设置。它们可以包含一个单一的属性：maxrunningapps，对于特定用户运行应用程序的一个限制。 userMaxAppsDefault元素：对于未指定其他指定的任何用户，设置默认运行应用程序限制。 defaultFairSharePreemption Timeout 元素：对root队列设置公平共享强占超时时间，被root队列中的fairSharePreemptionTimeout 元素重写 defaultMinSharePreemptionTimeout 元素：对root队列设置最小共享强占超时时间，被root队列中defaultMinSharePreemptionTimeout元素重写 defaultFairSharePreemptionThreshold 元素：对root队列设置最小共享强占阈值，被root队列中的defaultFairSharePreemptionThreshold元素重写。 queueMaxAppsDefault 元素：为队列设置默认运行应用程序限制;被每个队列中的maxRunningApps元素重写。 queueMaxAMShareDefault 元素：为队列设置默认AM资源；被每个队列的maxAMShare 元素重写。 defaultQueueSchedulingPolicy 元素：为队列设置默认的调度策略;如果被指定的话，被每个队列中的schedulingPolicy元素重写。默认是fair queuePlacementPolicy元素：其中包含一个规则元素列表，告诉调度如何放置传入的应用程序到队列。规则按照它们被列出的顺序被应用。规则可以获取参数。所有的规则接受”create”参数，这表示规则是够能创建一个新队列。”create”默认是true，如果设置为false并且该规则应该放置应用程序到队列，但是没有在分配文件中配置，我们会继续到下一个规则。最后一个规则必须是一个可能永远不发出继续的规则。有效的规则是： specified：该应用程序配放置到其去修的队列。如果应用程序需求无队列，例如，其指定”default”，我们继续。如果应用程序被需求一个队列名开始或结束是””.””，例如，命名例如”.q1”或”q1.”,将被拒绝。 user:应用程序被放置到名称为提交的用户的队列中。用户名的”.”将被”dot“替换，例如对于用户”first.last”的用户是”first_dot_last” primaryGroup:应用程序被放置到一个名称为提交用户的主组的队列中，组名中的”.”被替换为”dot“，例如组名为”one.two”是”one_dot_two” secondaryGroupExistingQueue: 应用程序被放置到一个名称为提交用户的匹配第二组名的队列中。第一个被匹配的第二组名将被选中。组名中的”.”将被替换为”dot”，例如一个用户的第二组名为”one.two”将被置换为“one_dot_two”，如果这个队列存在的话。 nestedUserQueue：嵌套规则。 default:该应用程序被放置在默认规则的属性queue指定的队列中。如果queue为设置，那么应用程序被放置到’root.default’队列。 reject:应用程序被拒绝 Example一个公平调度的例子：12345678910111213141516171819202122&lt;allocations&gt; &lt;queue name="sample_queue"&gt; &lt;minResources&gt;10000 mb,0vcores&lt;/minResources&gt; &lt;maxResources&gt;90000 mb,0vcores&lt;/maxResources&gt; &lt;maxRunningApps&gt;50&lt;/maxRunningApps&gt; &lt;weight&gt;2.0&lt;/weight&gt; &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt; &lt;queue name="sample_sub_queue"&gt; &lt;aclSubmitApps&gt;charlie&lt;/aclSubmitApps&gt; &lt;minResources&gt;5000 mb,0vcores&lt;/minResources&gt; &lt;/queue&gt; &lt;/queue&gt; &lt;user name="sample_user"&gt; &lt;maxRunningApps&gt;30&lt;/maxRunningApps&gt; &lt;/user&gt; &lt;userMaxAppsDefault&gt;5&lt;/userMaxAppsDefault&gt; &lt;queuePlacementPolicy&gt; &lt;rule name="specified" /&gt; &lt;rule name="primaryGroup" create="false" /&gt; &lt;rule name="default" /&gt; &lt;/queuePlacementPolicy&gt;&lt;/allocations&gt; 通过Web UI 监控当前应用程序，队列，和公平共享可以通过资源管理器的web接口来检查，在http://*ResourceManager URL*/cluster/scheduler。在web界面上的每个队列都可以看到下列字段： userd Resources – 队列中分配给容器的资源总和。 Num Active Applications – 已接收至少一个容器的队列中的应用程序的数量。 Num Pending Applications –尚未接收任何容器的队列中的应用程序的数量。 Min Resources – 保证队列最小资源的配置。 Max Resources – 已被允许的队列的配置的最大资源。 Instantaneous Fair Share –队列的瞬时公平共享资源。这些共享只考虑活动的队列（那些运行的应用程序），和用于的调度策略。当其他队列没有使用它们时，队列可能在它们的共享之外分配资源。一个队列的资源消耗出来或低于其公平份额，而不会有瞬时容器争抢。 Steady Fair Share – 队列资源的稳定的公共共享。这些共享考虑所有的队列，不论它们是否是积极的（有运行应用程序）。当配置和容量变化时，这些都被计算较少的变更。它们的意思是提供可视性，在用户期望的资源上，并因此显示在WEB 界面上。]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Yarn</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume详解]]></title>
    <url>%2F2017%2F08%2F11%2Fflume%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是flumeflume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。但随着 FLume 功能的扩展，Flume OG 代码工程臃肿、核心组件设计不合理、核心配置不标准等缺点暴露出来，尤其是在 Flume OG 的最后一个发行版本 0.94.0 中，日志传输不稳定的现象尤为严重，为了解决这些问题，2011 年 10 月 22 号，cloudera 完成了 Flume-728，对 Flume 进行了里程碑式的改动：重构核心组件、核心配置以及代码架构，重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。官方用户手册：http://flume.apache.org/FlumeUserGuide.html flume的特点flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。flume的可靠性当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。flume的可恢复性还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。 flume核心概念 Agent：使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。 Client：生产数据，运行在一个独立的线程。 Sources：从Client收集数据，传递给Channel。 Sink：从Channel收集数据，运行在一个独立线程。 Channel：连接 sources 和 sinks ，这个有点像一个队列。 Events：可以是日志记录、 avro 对象等。 flume 案例案例1：Avrovro可以发送一个给定的文件给Flume，Avro 源使用AVRO RPC机制 创建agent配置文件123456789101112131415161718192021a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 4141# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Describe the sinka1.sinks.k1.type = logger# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 创建指定文件1echo "hello world" &gt; /home/flume/logs/log.00 使用avro-client发送文件1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 15:11:16,718 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 hello world &#125; 案例2：SpoolSpool监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点： 拷贝到spool目录下的文件不可以再打开编辑。 spool目录下不可包含相应的子目录 创建agent配置文件123456789101112131415161718192021a2.sources = r1a2.sinks = k1a2.channels = c1#Describe/configure the sourcea2.sources.r1.type = spooldira2.sources.r1.channels = c1a2.sources.r1.spoolDir = /home/flume/logsa2.sources.r1.fileHeader = true#Describe the sinka2.sinks.k1.type = logger#Use a channel which buffers events in memorya2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100#Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 启动flume agent1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 追加文件到/home/flume/logs目录1echo "spool test1" &gt; /home/flume/logs/1.txt 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 12345608 Aug 2017 16:18:46,736 INFO [lifecycleSupervisor-1-0] (org.apache.flume.source.SpoolDirectorySource.start:78) - SpoolDirectorySource source starting with directory: /home/flume/logs08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:120) - Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:96) - Component type: SOURCE, name: r1 started08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents:258) - Last read took us just up to a file boundary. Rolling to the next file, if there is one.08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile:348) - Preparing to move file /home/flume/logs/2.txt to /home/flume/logs/2.txt.COMPLETED08 Aug 2017 16:20:44,747 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;file=/home/flume/logs/2.txt&#125; body: 73 70 6F 6F 6C 20 74 65 73 74 31 spool test1 &#125; 案例3：ExecEXEC执行一个给定的命令获得输出的源,如果要使用tail命令，必选使得file足够大才能看到输出内容 创建agent配置文件1234567891011121314151617181920a3.sources = r1a3.sinks = k1a3.channels = c1# Describe/configure the sourcea3.sources.r1.type = execa3.sources.r1.channels = c1a3.sources.r1.command = tail -F /var/log/date.log# Describe the sinka3.sinks.k1.type = logger# Use a channel which buffers events in memorya3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela3.sources.r1.channels = c1a3.sinks.k1.channel = c1 创建业务监控的日志文件1234567[root@kiwi02 ~]# cat 1.sh #!/bin/bashwhile true; do echo `/bin/date` &gt;&gt; /var/log/date.log sleep 1done 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 123456708 Aug 2017 16:46:58,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125; 案例4：SyslogtcpSyslogtcp监听TCP的端口做为数据源 创建agent配置文件123456789101112131415161718192021a4.sources = r1a4.sinks = k1a4.channels = c1# Describe/configure the sourcea4.sources.r1.type = syslogtcpa4.sources.r1.port = 5140a4.sources.r1.host = localhosta4.sources.r1.channels = c1# Describe the sinka4.sinks.k1.type = logger# Use a channel which buffers events in memorya4.channels.c1.type = memorya4.channels.c1.capacity = 1000a4.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela4.sources.r1.channels = c1a4.sinks.k1.channel = c1 往5140端口发送信息1[root@kiwi02 ~]# echo "hello idoall.org syslog" | nc localhost 5140 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 21:53:13,584 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;Severity=0, flume.syslog.status=Invalid, Facility=0&#125; body: 68 65 6C 6C 6F 20 69 64 6F 61 6C 6C 2E 6F 72 67 hello idoall.org &#125;]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop2使用实例]]></title>
    <url>%2F2017%2F08%2F11%2Fsqoop2%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Sqoop是一款开源的工具，主要用于在Hadoop和传统的数据库(MySQL、postgresql等)进行数据的传递，可以将一个关系型数据库（例如：mysql、Oracle、Postgres等）中的数据导进到hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop中一大亮点就是可以通过hadoop的mapreduce把数据从关系型数据库中导入数据到HDFS。Sqoop目前版本已经到了1.99.7，我们可以在其官网上看到所有的版本，Sqoop1.99.7是属于sqoop2，Sqoop1的最高版本为1.4.6sqoop2的官方文档：http://sqoop.apache.org/docs/1.99.7/index.html sqoop2的使用过程使用 show connector 查看sqoop的所有连接123456789sqoop:000&gt; show connector+----+------------------------+----------------+------------------------------------------------------+----------------------+| Id | Name | Version | Class | Supported Directions |+----+------------------------+----------------+------------------------------------------------------+----------------------+| 1 | kite-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kite.KiteConnector | FROM/TO || 2 | kafka-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kafka.KafkaConnector | TO || 3 | hdfs-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.hdfs.HdfsConnector | FROM/TO || 4 | generic-jdbc-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.jdbc.GenericJdbcConnector | FROM/TO |+----+------------------------+----------------+------------------------------------------------------+----------------------+ 在向hdfs导入导出数据时，需要依赖以上四个连接创建link （在1.99.4版本之后 用户不需要再创建连接） 查看当前的所有link12345sqoop:000&gt; show link +----+------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------+--------------+----------------+---------++----+------+--------------+----------------+---------+ 查看当前的所有job12345sqoop:000&gt; show job+----+------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+------+----------------+--------------+---------++----+------+----------------+--------------+---------+ 创建link创建hdfs连接（根据connector选择一个合适的连接方式） 12345678sqoop:000&gt; create link -cid 3Creating link for connector with id 3Please fill following values to create new link objectName: hdfs_linkLink configurationHDFS URI: hdfs://kiwi01.novalocal:8020Hadoop conf directory: /cmss/bch/bc1.3.4/hadoop/etc/hadoopNew link was successfully created with validation status OK and persistent id 2 查看创建的连接 123456sqoop:000&gt; show link+----+-----------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+-----------+--------------+----------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true |+----+-----------+--------------+----------------+---------+ 创建mysql的link 123456789101112131415161718sqoop:000&gt; create link -cid 4Creating link for connector with id 4Please fill following values to create new link objectName: mysql_linkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://hcontrol1341/kiwiUsername: kiwiPassword: ****JDBC Connection Properties: There are currently 0 values in the map:entry# protocol=tcpThere are currently 1 values in the map:protocol = tcpentry# New link was successfully created with validation status OK and persistent id 5 查看创建的link 1234567sqoop:000&gt; show link+----+------------+--------------+------------------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------------+--------------+------------------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true || 5 | mysql_link | 4 | generic-jdbc-connector | true |+----+------------+--------------+------------------------+---------+ 创建job123456789101112131415161718192021222324252627282930313233343536373839404142434445464748sqoop:000&gt; create job -f 5 -t 2Creating job for links with from id 5 and to id 2Please fill following values to create new job objectName: mysql_to_hdfsFrom database configurationSchema name: employeesTable name: dept_managerTable SQL statement: Table column names: Partition column name: Null value allowed for the partition column: Boundary query: Incremental readCheck column: Last value: To HDFS configurationOverride null value: Null value: Output format: 0 : TEXT_FILE 1 : SEQUENCE_FILEChoose: 0Compression format: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0Custom compression format: Output directory: hdfs://kiwi01.novalocal:8020/sqoopAppend mode: Throttling resourcesExtractors: 2Loaders: 2New job was successfully created with validation status OK and persistent id 1 查看job的状态 123456sqoop:000&gt; show job+----+---------------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+---------------+----------------+--------------+---------+| 1 | mysql_to_hdfs | 4 | 3 | true |+----+---------------+----------------+--------------+---------+ 启动job12345678910sqoop:000&gt; start job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:55:46 CST: BOOTING - Progress is not available 查看job的状态12345678910sqoop:000&gt; status job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:56:06 CST: RUNNING - 0.00 % End]]></content>
      <categories>
        <category>Sqoop2</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
        <tag>sqoop2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
</search>
