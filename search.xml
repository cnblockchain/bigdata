<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[klist命令]]></title>
    <url>%2F2017%2F08%2F17%2Fklist%2F</url>
    <content type="text"><![CDATA[用途显示 Kerberos 凭证高速缓存或密钥表的内容。 语法klist [[ -c] [ -f] [ -e] [ -s] [ -a] [ -n]] [ -k [ -t] [ -K]] [ name] 描述klist 命令显示 Kerberos 凭证高速缓存或密钥表的内容。 选项 描述 -a 显示所有在凭证高速缓存中的票据，包括过期的票据。如果不指定该标志，不列出过期的票据。仅当列出凭证高速缓存时该标志有效。 -c 列出凭证高速缓存中票据。如果 -c 或 -k 标志都不指定，这个是缺省的。该标志和 -k 标志是互斥的。 -e 显示为会话密钥和票据的加密类型。 -f 用以下缩写显示票据的标志 name 指定凭证高速缓存或密钥表的名称。如果不指定一个文件名则用缺省的凭证高速缓存或密钥表。如果不指定表示高速缓存名称的文件名或密钥表名，klist 显示在缺省凭证高速缓存或密钥表文件的凭证。如果设置了 KRB5CCNAME 环境变量，它的值就用来命名缺省凭证（票据）高速缓存。 -k 列出在密钥表中的条目。该标志和 -c 标志互斥。 -K 为每个密钥表条目显示加密密钥值。仅当列出一个密钥表时该标志有效。 -n 显示数字因特网地址而不是主机名。没有 -n 的缺省情况是显示主机名。该命令与 -a 标志连用。 -s 禁止命令输出但是如果一个有效的票据授权票据（ticket-granting ticket）在凭证高速缓存中被发现，那么设置退出状态为 0。仅当列出凭证高速缓存时该标志有效。 -t 为密钥表条目显示时间戳。仅当列出一个密钥表时该标志有效。 示例 要列出在缺省凭证高速缓存中的所有条目，请输入：bash klist 要列出在 etc/krb5/my_keytab 密钥表中所有条目还有时间戳，请输入:klist -t -k etc/krb5/my_keytab]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
        <tag>klist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kinit命令]]></title>
    <url>%2F2017%2F08%2F17%2Fkinit%2F</url>
    <content type="text"><![CDATA[用途获得或更新 Kerberos 票据授权票据（ticket-granting ticket）。 语法1kinit [ -l lifetime ] [ -r renewable_life ] [ -f ] [ -p ] [ -A ] [ -s start_time ] [ -S target_service ] [ -k [ -t keytab_file ] ] [ -R ] [ -v ] [ -u ] [ -c cachename ] [ principal ] 描述kinit 命令获得或更新 Kerberos 票据授权票据。如果不在命令行上指定一个票据标志，那么使用由在 Kerberos 配置文件（kdc.conf ）中的 [kdcdefault] 和 [realms] 指定的密钥分发中心（KDC）选项.如果不更新一个存在的票据，该命令重新初始化凭证高速缓存并将包含从 KDC 接受的新的票据授权票据。如果不在命令行上指定 Principal 名并且指定 -s 标志，Principal 名从凭证高速缓存中获取。新的凭证高速缓存成为缺省的高速缓存，除非用 -c 标志指定高速缓存的名称。-l、-r 和 -s 标志的票据 Time 值被表达为 ndnhnmns 其中： n 代表一个数字 d 代表天 h 代表小时 m 代表分钟 s 代表秒,必须以这种顺序指定各个部分，但可省略任何部分，例如 4h5m 代表 4 小时 5 分钟，1d2s 代表 1 天 2 秒。 标志描述 选项 描述 -A 指定这个票据包含一个客户机地址的列表。如果不指定这个选项，这个票据将包含本地主机地址。当一个初始票据包含一个地址列表时，它仅可从地址列表中的一个地址中使用。 -c cachename 指定要用的凭证高速缓存的名称。如果该标志没被指定，应用缺省凭证高速缓存。如果 KRB5CCNAME 环境变量被设置，它的值被用来命名缺省票据高速缓存。高速缓存的任何存在的内容可由 kinit 破坏 -f 指定票据是可转发的。为转发票据，该标志必须被指定。 -k 指定从密钥表获得票据主体的密钥。如果不指定该标志，将提示您为票据主体输入密码。 -l lifetime 指定票据结束时间间隔。在到期失效后，不能再用此票据，除非票据被更新。这个间隔缺省时间是 10 小时。 -p 指定这个票据是可代理的。要使票据可代理，该标志必须被指定。principal 指定票据的主体。如果主体不在命令行中指定，那么主体从凭证高速缓存获得。 -r renewable_life 为可更新的票据指定更新时间间隔。在间隔到期后，票据不能被更新。更新时间必须大于结束时间。如果该标志不指定，那么这个票据是不可更新的，尽管如果请求的票据的生命期超出最大票据生命期仍能生成一个可更新的票据。 -R 指定更新一个存在的票据。当更新一个存在的票据时,可能没指定其他标志。 -s start_time 为一个迟后的票据指定一个请求，从 start_time 开始有效。 -S target_service 当得到初始票据时指定一个备用服务名来用。 -t keytab_file 指定密钥表名。如果没指定该标志并且 -k 标志被指定，用缺省的密钥表。-t 标志意味着 -k 标志。 -v 指定在高速缓存中的票据授权票据应被传到 kdc 来确认。如果票据在它的请求的时间范围内，高速缓存用确认过的票据替换。 -u 指定 kinit 命令创建进程的唯一凭证高速缓存文件。如果 kinit 命令成功，那么凭证高速缓存文件名将包含一个唯一编号（进程认证组或 PAG）。在 AIX® V5.3 和更高版本中，将从操作系统服务生成 PAG。KRB5CCNAME 环境变量设置为此凭证高速缓存文件，而 kinit 命令执行新的 shell。 示例 要获得一个生命期为 10 小时五天内可更新的票据授权票据，请输入：kinit -l 10h -r 5d my_principal 要更新一个存在票据，请输入：kinit -R]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
        <tag>kinit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrips]]></title>
    <url>%2F2017%2F08%2F17%2Fscrips%2F</url>
    <content type="text"><![CDATA[1.123456789101112#! /bin/bashDAY=`/bin/date "+%Y%m%d%H" -d "-1 hours"`echo `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/cpu.logls /home/bcstream/monitor-storm/monitor*.log.$DAY* | xargs cat &gt; file.log a=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|head -1` b=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|tail -1` c=`cat file.log |sort -n|wc -l` d=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|awk '&#123;a+=$1&#125;'END'&#123;print a&#125;'`echo "max cpu_usage:"$b &gt;&gt; /root/statistics/cpu.logecho "min cpu_usage:"$a &gt;&gt; /root/statistics/cpu.logecho "average cpu_usage is:" `awk 'BEGIN&#123;printf "%.2f\n", '$d'/'$c'&#125;'` &gt;&gt; /root/statistics/cpu.logrm -rf file.log 2.12345678910111213141516#! /bin/bashDAY=`/bin/date "+%Y%m%d%H" -d "-1 hours"`echo `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/cpu.logls /home/bcstream/monitor-storm/monitor*.log.$DAY* | xargs cat &gt; file.log a=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|head -1` b=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|tail -1` c=`cat file.log |sort -n|wc -l` d=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|awk '&#123;a+=$1&#125;'END'&#123;print a&#125;'`echo "max cpu_usage:"$b &gt;&gt; /root/statistics/cpu.logecho "min cpu_usage:"$a &gt;&gt; /root/statistics/cpu.logecho "average cpu_usage is:" `awk 'BEGIN&#123;printf "%.2f\n", '$d'/'$c'&#125;'` &gt;&gt; /root/statistics/cpu.logrm -rf file.log[root@YP-TYHJ-APOLLO4200-5031 statistics]# cat kafka.sh#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/kafka.logcat /home/bcstream/srv/logs/writer/writer.log.`/bin/date "+%Y-%m-%d-%H"`-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;' &gt;&gt; /root/statistics/kafka.log 3.12345678910111213#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/kafka.logcat /home/bcstream/srv/logs/writer/writer.log.`/bin/date "+%Y-%m-%d-%H"`-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;' &gt;&gt; /root/statistics/kafka.log[root@YP-TYHJ-APOLLO4200-5031 statistics]# cat storm.sh#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/storm.logDAY=`/bin/date "+%Y-%m-%d-%H"`COUNT=`cat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |wc -l`for((i=1;i&lt;=$COUNT;i++)); do cat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |awk -F [\:] '&#123;print $2&#125;' | awk -F [\-] '&#123;print $3&#125;' |sed -n "$i p" &gt;&gt; /root/statistics/storm.logcat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |awk -F [\;] '&#123;print $4&#125;' |sed -n "$i p" &gt;&gt; /root/statistics/storm.logdone 4.12345#!/bin/bashLOG_DIR=/home/bcstream/monitorLOG_RETAIN_DAY=1find $LOG_DIR -type f -mtime +$LOG_RETAIN_DAY -name "*.complete" -exec rm &#123;&#125; \; &gt;/dev/null 5.12345678910#! /bin/bashDAY=$(date +%Y%m%d%H)HOST=$(hostname --fqdn)LOG_DIR=/home/bcstream/monitorif [ ! -d $LOG_DIR ]; then mkdir $LOG_DIRfiLOG=$LOG_DIR/monitor_$HOST.log.$DAYcpuUsage=`sar -C 1 1|grep "Average" | awk '&#123;print $3&#125;'`echo "$(date +"%Y%m%d %H:%M")|$cpuUsage"&gt;&gt; $LOG 6.12345678910111213141516171819#/bin/bashLOG_DIR=/home/bcstream/monitorCOORDINATE_HOST=10.11.94.52COORDINATE_DIR=/home/bcstream/monitor-stormpasswd=Tyhj@1118for file in `ls $LOG_DIR | grep -v complete`do echo $file expect -c " spawn scp $LOG_DIR/$file bcstream@$COORDINATE_HOST:$COORDINATE_DIR expect &#123; \"assword\" &#123;set timeout 300; send \"$passwd\r\";&#125; \"yes/no\" &#123;send \"yes\r\"; exp_continue;&#125; &#125;expect eof"mv $LOG_DIR/$file $LOG_DIR/$file.completedone 7.12345#! /bin/basha=`cat /home/bcstream/srv/logs/writer/writer.log.2017-08-17-14-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;'`b=`cat /home/bcstream/srv/logs/writer/writer.log.2017-08-17-14-10.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;'`c=$(($b-$a))echo "Kafka statistics:"$c]]></content>
      <categories>
        <category>Bash</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Dynamic Resource Allocation 动态资源分配]]></title>
    <url>%2F2017%2F08%2F16%2FSpark-Dynamic-Resource-Allocation%2F</url>
    <content type="text"><![CDATA[http://lxw1234.com/archives/2015/12/593.htm Spark中，所谓资源单位一般指的是executors，和Yarn中的Containers一样，在Spark On Yarn模式下，通常使用–num-executors来指定Application使用的executors数量，而–executor-memory和–executor-cores分别用来指定每个executor所使用的内存和虚拟CPU核数。相信很多朋友至今在提交Spark应用程序时候都使用该方式来指定资源。 假设有这样的场景，如果使用Hive，多个用户同时使用hive-cli做数据开发和分析，只有当用户提交执行了Hive SQL时候，才会向YARN申请资源，执行任务，如果不提交执行，无非就是停留在Hive-cli命令行，也就是个JVM而已，并不会浪费YARN的资源。现在想用Spark-SQL代替Hive来做数据开发和分析，也是多用户同时使用，如果按照之前的方式，以yarn-client模式运行spark-sql命令行，在启动时候指定–num-executors 10，那么每个用户启动时候都使用了10个YARN的资源（Container），这10个资源就会一直被占用着，只有当用户退出spark-sql命令行时才会释放。 spark-sql On Yarn，能不能像Hive一样，执行SQL的时候才去申请资源，不执行的时候就释放掉资源呢，其实从Spark1.2之后，对于On Yarn模式，已经支持动态资源分配（Dynamic Resource Allocation），这样，就可以根据Application的负载（Task情况），动态的增加和减少executors，这种策略非常适合在YARN上使用spark-sql做数据开发和分析，以及将spark-sql作为长服务来使用的场景。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop FairScheduler介绍]]></title>
    <url>%2F2017%2F08%2F15%2FHadoop-FairScheduler%2F</url>
    <content type="text"><![CDATA[介绍FairScheduler是一个资源分配方式，在整个时间线上，所有的applications平均的获取资源。Hadoop NextGen能够调度多种类型的资源。默认情况下，FairScheduler只是对内存资源做公平的调度(分配)。当集群中只有一个application运行时，那么此application占用这个集群资源。当其他的applications提交后，那些释放的资源将会被分配给新的applications，所以每个applicaiton最终都能获取几乎一样多的资源。不像Hadoop默认的Scheduler(CapacityScheduler)，CapacityScheduler将applications以queue的方式组成，它可以让short applications在何时的时间内完成，而不会starving那些长期运行的applications，它也是一个合理的方式在多个用户之间共享集群。最终，Fair共享也可以与application priorities一起工作—–“priorities”作为权重来使用，以决定每个application需要获取资源的量。 Scheduler将applications以queues的方式组织，在这些queues之间公平的共享资源。默认，所有的users共享一个queue，名称为“default”。如果application在请求资源时指定了queue，那么请求将会被提交到指定的queue中；仍然可以通过配置，根据请求中包含的user名称来分配queue。在每个queue内部，调度策略是在运行中的applicaitons之间共享资源。默认是基于内存的公平共享，不过FIFO和multi-resource with Dominant Resource Fairness也能够配置。Queues可以分级来划分资源，配置权重以特定的比例共享集群资源。 FairScheduler允许为queues分配担保性的最小的共享资源量，这对保证某些用户、groups或者applications总能获取充足的资源是有帮助的。当一个queue中包含applications时，它至少能够获取最小量的共享资源，当queue不在需要时，那些过剩的资源将会被拆分给其他的运行中的application。这就让Scheduler在有效利用资源是，保证了queue的capacity。 FairScheudler在默认情况下允许所有的application运行，但是这也可以通过配置文件来限制每个用户下和每个queue下运行applications的个数。这对限制一个用户一次提交大量applications是有用的，或者通过限制running applications个数来提升性能，因为大量的running applicaiton会导致创建大量的中间数据或者过多的上下文切换。限制applications不会导致随后的提交失败，只是在Scheduler queue中等待，直到先前的application结束。 Hierarchical queuesFairScheduler支持分层的queues。所有的queues继承自“root” queue。有效的资源在root子节点中，以典型的公平调度的方式分布；子节点再将分配给自己的资源以相同的方式分配给自己的子节点。applications只能在queue的叶子节点上调度。可以通过FairScheduler相关的配置文件，Queues可以被指定作为其他queues的子节点。 Queue的名字，以其父节点path作为开始，以“.”作为分割符。比如名称为“queue1”的queue作为root的子节点，那么应该表示为“root.queue1”，名称为“queue2”的queue为“parent1”的子节点，那么应该表示为“root.parent1.queue2”。当指明一个queue时，root部分是可选的，比如“queue1”就是指queue1，而queue2指“parent1.queue2”。 此外，FairScheduler允许为每个queue指定不同的policy，每个queue都可以根据用户希望的方式共享queue的资源。这些自定义的Policy可以通过实现“org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy”来构建；FifoPolicy，FairSharePolicy(默认)，以及DominantResouceFairnessPolicy都是内置的，可以直接使用。 Installation为了使用FairScheduler，首先需要在yarn-site.xml配置：1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt; 配置定制FairScheduler涉及到2个文件。首先，scheduler有关的选项可以在yarn-site.xml中配置。此外，多数情况，用户需要创建一个“allocation”文件来列举存在的queues和它们相应的weights和capacities。这个“allocation”文件每隔10秒钟加载一次，更新的配置可以更快的生效。 yarn-site.xml中的配置 yarn.scheduler.fair.allocation.file： “allocation”文件的位置，“allocation”文件是一个用来描述queue以及它们的属性的配置文件。这个文件必须为格式严格的xml文件。如果为相对路径，那么将会在classpath下查找此文件(conf目录下)。默认值为“fair-scheduler.xml”。 yarn.scheduler.fair.user-as-default-queue：是否将与allocation有关的username作为默认的queue name，当queue name没有指定的时候。如果设置成false(且没有指定queue name) 或者没有设定，所有的jobs将共享“default” queue。默认值为true。 yarn.scheduler.fair.preemption：是否使用“preemption”(优先权，抢占)，默认为fasle，在此版本中此功能为测试性的。 yarn.scheduler.fair.assignmultiple：是在允许在一个心跳中，发送多个container分配信息。默认值为false。 yarn.scheduler.fair.max.assign：如果assignmultuple为true，那么在一次心跳中，最多发送分配container的个数。默认为-1，无限制。 yarn.scheduler.fair.locality.threshold.node：一个float值，在0~1之间，表示在等待获取满足node-local条件的containers时，最多放弃不满足node-local的container的机会次数，放弃的nodes个数为集群的大小的比例。默认值为-1.0表示不放弃任何调度的机会。 yarn.scheduler.fair.locality.threashod.rack：同上，满足rack-local。 yarn.scheduler.fair.sizebaseweight：是否根据application的大小(job的个数)作为权重。默认为false，如果为true，那么复杂的application将获取更多的资源。 Example一个公平调度的例子：12345678910111213141516171819202122&lt;allocations&gt; &lt;queue name="sample_queue"&gt; &lt;minResources&gt;10000 mb,0vcores&lt;/minResources&gt; &lt;maxResources&gt;90000 mb,0vcores&lt;/maxResources&gt; &lt;maxRunningApps&gt;50&lt;/maxRunningApps&gt; &lt;weight&gt;2.0&lt;/weight&gt; &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt; &lt;queue name="sample_sub_queue"&gt; &lt;aclSubmitApps&gt;charlie&lt;/aclSubmitApps&gt; &lt;minResources&gt;5000 mb,0vcores&lt;/minResources&gt; &lt;/queue&gt; &lt;/queue&gt; &lt;user name="sample_user"&gt; &lt;maxRunningApps&gt;30&lt;/maxRunningApps&gt; &lt;/user&gt; &lt;userMaxAppsDefault&gt;5&lt;/userMaxAppsDefault&gt; &lt;queuePlacementPolicy&gt; &lt;rule name="specified" /&gt; &lt;rule name="primaryGroup" create="false" /&gt; &lt;rule name="default" /&gt; &lt;/queuePlacementPolicy&gt;&lt;/allocations&gt;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Yarn</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume详解]]></title>
    <url>%2F2017%2F08%2F11%2Fflume%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是flumeflume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。但随着 FLume 功能的扩展，Flume OG 代码工程臃肿、核心组件设计不合理、核心配置不标准等缺点暴露出来，尤其是在 Flume OG 的最后一个发行版本 0.94.0 中，日志传输不稳定的现象尤为严重，为了解决这些问题，2011 年 10 月 22 号，cloudera 完成了 Flume-728，对 Flume 进行了里程碑式的改动：重构核心组件、核心配置以及代码架构，重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。官方用户手册：http://flume.apache.org/FlumeUserGuide.html flume的特点flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。flume的可靠性当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。flume的可恢复性还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。 flume核心概念 Agent：使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。 Client：生产数据，运行在一个独立的线程。 Sources：从Client收集数据，传递给Channel。 Sink：从Channel收集数据，运行在一个独立线程。 Channel：连接 sources 和 sinks ，这个有点像一个队列。 Events：可以是日志记录、 avro 对象等。 flume 案例案例1：Avrovro可以发送一个给定的文件给Flume，Avro 源使用AVRO RPC机制 创建agent配置文件123456789101112131415161718192021a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 4141# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Describe the sinka1.sinks.k1.type = logger# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 创建指定文件1echo "hello world" &gt; /home/flume/logs/log.00 使用avro-client发送文件1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 15:11:16,718 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 hello world &#125; 案例2：SpoolSpool监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点： 拷贝到spool目录下的文件不可以再打开编辑。 spool目录下不可包含相应的子目录 创建agent配置文件123456789101112131415161718192021a2.sources = r1a2.sinks = k1a2.channels = c1#Describe/configure the sourcea2.sources.r1.type = spooldira2.sources.r1.channels = c1a2.sources.r1.spoolDir = /home/flume/logsa2.sources.r1.fileHeader = true#Describe the sinka2.sinks.k1.type = logger#Use a channel which buffers events in memorya2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100#Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 启动flume agent1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 追加文件到/home/flume/logs目录1echo "spool test1" &gt; /home/flume/logs/1.txt 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 12345608 Aug 2017 16:18:46,736 INFO [lifecycleSupervisor-1-0] (org.apache.flume.source.SpoolDirectorySource.start:78) - SpoolDirectorySource source starting with directory: /home/flume/logs08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:120) - Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:96) - Component type: SOURCE, name: r1 started08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents:258) - Last read took us just up to a file boundary. Rolling to the next file, if there is one.08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile:348) - Preparing to move file /home/flume/logs/2.txt to /home/flume/logs/2.txt.COMPLETED08 Aug 2017 16:20:44,747 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;file=/home/flume/logs/2.txt&#125; body: 73 70 6F 6F 6C 20 74 65 73 74 31 spool test1 &#125; 案例3：ExecEXEC执行一个给定的命令获得输出的源,如果要使用tail命令，必选使得file足够大才能看到输出内容 创建agent配置文件1234567891011121314151617181920a3.sources = r1a3.sinks = k1a3.channels = c1# Describe/configure the sourcea3.sources.r1.type = execa3.sources.r1.channels = c1a3.sources.r1.command = tail -F /var/log/date.log# Describe the sinka3.sinks.k1.type = logger# Use a channel which buffers events in memorya3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela3.sources.r1.channels = c1a3.sinks.k1.channel = c1 创建业务监控的日志文件1234567[root@kiwi02 ~]# cat 1.sh #!/bin/bashwhile true; do echo `/bin/date` &gt;&gt; /var/log/date.log sleep 1done 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 123456708 Aug 2017 16:46:58,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125; 案例4：SyslogtcpSyslogtcp监听TCP的端口做为数据源 创建agent配置文件123456789101112131415161718192021a4.sources = r1a4.sinks = k1a4.channels = c1# Describe/configure the sourcea4.sources.r1.type = syslogtcpa4.sources.r1.port = 5140a4.sources.r1.host = localhosta4.sources.r1.channels = c1# Describe the sinka4.sinks.k1.type = logger# Use a channel which buffers events in memorya4.channels.c1.type = memorya4.channels.c1.capacity = 1000a4.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela4.sources.r1.channels = c1a4.sinks.k1.channel = c1 往5140端口发送信息1[root@kiwi02 ~]# echo "hello idoall.org syslog" | nc localhost 5140 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 21:53:13,584 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;Severity=0, flume.syslog.status=Invalid, Facility=0&#125; body: 68 65 6C 6C 6F 20 69 64 6F 61 6C 6C 2E 6F 72 67 hello idoall.org &#125;]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop2使用实例]]></title>
    <url>%2F2017%2F08%2F11%2Fsqoop2%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Sqoop是一款开源的工具，主要用于在Hadoop和传统的数据库(MySQL、postgresql等)进行数据的传递，可以将一个关系型数据库（例如：mysql、Oracle、Postgres等）中的数据导进到hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop中一大亮点就是可以通过hadoop的mapreduce把数据从关系型数据库中导入数据到HDFS。Sqoop目前版本已经到了1.99.7，我们可以在其官网上看到所有的版本，Sqoop1.99.7是属于sqoop2，Sqoop1的最高版本为1.4.6sqoop2的官方文档：http://sqoop.apache.org/docs/1.99.7/index.html sqoop2的使用过程使用 show connector 查看sqoop的所有连接123456789sqoop:000&gt; show connector+----+------------------------+----------------+------------------------------------------------------+----------------------+| Id | Name | Version | Class | Supported Directions |+----+------------------------+----------------+------------------------------------------------------+----------------------+| 1 | kite-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kite.KiteConnector | FROM/TO || 2 | kafka-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kafka.KafkaConnector | TO || 3 | hdfs-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.hdfs.HdfsConnector | FROM/TO || 4 | generic-jdbc-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.jdbc.GenericJdbcConnector | FROM/TO |+----+------------------------+----------------+------------------------------------------------------+----------------------+ 在向hdfs导入导出数据时，需要依赖以上四个连接创建link （在1.99.4版本之后 用户不需要再创建连接） 查看当前的所有link12345sqoop:000&gt; show link +----+------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------+--------------+----------------+---------++----+------+--------------+----------------+---------+ 查看当前的所有job12345sqoop:000&gt; show job+----+------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+------+----------------+--------------+---------++----+------+----------------+--------------+---------+ 创建link创建hdfs连接（根据connector选择一个合适的连接方式） 12345678sqoop:000&gt; create link -cid 3Creating link for connector with id 3Please fill following values to create new link objectName: hdfs_linkLink configurationHDFS URI: hdfs://kiwi01.novalocal:8020Hadoop conf directory: /cmss/bch/bc1.3.4/hadoop/etc/hadoopNew link was successfully created with validation status OK and persistent id 2 查看创建的连接 123456sqoop:000&gt; show link+----+-----------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+-----------+--------------+----------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true |+----+-----------+--------------+----------------+---------+ 创建mysql的link 123456789101112131415161718sqoop:000&gt; create link -cid 4Creating link for connector with id 4Please fill following values to create new link objectName: mysql_linkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://hcontrol1341/kiwiUsername: kiwiPassword: ****JDBC Connection Properties: There are currently 0 values in the map:entry# protocol=tcpThere are currently 1 values in the map:protocol = tcpentry# New link was successfully created with validation status OK and persistent id 5 查看创建的link 1234567sqoop:000&gt; show link+----+------------+--------------+------------------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------------+--------------+------------------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true || 5 | mysql_link | 4 | generic-jdbc-connector | true |+----+------------+--------------+------------------------+---------+ 创建job123456789101112131415161718192021222324252627282930313233343536373839404142434445464748sqoop:000&gt; create job -f 5 -t 2Creating job for links with from id 5 and to id 2Please fill following values to create new job objectName: mysql_to_hdfsFrom database configurationSchema name: employeesTable name: dept_managerTable SQL statement: Table column names: Partition column name: Null value allowed for the partition column: Boundary query: Incremental readCheck column: Last value: To HDFS configurationOverride null value: Null value: Output format: 0 : TEXT_FILE 1 : SEQUENCE_FILEChoose: 0Compression format: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0Custom compression format: Output directory: hdfs://kiwi01.novalocal:8020/sqoopAppend mode: Throttling resourcesExtractors: 2Loaders: 2New job was successfully created with validation status OK and persistent id 1 查看job的状态 123456sqoop:000&gt; show job+----+---------------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+---------------+----------------+--------------+---------+| 1 | mysql_to_hdfs | 4 | 3 | true |+----+---------------+----------------+--------------+---------+ 启动job12345678910sqoop:000&gt; start job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:55:46 CST: BOOTING - Progress is not available 查看job的状态12345678910sqoop:000&gt; status job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:56:06 CST: RUNNING - 0.00 % End]]></content>
      <categories>
        <category>Sqoop2</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
        <tag>sqoop2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
</search>
