<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hadoop FairScheduler介绍]]></title>
    <url>%2F2017%2F08%2F15%2FHadoop-FairScheduler%2F</url>
    <content type="text"><![CDATA[介绍FairScheduler是一个资源分配方式，在整个时间线上，所有的applications平均的获取资源。Hadoop NextGen能够调度多种类型的资源。默认情况下，FairScheduler只是对内存资源做公平的调度(分配)。当集群中只有一个application运行时，那么此application占用这个集群资源。当其他的applications提交后，那些释放的资源将会被分配给新的applications，所以每个applicaiton最终都能获取几乎一样多的资源。不像Hadoop默认的Scheduler(CapacityScheduler)，CapacityScheduler将applications以queue的方式组成，它可以让short applications在何时的时间内完成，而不会starving那些长期运行的applications，它也是一个合理的方式在多个用户之间共享集群。最终，Fair共享也可以与application priorities一起工作—–“priorities”作为权重来使用，以决定每个application需要获取资源的量。 Scheduler将applications以queues的方式组织，在这些queues之间公平的共享资源。默认，所有的users共享一个queue，名称为“default”。如果application在请求资源时指定了queue，那么请求将会被提交到指定的queue中；仍然可以通过配置，根据请求中包含的user名称来分配queue。在每个queue内部，调度策略是在运行中的applicaitons之间共享资源。默认是基于内存的公平共享，不过FIFO和multi-resource with Dominant Resource Fairness也能够配置。Queues可以分级来划分资源，配置权重以特定的比例共享集群资源。 FairScheduler允许为queues分配担保性的最小的共享资源量，这对保证某些用户、groups或者applications总能获取充足的资源是有帮助的。当一个queue中包含applications时，它至少能够获取最小量的共享资源，当queue不在需要时，那些过剩的资源将会被拆分给其他的运行中的application。这就让Scheduler在有效利用资源是，保证了queue的capacity。 FairScheudler在默认情况下允许所有的application运行，但是这也可以通过配置文件来限制每个用户下和每个queue下运行applications的个数。这对限制一个用户一次提交大量applications是有用的，或者通过限制running applications个数来提升性能，因为大量的running applicaiton会导致创建大量的中间数据或者过多的上下文切换。限制applications不会导致随后的提交失败，只是在Scheduler queue中等待，直到先前的application结束。]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Yarn</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume详解]]></title>
    <url>%2F2017%2F08%2F11%2Fflume%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是flumeflume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。但随着 FLume 功能的扩展，Flume OG 代码工程臃肿、核心组件设计不合理、核心配置不标准等缺点暴露出来，尤其是在 Flume OG 的最后一个发行版本 0.94.0 中，日志传输不稳定的现象尤为严重，为了解决这些问题，2011 年 10 月 22 号，cloudera 完成了 Flume-728，对 Flume 进行了里程碑式的改动：重构核心组件、核心配置以及代码架构，重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。官方用户手册：http://flume.apache.org/FlumeUserGuide.html flume的特点flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。flume的可靠性当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。flume的可恢复性还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。 flume核心概念 Agent：使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。 Client：生产数据，运行在一个独立的线程。 Sources：从Client收集数据，传递给Channel。 Sink：从Channel收集数据，运行在一个独立线程。 Channel：连接 sources 和 sinks ，这个有点像一个队列。 Events：可以是日志记录、 avro 对象等。 flume 案例案例1：Avrovro可以发送一个给定的文件给Flume，Avro 源使用AVRO RPC机制 创建agent配置文件123456789101112131415161718192021a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 4141# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Describe the sinka1.sinks.k1.type = logger# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 创建指定文件1echo "hello world" &gt; /home/flume/logs/log.00 使用avro-client发送文件1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 15:11:16,718 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 hello world &#125; 案例2：SpoolSpool监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点： 拷贝到spool目录下的文件不可以再打开编辑。 spool目录下不可包含相应的子目录 创建agent配置文件123456789101112131415161718192021a2.sources = r1a2.sinks = k1a2.channels = c1#Describe/configure the sourcea2.sources.r1.type = spooldira2.sources.r1.channels = c1a2.sources.r1.spoolDir = /home/flume/logsa2.sources.r1.fileHeader = true#Describe the sinka2.sinks.k1.type = logger#Use a channel which buffers events in memorya2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100#Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 启动flume agent1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 追加文件到/home/flume/logs目录1echo "spool test1" &gt; /home/flume/logs/1.txt 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 12345608 Aug 2017 16:18:46,736 INFO [lifecycleSupervisor-1-0] (org.apache.flume.source.SpoolDirectorySource.start:78) - SpoolDirectorySource source starting with directory: /home/flume/logs08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:120) - Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:96) - Component type: SOURCE, name: r1 started08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents:258) - Last read took us just up to a file boundary. Rolling to the next file, if there is one.08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile:348) - Preparing to move file /home/flume/logs/2.txt to /home/flume/logs/2.txt.COMPLETED08 Aug 2017 16:20:44,747 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;file=/home/flume/logs/2.txt&#125; body: 73 70 6F 6F 6C 20 74 65 73 74 31 spool test1 &#125; 案例3：ExecEXEC执行一个给定的命令获得输出的源,如果要使用tail命令，必选使得file足够大才能看到输出内容 创建agent配置文件1234567891011121314151617181920a3.sources = r1a3.sinks = k1a3.channels = c1# Describe/configure the sourcea3.sources.r1.type = execa3.sources.r1.channels = c1a3.sources.r1.command = tail -F /var/log/date.log# Describe the sinka3.sinks.k1.type = logger# Use a channel which buffers events in memorya3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela3.sources.r1.channels = c1a3.sinks.k1.channel = c1 创建业务监控的日志文件1234567[root@kiwi02 ~]# cat 1.sh #!/bin/bashwhile true; do echo `/bin/date` &gt;&gt; /var/log/date.log sleep 1done 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 123456708 Aug 2017 16:46:58,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125; 案例4：SyslogtcpSyslogtcp监听TCP的端口做为数据源 创建agent配置文件123456789101112131415161718192021a4.sources = r1a4.sinks = k1a4.channels = c1# Describe/configure the sourcea4.sources.r1.type = syslogtcpa4.sources.r1.port = 5140a4.sources.r1.host = localhosta4.sources.r1.channels = c1# Describe the sinka4.sinks.k1.type = logger# Use a channel which buffers events in memorya4.channels.c1.type = memorya4.channels.c1.capacity = 1000a4.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela4.sources.r1.channels = c1a4.sinks.k1.channel = c1 往5140端口发送信息1[root@kiwi02 ~]# echo "hello idoall.org syslog" | nc localhost 5140 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 21:53:13,584 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;Severity=0, flume.syslog.status=Invalid, Facility=0&#125; body: 68 65 6C 6C 6F 20 69 64 6F 61 6C 6C 2E 6F 72 67 hello idoall.org &#125;]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop2使用实例]]></title>
    <url>%2F2017%2F08%2F11%2Fsqoop2%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Sqoop是一款开源的工具，主要用于在Hadoop和传统的数据库(MySQL、postgresql等)进行数据的传递，可以将一个关系型数据库（例如：mysql、Oracle、Postgres等）中的数据导进到hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop中一大亮点就是可以通过hadoop的mapreduce把数据从关系型数据库中导入数据到HDFS。Sqoop目前版本已经到了1.99.7，我们可以在其官网上看到所有的版本，Sqoop1.99.7是属于sqoop2，Sqoop1的最高版本为1.4.6sqoop2的官方文档：http://sqoop.apache.org/docs/1.99.7/index.html sqoop2的使用过程使用 show connector 查看sqoop的所有连接123456789sqoop:000&gt; show connector+----+------------------------+----------------+------------------------------------------------------+----------------------+| Id | Name | Version | Class | Supported Directions |+----+------------------------+----------------+------------------------------------------------------+----------------------+| 1 | kite-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kite.KiteConnector | FROM/TO || 2 | kafka-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kafka.KafkaConnector | TO || 3 | hdfs-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.hdfs.HdfsConnector | FROM/TO || 4 | generic-jdbc-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.jdbc.GenericJdbcConnector | FROM/TO |+----+------------------------+----------------+------------------------------------------------------+----------------------+ 在向hdfs导入导出数据时，需要依赖以上四个连接创建link （在1.99.4版本之后 用户不需要再创建连接） 查看当前的所有link12345sqoop:000&gt; show link +----+------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------+--------------+----------------+---------++----+------+--------------+----------------+---------+ 查看当前的所有job12345sqoop:000&gt; show job+----+------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+------+----------------+--------------+---------++----+------+----------------+--------------+---------+ 创建link创建hdfs连接（根据connector选择一个合适的连接方式） 12345678sqoop:000&gt; create link -cid 3Creating link for connector with id 3Please fill following values to create new link objectName: hdfs_linkLink configurationHDFS URI: hdfs://kiwi01.novalocal:8020Hadoop conf directory: /cmss/bch/bc1.3.4/hadoop/etc/hadoopNew link was successfully created with validation status OK and persistent id 2 查看创建的连接 123456sqoop:000&gt; show link+----+-----------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+-----------+--------------+----------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true |+----+-----------+--------------+----------------+---------+ 创建mysql的link 123456789101112131415161718sqoop:000&gt; create link -cid 4Creating link for connector with id 4Please fill following values to create new link objectName: mysql_linkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://hcontrol1341/kiwiUsername: kiwiPassword: ****JDBC Connection Properties: There are currently 0 values in the map:entry# protocol=tcpThere are currently 1 values in the map:protocol = tcpentry# New link was successfully created with validation status OK and persistent id 5 查看创建的link 1234567sqoop:000&gt; show link+----+------------+--------------+------------------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------------+--------------+------------------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true || 5 | mysql_link | 4 | generic-jdbc-connector | true |+----+------------+--------------+------------------------+---------+ 创建job123456789101112131415161718192021222324252627282930313233343536373839404142434445464748sqoop:000&gt; create job -f 5 -t 2Creating job for links with from id 5 and to id 2Please fill following values to create new job objectName: mysql_to_hdfsFrom database configurationSchema name: employeesTable name: dept_managerTable SQL statement: Table column names: Partition column name: Null value allowed for the partition column: Boundary query: Incremental readCheck column: Last value: To HDFS configurationOverride null value: Null value: Output format: 0 : TEXT_FILE 1 : SEQUENCE_FILEChoose: 0Compression format: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0Custom compression format: Output directory: hdfs://kiwi01.novalocal:8020/sqoopAppend mode: Throttling resourcesExtractors: 2Loaders: 2New job was successfully created with validation status OK and persistent id 1 查看job的状态 123456sqoop:000&gt; show job+----+---------------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+---------------+----------------+--------------+---------+| 1 | mysql_to_hdfs | 4 | 3 | true |+----+---------------+----------------+--------------+---------+ 启动job12345678910sqoop:000&gt; start job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:55:46 CST: BOOTING - Progress is not available 查看job的状态12345678910sqoop:000&gt; status job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:56:06 CST: RUNNING - 0.00 % End]]></content>
      <categories>
        <category>Sqoop2</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
        <tag>sqoop2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>测试</tag>
      </tags>
  </entry>
</search>
