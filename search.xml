<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Oracle install]]></title>
    <url>%2F2017%2F11%2F18%2FOracle-install%2F</url>
    <content type="text"><![CDATA[操作系统配置配置网路1234567891011121314151617181920212223cd /etc/sysconfig/network-scripts vi ifcfg-eth0``` 修改onboot=no ===&gt; yes，其他保持不变 ![](http://ouh6z710l.bkt.clouddn.com/WX20171118-141622@2x.png)重新启动linux网络 ``` bashservice network restart``` ip addr 查询到ip地址 ![](http://ouh6z710l.bkt.clouddn.com/WX20171118-141521@2x.png)检查网络是否连通![](http://ouh6z710l.bkt.clouddn.com/WX20171118-142542@2x.png)&lt;!-- more --&gt;## 关闭防火墙关闭防火墙服务``` bashservice iptables stop``` 关闭防火墙开机自启动![](http://ouh6z710l.bkt.clouddn.com/WX20171118-142001@2x.png)## 关闭selinux临时关闭``` bash[root@sannetocp ~]# getenforce Enforcing [root@sannetocp ~]# setenforce 0 [root@sannetocp ~]# getenforce Permissive 永久关闭1vi /etc/selinux/config 修改SELINUX= enforcing ==&gt;disabled reboot重启服务器 [root@sannetocp ~]# getenforce Disabled 添加主机名称解析 检查操作系统缺包由于Oracle软件安装，需要依赖很多的linux操作系统软件，所以我们需要安装很多的操作系统软件包，我们可以使用如下的命令来检查操作系统缺少的软件包1rpm -q binutils compat-libstdc++-33 elfutils-libelf elfutils-libelf-devel expat gcc gcc-c++ glibc glibc-common glibc-devel glibc-headers libaio libaio-devel libgcc libstdc++ libstdc++-devel make pdksh sysstat unixODBC unixODBC-devel | grep "not installed" 使用yum install的命令安装相关软件包注意pdksh的包需要额外进行下载安装上传pdksh的rpm包1yum localinstall pdksh-5.2.14-1.i386.rpm -y 安装完成后进行检查，确保没有缺包 创建Oracle用户，创建相关文件夹创建用户1groupadd dba groupadd oinstall useradd -g oinstall -G dba oracle 设置oracle用户密码1passwd oracle 设定oracle用户环境变量12345su - oraclevi .bash_profile添加export ORACLE_BASE=/u01/app/oracle export ORACLE_SID=san export ORACLE_HOME=$ORACLE_BASE/product/11.2.0/db export PATH=$ORACLE_HOME/bin:$PATHalias sql='sqlplus / as sysdba' 使环境变量生效1source .bash_profile 创建文件夹``` bashmkdir -p /u01/app/oracle/product/11.2.0/dbchown -R oracle:oinstall /u011234567891011![](http://ouh6z710l.bkt.clouddn.com/WX20171118-143833@2x.png)## 配置系统参数（可选项）### 设置sysctl.conf``` bashvi /etc/sysctl.conf kernel.shmmni = 4096 kernel.sem = 2048 65536 1024 256 fs.file-max = 6815744 fs.aio-max-nr = 1048576 net.ipv4.ip_local_port_range = 9000 65500 net.core.rmem_default = 262144 net.core.rmem_max = 4194304 net.core.wmem_default = 262144 net.core.wmem_max = 1048576``` sysctl -p 使系统参数生效 ### 设置用户在资源限制``` bashvi /etc/security/limits.conf oracle soft nproc 65536 oracle hard nproc 65536 oracle soft nofile 65536 oracle hard nofile 65536 上传Oracle软件包使用SecureFX软件上传Oracle软件包，并进行解压，修改文件加属组1chown -R oracle:oinstall /opt/database 图形化安装Oracle软件重要运行xshell软件，使用Oracle用户登录服务器，执行runInstall的命令swap空间不足的解决办法 123456789101112131415[root@sannet opt]# dd if=/dev/zero of=/opt/swap bs=1M count=51205120+0 records in5120+0 records out5368709120 bytes (5.4 GB) copied, 14.335 s, 375 MB/s[root@sannet opt]# mkswap /opt/swapmkswap: /opt/swap: warning: don't erase bootbits sectors on whole disk. Use -f to force.Setting up swapspace version 1, size = 5242876 KiBno label, UUID=700ed992-329b-4767-bcf6-9c649d3b1895[root@sannet opt]# swapon /opt/swap[root@sannet opt]# free -m total used free shared buffers cachedMem: 3818 3670 147 0 7 3173-/+ buffers/cache: 488 3329Swap: 5119 0 5119 执行两个脚本，oracle数据库软件安装完成 图形化配置网络123[oracle@sannet ~]$ netcaOracle Net Services Configuration: 创建数据库 安装完成验证]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Oracle</tag>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kerberos安装]]></title>
    <url>%2F2017%2F11%2F08%2Fkerberos%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Kerberos服务]]></title>
    <url>%2F2017%2F11%2F07%2FKerberos%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[什么是kerberos“Kerberos 服务”是一种客户机/服务器体系结构，用于在网络上提供安全事务。该服务可提供功能强大的用户验证以及完整性和保密性。通过验证，可保证网络事务的发送者和接收者的身份真实。该服务还可以检验来回传递的数据的有效性（完整性），并在传输过程中对数据进行加密（保密性）。使用 Kerberos 服务，可以安全登录到其他计算机、执行命令、交换数据以及传输文件。此外，该服务还提供授权服务，这样，管理员便可限制对服务和计算机的访问。而且，作为 Kerberos 用户，您还可以控制其他用户对您帐户的访问。Kerberos 服务是单点登录系统，这意味着您对于每个会话只需要向服务进行一次自我验证，即可自动保护该会话过程中所有后续事务的安全。服务对您进行验证后，便无需在每次使用基于 Kerberos 的命令（如 ftp 或 rsh）或访问 NFS 文件系统上数据时都进行自我验证。因此，无需在每次使用这些服务时都在网络上发送口令（口令在网络上可能会被拦截）。 Kerberos 服务的工作原理Kerberos 系统的工作围绕票证的概念展开。票证(tickets)是一组标识用户或服务（如 NFS 服务）的电子信息。正如您的驾驶证可标识您的身份并表明您的驾驶级别一样，票证也可标识您的身份以及您的网络访问特权。执行基于 Kerberos 的事务时（例如，远程登录到另一台计算机），您将透明地向密钥分发中心 (Key Distribution Center, KDC) 发送票证请求。KDC 将访问数据库以验证您的身份，然后返回授予您访问其他计算机的权限的票证。“透明”意味着您无需显式请求票证。请求是在执行 rlogin 命令过程中进行的。因为只有通过验证的客户机可以获取特定服务的票证，所以其他客户机不能以虚假身份使用 rlogin。具体过程如下： 初始验证：票证授予票证Kerberos 验证分为两个阶段：允许进行后续验证的初始验证以及所有后续验证自身。 下图显示了如何进行初始验证。 客户机（用户或 NFS 等服务）通过从密钥分发中心 (Key Distribution Center, KDC) 请求票证授予票证 (ticket-granting ticket, TGT 后面简称票据) 开始 Kerberos 会话。此请求通常在登录时自动完成。 要获取特定服务的其他票证，需要票据。票据类似于护照。与护照一样，票据可标识您的身份并允许您获取多个“签证”，此处的“签证”（票证）不是用于外国，而是用于远程计算机或网络服务。与护照和签证一样，票据和其他各种票证具有有限的生命周期。区别在于基于 Kerberos 的命令会通知您拥有护照并为您取得签证。您不必亲自执行该事务。 与票据类似的另一种情况是可以在四个不同的滑雪场使用的三天滑雪入场卷。只要入场券未过期，您就可以在决定要去的任意一个滑雪场出示入场卷，并获取该滑雪场提供的缆车票。获取缆车票后，即可在该滑雪场随意滑雪。如果第二天去另一个滑雪场，您需要再次出示入场卷，并获取新滑雪场的另一张缆车票。区别在于基于 Kerberos 的命令会通知您拥有周末滑雪入场卷，并会为您取得缆车票。因此，您不必亲自执行该事务。 KDC 可创建票据，并采用加密形式将其发送回客户机。客户机使用其口令来解密票据。 拥有有效的票据后，只要该票据未过期，客户机便可以请求所有类型的网络操作（如 rlogin 或 telnet）的票证。此票证的有效期通常为几个小时。每次客户机执行唯一的网络操作时，都将从 KDC 请求该操作的票证。 后续 Kerberos 验证客户机收到初始验证后，每个后续验证都按下图所示的模式进行 客户机通过向 KDC 发送票据作为其身份证明，从 KDC 请求特定服务（例如，远程登录到另一台计算机）的票证。 KDC 将该特定服务的票证发送到客户机。 例如，假设用户 joe 要访问已通过要求的 krb5 验证共享的 NFS 文件系统。由于该用户已经通过了验证（即，该用户已经拥有票据），因此当其尝试访问文件时，NFS 客户机系统将自动透明地从 KDC 获取 NFS 服务的票证。 客户机将票证发送到服务器。使用 NFS 服务时，NFS 客户机会自动透明地将 NFS 服务的票证发送到 NFS 服务器。 服务器允许此客户机进行访问。 Kerberos 主体Kerberos 服务中的客户机由其主体标识。主体是 KDC 可以为其分配票证的唯一标识。主体可以是用户（如 joe）或服务（如 nfs 或 telnet）。 根据约定，主体名称分为三个部分：主名称、实例和领域。例如，典型的 Kerberos 主体可以是joe/admin@ENG.EXAMPLE.COM在此示例中：joe 是主名称。主名称可以是用户名（如此处所示）或服务（如 nfs）。主名称还可以是单词 host，这表示此主体是设置用于提供各种网络服务（如 ftp、rcp 和 rlogin 等）的服务主体。 admin 是实例。对于用户主体，实例是可选的；但对于服务主体，实例则是必需的。例如，如果用户 joe 有时充当系统管理员，则他可以使用 joe/admin 将其自身与其平时的用户身份区分开来。同样，如果 joe 在两台不同的主机上拥有帐户，则他可以使用两个具有不同实例的主体名称，例如 joe/denver.example.com 和 joe/boston.example.com。请注意，Kerberos 服务会将 joe 和 joe/admin 视为两个完全不同的主体。 对于服务主体，实例是全限定主机名。例如，bigmachine.eng.example.com 就是这种实例。此示例的主名称/实例可以为 ftp/bigmachine.eng.example.com 或 host/bigmachine.eng.example.com。 ENG.EXAMPLE.COM 是 Kerberos 领域。领域将在Kerberos 领域中介绍。 以下都是有效的主体名称： joe joe/admin joe/admin@ENG.EXAMPLE.COM nfs/host.eng.example.com@ENG.EXAMPLE.COM host/eng.example.com@ENG.EXAMPLE.COM #链接到 oracle.com Oracle Solaris 管理：安全服务 Oracle Solaris 11 Information Library (简体中文) Oracle 技术网 文档库 PDF 打印视图 反馈search filter icon搜索 search icon文档信息前言第 1 部分安全性概述 安全性服务（概述）第 2 部分系统、文件和设备安全性 管理计算机安全性（概述） 控制对系统的访问（任务） 病毒扫描服务（任务） 控制对设备的访问（任务） 使用基本审计报告工具（任务） 控制对文件的访问（任务）第 3 部分角色、权限配置文件和特权 使用角色和特权（概述） 使用基于角色的访问控制（任务） Oracle Solaris 中的安全属性（参考）第 4 部分加密服务 加密框架（概述） 加密框架（任务） 密钥管理框架第 5 部分验证服务和安全通信 网络服务验证（任务） 使用 PAM 使用 SASL 使用 安全 Shell（任务） 安全 Shell（参考）第 6 部分Kerberos 服务 Kerberos 服务介绍什么是 Kerberos 服务？Kerberos 服务的工作原理初始验证：票证授予票证后续 Kerberos 验证Kerberos 远程应用程序Kerberos 主体Kerberos 领域Kerberos 服务器Kerberos 安全服务各种 Kerberos 发行版的组件Kerberos 组件关于 Oracle Solaris 11 发行版中的 Kerberos 规划 Kerberos 服务 配置 Kerberos 服务（任务） Kerberos 错误消息和故障排除 管理 Kerberos 主体和策略（任务） 使用 Kerberos 应用程序（任务） Kerberos 服务（参考）第 7 部分在 Oracle Solaris 中审计 审计（概述） 规划审计 管理审计（任务） 审计（参考）词汇表索引Kerberos 服务的工作原理 下面概述了 Kerberos 验证系统。有关更详细的说明，请参见Kerberos 验证系统的工作原理。 从用户的角度来看，启动 Kerberos 会话后，Kerberos 服务通常不可见。一些命令（如 rsh 或 ftp）也是如此。初始化 Kerberos 会话通常仅包括登录和提供 Kerberos 口令。 Kerberos 系统的工作围绕票证的概念展开。票证是一组标识用户或服务（如 NFS 服务）的电子信息。正如您的驾驶证可标识您的身份并表明您的驾驶级别一样，票证也可标识您的身份以及您的网络访问特权。执行基于 Kerberos 的事务时（例如，远程登录到另一台计算机），您将透明地向密钥分发中心 (Key Distribution Center, KDC) 发送票证请求。KDC 将访问数据库以验证您的身份，然后返回授予您访问其他计算机的权限的票证。“透明”意味着您无需显式请求票证。请求是在执行 rlogin 命令过程中进行的。因为只有通过验证的客户机可以获取特定服务的票证，所以其他客户机不能以虚假身份使用 rlogin。 票证具有与之关联的特定属性。例如，票证可以是可转发的，这意味着它可以在其他计算机上使用，而不必进行新的验证。票证也可以是以后生效的，这意味着它要到指定时间后才会生效。票证的使用方式（例如，如何指定允许哪些用户获取哪些类型的票证）由策略设置。策略在安装或管理 Kerberos 服务时确定。 注 - 您可能会经常看到术语凭证和票证。在更为广泛的 Kerberos 范围内，两者通常可互换使用。但是，从技术上讲，凭证指的是票证和会话的会话密钥。使用 Kerberos 获取服务访问权限中对此区别进行了更详细的说明。 以下各节将进一步说明 Kerberos 验证过程。 初始验证：票证授予票证Kerberos 验证分为两个阶段：允许进行后续验证的初始验证以及所有后续验证自身。 下图显示了如何进行初始验证。 图 19-1 Kerberos 会话的初始验证 image:流程图显示了客户机从 KDC 请求 TGT，然后对 KDC 返回到客户机的 TGT 进行解密。客户机（用户或 NFS 等服务）通过从密钥分发中心 (Key Distribution Center, KDC) 请求票证授予票证 (ticket-granting ticket, TGT) 开始 Kerberos 会话。此请求通常在登录时自动完成。 要获取特定服务的其他票证，需要票证授予票证。票证授予票证类似于护照。与护照一样，票证授予票证可标识您的身份并允许您获取多个“签证”，此处的“签证”（票证）不是用于外国，而是用于远程计算机或网络服务。与护照和签证一样，票证授予票证和其他各种票证具有有限的生命周期。区别在于基于 Kerberos 的命令会通知您拥有护照并为您取得签证。您不必亲自执行该事务。 与票证授予票证类似的另一种情况是可以在四个不同的滑雪场使用的三天滑雪入场卷。只要入场券未过期，您就可以在决定要去的任意一个滑雪场出示入场卷，并获取该滑雪场提供的缆车票。获取缆车票后，即可在该滑雪场随意滑雪。如果第二天去另一个滑雪场，您需要再次出示入场卷，并获取新滑雪场的另一张缆车票。区别在于基于 Kerberos 的命令会通知您拥有周末滑雪入场卷，并会为您取得缆车票。因此，您不必亲自执行该事务。 KDC 可创建票证授予票证，并采用加密形式将其发送回客户机。客户机使用其口令来解密票证授予票证。 拥有有效的票证授予票证后，只要该票证授予票证未过期，客户机便可以请求所有类型的网络操作（如 rlogin 或 telnet）的票证。此票证的有效期通常为几个小时。每次客户机执行唯一的网络操作时，都将从 KDC 请求该操作的票证。 后续 Kerberos 验证客户机收到初始验证后，每个后续验证都按下图所示的模式进行。 图 19-2 使用 Kerberos 验证获取对服务的访问权 image:流程图显示了客户机使用 TGT 从 KDC 请求票证，然后使用返回的票证访问服务器。客户机通过向 KDC 发送其票证授予票证作为其身份证明，从 KDC 请求特定服务（例如，远程登录到另一台计算机）的票证。 KDC 将该特定服务的票证发送到客户机。 例如，假设用户 joe 要访问已通过要求的 krb5 验证共享的 NFS 文件系统。由于该用户已经通过了验证（即，该用户已经拥有票证授予票证），因此当其尝试访问文件时，NFS 客户机系统将自动透明地从 KDC 获取 NFS 服务的票证。 例如，假设用户 joe 在服务器 boston 上使用 rlogin。由于该用户已经通过了验证（即，该用户已经拥有票证授予票证），所以在运行 rlogin 命令时，该用户将自动透明地获取票证。该用户使用此票证可随时远程登录到 boston，直到票证到期为止。如果 joe 要远程登录到计算机 denver，则需要按照步骤 1 获取另一个票证。 客户机将票证发送到服务器。 使用 NFS 服务时，NFS 客户机会自动透明地将 NFS 服务的票证发送到 NFS 服务器。 服务器允许此客户机进行访问。 从这些步骤来看，服务器似乎并未与 KDC 通信。但服务器实际上与 KDC 进行了通信，并向 KDC 注册了其自身，正如第一台客户机所执行的操作。为简单起见，该部分已省略。 Kerberos 远程应用程序用户（如 joe）可以使用的基于 Kerberos 的（即 “Kerberized”）命令包括： ftp rcp rlogin rsh ssh telnet 这些应用程序与同名的 Solaris 应用程序相同。但是，它们已扩展为使用 Kerberos 主体来验证事务，因此会提供基于 Kerberos 的安全性。有关主体的信息，请参见Kerberos 主体。 Kerberos 用户命令中将进一步介绍这些命令。 Kerberos 主体Kerberos 服务中的客户机由其主体标识。主体是 KDC 可以为其分配票证的唯一标识。主体可以是用户（如 joe）或服务（如 nfs 或 telnet）。 根据约定，主体名称分为三个部分：主名称、实例和领域。例如，典型的 Kerberos 主体可以是 joe/admin@ENG.EXAMPLE.COM。在此示例中： joe 是主名称。主名称可以是用户名（如此处所示）或服务（如 nfs）。主名称还可以是单词 host，这表示此主体是设置用于提供各种网络服务（如 ftp、rcp 和 rlogin 等）的服务主体。 admin 是实例。对于用户主体，实例是可选的；但对于服务主体，实例则是必需的。例如，如果用户 joe 有时充当系统管理员，则他可以使用 joe/admin 将其自身与其平时的用户身份区分开来。同样，如果 joe 在两台不同的主机上拥有帐户，则他可以使用两个具有不同实例的主体名称，例如 joe/denver.example.com 和 joe/boston.example.com。请注意，Kerberos 服务会将 joe 和 joe/admin 视为两个完全不同的主体。 对于服务主体，实例是全限定主机名。例如，bigmachine.eng.example.com 就是这种实例。此示例的主名称/实例可以为 ftp/bigmachine.eng.example.com 或 host/bigmachine.eng.example.com。 ENG.EXAMPLE.COM 是 Kerberos 领域。领域将在Kerberos 领域中介绍。 以下都是有效的主体名称： joe joe/admin joe/admin@ENG.EXAMPLE.COM nfs/host.eng.example.com@ENG.EXAMPLE.COM host/eng.example.com@ENG.EXAMPLE.COM Kerberos 领域领域是定义属于同一主 KDC 的一组系统的逻辑网络，类似于域。下图显示了各领域相互之间的关系。有些领域是层次化的，其中，一个领域是另一个领域的超集。另外一些领域是非层次化（或“直接”）的，必须定义两个领域之间的映射。Kerberos 服务的一种功能是它允许进行跨领域验证。每个领域只需在其 KDC 中有对应于另一个领域的主体项即可。此 Kerberos 功能称为跨领域验证。 Kerberos 服务器每个领域都必须包括一台用于维护主体数据库主副本的服务器。此服务器称为主 KDC 服务器。此外，每个领域还应至少包含一台从 KDC 服务器，该服务器包含主体数据库的多个副本。主 KDC 服务器和从 KDC 服务器都可创建用于建立验证的票证。下图显示了一个假设的领域可能包含的内容。]]></content>
      <categories>
        <category>kerberos</category>
      </categories>
      <tags>
        <tag>kerberos</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs 读取数据流程]]></title>
    <url>%2F2017%2F10%2F25%2Fhdfs-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Namenode HA详解]]></title>
    <url>%2F2017%2F10%2F16%2FNamenode-HA%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[NameNode HA架构概述在 Hadoop 1.0 时代，Hadoop 的两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，这其中以 NameNode 的单点问题尤为严重。因为 NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。 所幸的是，在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。HDFS NameNode 和 YARN ResourceManger 的高可用 (High Availability，HA) 方案基本类似，两者也复用了部分代码，但是由于 HDFS NameNode 对于数据存储和数据一致性的要求比 YARN ResourceManger 高得多，所以 HDFS NameNode 的高可用实现更为复杂一些，本文从内部实现的角度对 HDFS NameNode 的高可用机制进行详细的分析。从上图中，我们可以看出 NameNode 的高可用架构主要分为下面几个部分： Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。 主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。 共享存储系统Journalnode：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 NameNode 的主备切换NameNode 主备切换主要由 ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现： ZKFailoverController 作为 NameNode 机器上一个独立的进程启动 (在 hdfs 启动脚本之中的进程名为 zkfc)，启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，也会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调方法。 HealthMonitor 主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举。 ActiveStandbyElector 主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。 具体步骤如下： HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。 HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。 如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。 ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。 ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。 ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。 HealthMonitor实现分析ZKFailoverController 在初始化的时候会创建 HealthMonitor，HealthMonitor 在内部会启动一个线程来循环调用 NameNode 的 HAServiceProtocol RPC 接口的方法来检测 NameNode 的状态，并将状态的变化通过回调的方式来通知 ZKFailoverController。 HealthMonitor 主要检测 NameNode 的两类状态，分别是 HealthMonitor.State 和 HAServiceStatus。HealthMonitor.State 是通过 HAServiceProtocol RPC 接口的 monitorHealth 方法来获取的，反映了 NameNode 节点的健康状况，主要是磁盘存储资源是否充足。HealthMonitor.State 包括下面几种状态： INITIALIZING：HealthMonitor 在初始化过程中，还没有开始进行健康状况检测； SERVICE_HEALTHY：NameNode 状态正常； SERVICE_NOT_RESPONDING：调用 NameNode 的 monitorHealth 方法调用无响应或响应超时； SERVICE_UNHEALTHY：NameNode 还在运行，但是 monitorHealth 方法返回状态不正常，磁盘存储资源不足； HEALTH_MONITOR_FAILED：HealthMonitor 自己在运行过程中发生了异常，不能继续检测 NameNode 的健康状况，会导致 ZKFailoverController 进程退出； HealthMonitor.State 在状态检测之中起主要的作用，在 HealthMonitor.State 发生变化的时候，HealthMonitor 会回调 ZKFailoverController 的相应方法来进行处理。 而 HAServiceStatus 则是通过 HAServiceProtocol RPC 接口的 getServiceStatus 方法来获取的，主要反映的是 NameNode 的 HA 状态，包括： INITIALIZING：NameNode 在初始化过程中； ACTIVE：当前 NameNode 为主 NameNode； STANDBY：当前 NameNode 为备 NameNode； STOPPING：当前 NameNode 已停止； HAServiceStatus 在状态检测之中只是起辅助的作用，在 HAServiceStatus 发生变化时，HealthMonitor 也会回调 ZKFailoverController 的相应方法来进行处理。 ActiveStandbyElector 实现分析Namenode(包括 YARN ResourceManager) 的主备选举是通过 ActiveStandbyElector 来完成的，ActiveStandbyElector 主要是利用了 Zookeeper 的写一致性和临时节点机制，具体的主备选举实现如下： 创建锁节点如果 HealthMonitor 检测到对应的 NameNode 的状态正常，那么表示这个 NameNode 有资格参加 Zookeeper 的主备选举。如果目前还没有进行过主备选举的话，那么相应的 ActiveStandbyElector 就会发起一次主备选举，尝试在 Zookeeper 上创建一个路径为/hadoop-ha/{dfs.nameservices}/ActiveStandbyElectorLock 的临时节点({dfs.nameservices} 为 Hadoop 的配置参数 dfs.nameservices 的值，下同)，Zookeeper 的写一致性会保证最终只会有一个 ActiveStandbyElector 创建成功，那么创建成功的 ActiveStandbyElector 对应的 NameNode 就会成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Active 状态。而创建失败的 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Standby 状态。Zookeeper中的znode信息 1234567891011121314[zk: localhost:2181(CONNECTED) 6] get /hadoop-ha/hadoopcluster/ActiveStandbyElectorLockhadoopclusternn1hc1 cZxid = 0xc0035e875ctime = Mon Oct 16 20:58:32 CST 2017mZxid = 0xc0035e875mtime = Mon Oct 16 20:58:32 CST 2017pZxid = 0xc0035e875cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x15e3a70f41170c2dataLength = 31numChildren = 0 注册 Watcher 监听不管创建/hadoop-ha/{dfs.nameservices}/ActiveStandbyElectorLock节点是否成功，ActiveStandbyElector 随后都会向 Zookeeper 注册一个 Watcher 来监听这个节点的状态变化事件，ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件。 自动触发主备选举如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点/hadoop-ha/{dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建/hadoop-ha/{dfs.nameservices}/ActiveStandbyElectorLock 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。 当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/{dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。 防止脑裂Zookeeper 在工程实践的过程中经常会发生的一个现象就是 Zookeeper 客户端“假死”，所谓的“假死”是指如果 Zookeeper 客户端机器负载过高或者正在进行 JVM Full GC，那么可能会导致 Zookeeper 客户端到 Zookeeper 服务端的心跳不能正常发出，一旦这个时间持续较长，超过了配置的 Zookeeper Session Timeout 参数的话，Zookeeper 服务端就会认为客户端的 session 已经过期从而将客户端的 Session 关闭。“假死”有可能引起分布式系统常说的双主或脑裂 (brain-split) 现象。 具体到本文所述的 NameNode，假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 对应的 ZKFailoverController 进程发生了“假死”现象，那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，即使随后 NameNode1 对应的 ZKFailoverController 因为负载下降或者 Full GC 结束而恢复了正常，感知到自己和 Zookeeper 的 Session 已经关闭，但是由于网络的延迟以及 CPU 线程调度的不确定性，仍然有可能会在接下来的一段时间窗口内 NameNode1 认为自己还是处于 Active 状态。 这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况对于 NameNode 这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。Zookeeper 社区对这种问题的解决方法叫做 fencing，中文翻译为隔离，也就是想办法把旧的 Active NameNode 隔离起来，使它不能正常对外提供服务。 ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/{dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为/hadoop-ha/{dfs.nameservices}/ActiveBreadCrumb 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息。 Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候 (注意由于/hadoop-ha/{dfs.nameservices}/ActiveStandbyElectorLock 是临时节点，也会随之删除)，会一起删除节点/hadoop-ha/{dfs.nameservices}/ActiveBreadCrumb。 但是如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于/hadoop-ha/{dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来。后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing，具体处理见后文 ZKFailoverController 部分所述。 ZKFailoverController 实现分析ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调函数，ZKFailoverController 的处理逻辑主要靠 HealthMonitor 和 ActiveStandbyElector 的回调函数来驱动。 对 HealthMonitor 状态变化的处理如前所述，HealthMonitor 会检测 NameNode 的两类状态，HealthMonitor.State 在状态检测之中起主要的作用，ZKFailoverController 注册到 HealthMonitor 上的处理 HealthMonitor.State 状态变化的回调函数主要关注 SERVICE_HEALTHY、SERVICE_NOT_RESPONDING 和 SERVICE_UNHEALTHY 这 3 种状态： 如果检测到状态为 SERVICE_HEALTHY，表示当前的 NameNode 有资格参加 Zookeeper 的主备选举，如果目前还没有进行过主备选举的话，ZKFailoverController 会调用 ActiveStandbyElector 的 joinElection 方法发起一次主备选举。 如果检测到状态为 SERVICE_NOT_RESPONDING 或者是 SERVICE_UNHEALTHY，就表示当前的 NameNode 出现问题了，ZKFailoverController 会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举，这样其它的 NameNode 就有机会成为主 NameNode。 而 HAServiceStatus 在状态检测之中仅起辅助的作用，在 HAServiceStatus 发生变化时，ZKFailoverController 注册到 HealthMonitor 上的处理 HAServiceStatus 状态变化的回调函数会判断 NameNode 返回的 HAServiceStatus 和 ZKFailoverController 所期望的是否一致，如果不一致的话，ZKFailoverController 也会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举。 对 ActiveStandbyElector 主备选举状态变化的处理在 ActiveStandbyElector 的主备选举状态发生变化时，会回调 ZKFailoverController 注册的回调函数来进行相应的处理： 如果 ActiveStandbyElector 选主成功，那么 ActiveStandbyElector 对应的 NameNode 成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeActive 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToActive 方法，将 NameNode 转换为 Active 状态。 如果 ActiveStandbyElector 选主失败，那么 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeStandby 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，将 NameNode 转换为 Standby 状态。 如果 ActiveStandbyElector 选主成功之后，发现了上一个 Active NameNode 遗留下来的/hadoop-ha/{dfs.nameservices}/ActiveBreadCrumb 节点 ，那么 ActiveStandbyElector 会首先回调 ZKFailoverController 注册的 fenceOldActive 方法，尝试对旧的 Active NameNode 进行 fencing，在进行 fencing 的时候，会执行以下的操作： 首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。 如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence： sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死； shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离； 只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。 NameNode 的共享存储实现(Journalnode)过去几年中 Hadoop 社区涌现过很多的 NameNode 共享存储方案，比如 shared NAS+NFS、BookKeeper、BackupNode 和 QJM(Quorum Journal Manager) 等等。目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。为了理解 QJM 的设计和实现，首先要对 NameNode 的元数据存储结构有所了解。 NameNode 的元数据存储概述一个典型的 NameNode 的元数据存储目录结构如图所示NameNode 在执行 HDFS 客户端提交的创建文件或者移动文件这样的写操作的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog 仅仅只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment。 正在写入的 EditLog Segment 处于 in-progress 状态，其文件名形如 edits_inprogress_start_txid，其中start_txid 表示这个 segment 的起始事务 id，例如上图中的 edits_inprogress_0000000000000000020。而已经写入完成的 EditLog Segment 处于 finalized 状态，其文件名形如 edits_start_txid-end_txid，其中start_txid 表示这个 segment 的起始事务 id，end_txid 表示这个 segment 的结束事务 id，例如上图中的 edits_0000000000000000001-0000000000000000019。 NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，FSImage 文件的文件名形如 fsimage_${end_txid}，其中${end_txid} 表示这个 fsimage 文件的结束事务 id，例如上图中的 fsimage_0000000000000000020。在 NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。 基于QJM的共享存储系统的总体架构基于 QJM 的共享存储系统主要用于保存 EditLog，并不保存 FSImage 文件。FSImage 文件还是在 NameNode 的本地磁盘上。QJM 共享存储的基本思想来自于 Paxos 算法 ，采用多个称为 JournalNode 的节点组成的 JournalNode 集群来存储 EditLog。每个 JournalNode 保存同样的 EditLog 副本。每次 NameNode 写 EditLog 的时候，除了向本地磁盘写入 EditLog 之外，也会并行地向 JournalNode 集群之中的每一个 JournalNode 发送写请求，只要大多数 (majority) 的 JournalNode 节点返回成功就认为向 JournalNode 集群写入 EditLog 成功。如果有 2N+1 台 JournalNode，那么根据大多数的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。 FSEditLog：这个类封装了对 EditLog 的所有操作，是 NameNode 对 EditLog 的所有操作的入口。 JournalSet： 这个类封装了对本地磁盘和 JournalNode 集群上的 EditLog 的操作，内部包含了两类 JournalManager：一类为 FileJournalManager，用于实现对本地磁盘上 EditLog 的操作。一类为 QuorumJournalManager，用于实现对 JournalNode 集群上共享目录的 EditLog 的操作。FSEditLog 只会调用 JournalSet 的相关方法，而不会直接使用 FileJournalManager 和 QuorumJournalManager。 FileJournalManager：封装了对本地磁盘上的 EditLog 文件的操作，不仅 NameNode 在向本地磁盘上写入 EditLog 的时候使用 FileJournalManager，JournalNode 在向本地磁盘写入 EditLog 的时候也复用了 FileJournalManager 的代码和逻辑。 QuorumJournalManager：封装了对 JournalNode 集群上的 EditLog 的操作，它会根据 JournalNode 集群的 URI 创建负责与 JournalNode 集群通信的类 AsyncLoggerSet， QuorumJournalManager 通过 AsyncLoggerSet 来实现对 JournalNode 集群上的 EditLog 的写操作，对于读操作，QuorumJournalManager 则是通过 Http 接口从 JournalNode 上的 JournalNodeHttpServer 读取 EditLog 的数据。 AsyncLoggerSet：内部包含了与 JournalNode 集群进行通信的 AsyncLogger 列表，每一个 AsyncLogger 对应于一个 JournalNode 节点，另外 AsyncLoggerSet 也包含了用于等待大多数 JournalNode 返回结果的工具类方法给 QuorumJournalManager 使用。 AsyncLogger：具体的实现类是 IPCLoggerChannel，IPCLoggerChannel 在执行方法调用的时候，会把调用提交到一个单线程的线程池之中，由线程池线程来负责向对应的 JournalNode 的 JournalNodeRpcServer 发送 RPC 请求。 JournalNodeRpcServer：运行在 JournalNode 节点进程中的 RPC 服务，接收 NameNode 端的 AsyncLogger 的 RPC 请求。 JournalNodeHttpServer：运行在 JournalNode 节点进程中的 Http 服务，用于接收处于 Standby 状态的 NameNode 和其它 JournalNode 的同步 EditLog 文件流的请求。 基于 QJM 的共享存储系统的数据同步机制分析Active NameNode 和 StandbyNameNode 使用 JouranlNode 集群来进行数据同步的过程如图所示，Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog： Active NameNode 提交 EditLog 到 JournalNode 集群当处于 Active 状态的 NameNode 调用 FSEditLog 类的 logSync 方法来提交 EditLog 的时候，会通过 JouranlSet 同时向本地磁盘目录和 JournalNode 集群上的共享存储目录写入 EditLog。写入 JournalNode 集群是通过并行调用每一个 JournalNode 的 QJournalProtocol RPC 接口的 journal 方法实现的，如果对大多数 JournalNode 的 journal 方法调用成功，那么就认为提交 EditLog 成功，否则 NameNode 就会认为这次提交 EditLog 失败。提交 EditLog 失败会导致 Active NameNode 关闭 JournalSet 之后退出进程，留待处于 Standby 状态的 NameNode 接管之后进行数据恢复。 从上面的叙述可以看出，Active NameNode 提交 EditLog 到 JournalNode 集群的过程实际上是同步阻塞的，但是并不需要所有的 JournalNode 都调用成功，只要大多数 JournalNode 调用成功就可以了。如果无法形成大多数，那么就认为提交 EditLog 失败，NameNode 停止服务退出进程。如果对应到分布式系统的 CAP 理论的话，虽然采用了 Paxos 的“大多数”思想对 C(consistency，一致性) 和 A(availability，可用性) 进行了折衷，但还是可以认为 NameNode 选择了 C 而放弃了 A，这也符合 NameNode 对数据一致性的要求。 Standby NameNode 从 JournalNode 集群同步 EditLog当 NameNode 进入 Standby 状态之后，会启动一个 EditLogTailer 线程。这个线程会定期调用 EditLogTailer 类的 doTailEdits 方法从 JournalNode 集群上同步 EditLog，然后把同步的 EditLog 回放到内存之中的文件系统镜像上 (并不会同时把 EditLog 写入到本地磁盘上)。 这里需要关注的是：从 JournalNode 集群上同步的 EditLog 都是处于 finalized 状态的 EditLog Segment。“NameNode 的元数据存储概述”一节说过 EditLog Segment 实际上有两种状态，处于 in-progress 状态的 Edit Log 当前正在被写入，被认为是处于不稳定的中间态，有可能会在后续的过程之中发生修改，比如被截断。Active NameNode 在完成一个 EditLog Segment 的写入之后，就会向 JournalNode 集群发送 finalizeLogSegment RPC 请求，将完成写入的 EditLog Segment finalized，然后开始下一个新的 EditLog Segment。一旦 finalizeLogSegment 方法在大多数的 JournalNode 上调用成功，表明这个 EditLog Segment 已经在大多数的 JournalNode 上达成一致。一个 EditLog Segment 处于 finalized 状态之后，可以保证它再也不会变化。 从上面描述的过程可以看出，虽然 Active NameNode 向 JournalNode 集群提交 EditLog 是同步的，但 Standby NameNode 采用的是定时从 JournalNode 集群上同步 EditLog 的方式，那么 Standby NameNode 内存中文件系统镜像有很大的可能是落后于 Active NameNode 的，所以 Standby NameNode 在转换为 Active NameNode 的时候需要把落后的 EditLog 补上来。 基于 QJM 的共享存储系统的数据恢复机制分析处于 Standby 状态的 NameNode 转换为 Active 状态的时候，有可能上一个 Active NameNode 发生了异常退出，那么 JournalNode 集群中各个 JournalNode 上的 EditLog 就可能会处于不一致的状态，所以首先要做的事情就是让 JournalNode 集群中各个节点上的 EditLog 恢复为一致。另外如前所述，当前处于 Standby 状态的 NameNode 的内存中的文件系统镜像有很大的可能是落后于旧的 Active NameNode 的，所以在 JournalNode 集群中各个节点上的 EditLog 达成一致之后，接下来要做的事情就是从 JournalNode 集群上补齐落后的 EditLog。只有在这两步完成之后，当前新的 Active NameNode 才能安全地对外提供服务。 补齐落后的 EditLog 的过程复用了前面描述的 Standby NameNode 从 JournalNode 集群同步 EditLog 的逻辑和代码，最终调用 EditLogTailer 类的 doTailEdits 方法来完成 EditLog 的补齐。使 JournalNode 集群上的 EditLog 达成一致的过程是一致性算法 Paxos 的典型应用场景，QJM 对这部分的处理可以看做是 Single Instance Paxos算法的一个实现，在达成一致的过程中，Active NameNode 和 JournalNode 集群之间的交互流程如图所示 使用Epoch进行隔离Epoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制，为什么需要 fencing 已经在前面“ActiveStandbyElector 实现分析”一节的“防止脑裂”部分进行了说明。产生新 Epoch 的流程与 Zookeeper 的 ZAB(Zookeeper Atomic Broadcast) 协议在进行数据恢复之前产生新 Epoch 的过程完全类似： Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。 每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功。 在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Journalnode详解]]></title>
    <url>%2F2017%2F10%2F16%2FJournalnode%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH5安装]]></title>
    <url>%2F2017%2F10%2F13%2FCDH5%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[集群环境123456789106台8核16G的云主机操作系统 CentOS 6.510.139.4.115 kiwi01.novalocal10.139.4.120 kiwi02.novalocal10.139.4.133 kiwi03.novalocal10.139.4.136 kiwi04.novalocal10.139.4.146 kiwi05.novalocal10.139.4.148 kiwi06.novalocal其中10.139.4.115作为CDM的管理主机 安装步骤获取CDH的YUM源1234567891011wget http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/cloudera-manager.repowget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.binyum makecacheyum list | grep clouderacloudera-manager-agent.x86_64 5.13.0-1.cm5130.p0.55.el6 cloudera-managercloudera-manager-daemons.x86_64 5.13.0-1.cm5130.p0.55.el6 cloudera-managercloudera-manager-server.x86_64 5.13.0-1.cm5130.p0.55.el6 cloudera-managercloudera-manager-server-db-2.x86_64 5.13.0-1.cm5130.p0.55.el6 cloudera-managerenterprise-debuginfo.x86_64 5.13.0-1.cm5130.p0.55.el6 cloudera-managerjdk.x86_64 2000:1.6.0_31-fcs cloudera-manageroracle-j2sdk1.7.x86_64 1.7.0+update67-1 cloudera-manager 安装CDM的Server端12chmod u+x cloudera-manager-installer.bin./cloudera-manager-installer.bin 在安装cloudera manager server的时候，直接安装的时候，速度比较慢，我们可以先把相关的软件先下载下来手动安装好，然后再启动CM相关的软件的下载路径如下：http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5.13/RPMS/x86_64/出现这样的页面代表这cloudera的大数据平台的管理软件已经安装完成了我们可以通过登录，用户名admin 登录密码admin http://10.139.4.115:7180/cmf/login 安装agent端输入集群中的所有主机选择默认不选择单用户模式配置主机信息开始安装 主机检查相关配置存在两个问题 Cloudera 建议将 /proc/sys/vm/swappiness 设置为最大值 10。当前设置为 60。使用 sysctl 命令在运行时更改该设置并编辑 /etc/sysctl.conf在最后一行添加vm.swappiness=10 已启用透明大页面压缩，可能会导致重大性能问题。在所有的节点运行echo never &gt; /sys/kernel/mm/transparent_hugepage/defragecho never &gt; /sys/kernel/mm/transparent_hugepage/enabled以禁用此设置，然后将同一命令添加到 /etc/rc.local 等初始化脚本中，以便在系统重启时予以设置。 最终确保所有的问题全部解决，继续进行安装选择安装的组件需要注意的是，很多的组件需要配置后台的数据库我们一般在生产上只有MySQL最后后台的源数据库，不推荐使用嵌入的postgreSQL数据库MySQL相关创建数据库的命令如下：1234CREATE DATABASE amon DEFAULT CHARACTER SET utf8;grant all privileges on amon.* to amon@'localhost' identified by 'amon';grant all privileges on amon.* to amon@'%' identified by 'amon';grant all privileges on rman.* to amon@'kiwi01.novalocal' identified by 'amon'; 测试连接的时候，可能会遇到：JDBC driver cannot be found. Unable to find the JDBC database jar on host解决方案：yum install mysql-connector-java -y最终，大数据集群安装完成]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>CDH</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F10%2F11%2FHControl%E7%AE%A1%E7%90%86%E5%91%98%E6%89%8B%E5%86%8C1.3.5%2F</url>
    <content type="text"><![CDATA[[TOC] 0 概述本文档是针对HControl下stack1.3.5所著。 1 关于本手册HControl是BC-Hadoop及大云大数据套件的管理软件，主要作用是能够自动化部署、启停、配置、监控整个集群，集群里的服务可以包括：HDFS、YARN、Mapreduce2、Spark、Tez、Metrics、Hive、HBase、Sqoop2、ZooKeeper、Storm、Impala、Flume、Kafka、Kerberos、Mahout、Ntp、Ranger、LDAP等。本文档主要针对管理员用户，详细介绍如何使用HControl来进行集群的操作和管理，以及对部署过程中常见的问题进行了描述。 2 系统运行环境与安装安装部署HControl之前，你必须了解所需要的环境，这包括每一台机器所需要的硬件环境及软件环境。 2.1 硬件环境 软件名称 要求 CPU 2GHZ，8核 内存 16G 硬盘 基本配置接口 网络 千兆网络 节点数 4（测试ha至少4台） 2.2 软件环境 硬件名称 要求 操作系统 centos/rhel6.5或是bclinux 软件源YUM 内网：10.254.2.95；外网：223.105.0.153 JDK 1.7或者1.8版本 2.3 软件的安装2.3.1 yum源配置 内网访问地址：10.128.2.95（例如安装hcontrol1.3.5的版本，地址为: http://10.128.2.95/bch/1.3.5/centos/6.8/x86_64)【虚拟机访问http地址为10.128.2.95；物理机访问http地址为10.254.2.95】 外网访问地址：223.105.0.153:882.3.2 安装前准备 集群机器要求: 确保集群各机器为英文操作系统.目前支持CentOS 6.5和Red Hat Enterprise Linux6.5 JDK版本为1.7版 OpenSSL版本为1.6 以上 Python版本为2.6.6或者以上 确保集群各机器root密码一致 规划要求： 在部署前，请先做好规划，各节点部署哪种组件。 主机要求： 确认/etc/hosts包含所有主机； 关闭防火墙：service iptables stop 关闭selinux: setenforce 0 检查openssl版本：至少为30 禁用THP。2.3.3 安装HControl 配置repo源（用root用户登录） 下载bch-centos.repo文件cd /etc/yum.repos.d/ 物理机执行：wget http://{repo_address}/bch/1.3.5/centos/6.8/x86_64/bch-centos.repo 虚拟机执行:wget http://{repo_address}/bch/1.3.5/centos/6.8/x86_64/bch-centos-virtual.repo 注意：（该目录下/etc/yum.repos.d/目录下请保证只有bch的相关repo包） 配置bch-centos.repo修改bch-centtos.repo中的ip地址为{repo_address}配置的值。 检查仓库配置是否正确命令：yum repolist若显示bch-centos-1.3.5 134（该数字不为0），说明配置OK；否则请检查bch-centos.repo配置 安装HControl-server 在需要安装HControl-Server的主节点上执行yum install ambari-server； 修改ambari-properties配置文件：/etc/ambari-server/conf/ambari.properties；配置jdk的下载地址为 ；配置api.csrfPrevention.allowed.uris=http://localhost:8080,http://{server-IP}:8080/,http://bdoc.com/；其中{servre-IP}是ambari-server所在主机的iP地址； 执行命令：ambar-server setup，根据向导选择默认值即可； 启动：ambari-server start 安装ambari-agent 在集群所有节点上（包括HControl-Server）执行yum install ambari-agent安装HControl-Agent； 在所有节点修改ambari-agent.ini配置文件：/etc/ambari-agent/conf/ambari-agent.ini； 修改其中的hostname为：hcontrol-server所在的机器的主机名； 启动agent：在所有节点执行命令ambari-agent start。###2.3.4 部署HControl在浏览器中输入{ Your.HControl.Server.IP }:8080登录到HControl Server页面，在登录表单中输入用户名和密码admin/HCq1w2E#R$，登录到HControl部署控制台。 输入集群名称； 选择stack1.3.5,输入地址http://{repo_address}/bch/1.3.5/centos/6.8/x86_64/（修改{repo_address}为真实yum地址）； 输入主机名（已回车隔开） 根据页面提示注册agent; 选择安装的服务； 根据提示填写对应的属性（比如hive、ranger需要填写数据库密码等） 检查主机服务是否在正确主机； 完成部署 2.4软件的卸载卸载请参考《HC安装文档》中卸载篇章 3 系统功能介绍功能介绍分三个角色，分别介绍： 3.1 管理员用户3.1 用户管理 权限管理功能，HControl管理员登录之后，可以对当前所有用户、用户组、以及权限进行管理修改。 创建管理员，管理员用户拥有操作和查看HControl的权限。 创建普通用户，普通用户只有查看HControl的权限。 管理员用户可对用户、用户组、以及权限进行修改管理。 3.2 仪表盘管理 仪表盘添加删除。 仪表盘编辑。 历史配置，实现配置的高效管理。 heatmap，可全局设置警告颜色显示。 3.3 服务管理 服务添加删除，该功能可以实现对HControl下所有服务的删除和添加，在使用的时候会更加灵活。 服务启停，该功能实现对服务状态的控制。 服务检查，该功能可以实现对服务状态的检查。 删除组件，通过组件删除，实现组件位置的灵活调配。 启动HA，启动HA后将实现集群的高可用，提高集群的安全性。 服务配置文件管理，实现各个服务配置信息的界面化管理。 开启维护模式。 服务快速链接，实现相关链接的界面化跳转，体验更好 3.4 主机管理 组件启停。 节点筛选功能，在节点较多的情况下实现节点快速查找。 节点添加删除，实现节点灵活配置 3.5 告警管理 邮件提醒，实现邮件告警，方便用户运维。 仪表盘监控，实现对各个组件以及服务器性能指标的监控 3.6 审计日志管理 hc-server升级日志管理 yarn审计日志的管理； hive审计日志的管理； hbase审计日志的管理。 3.2 普通用户普通用户仅具有登录查看服务的功能，无法对服务进行操作。 3.3 特殊用户特殊用户，是指管理员用户通过用户管理对该用户进行一定的权限配置，从而可以对集群进行相关操作的用户。 3.4 运维工作者概述：主要针对运维人员，详细介绍HControl运维过程中的巡检操作、监控告警信息，并对其间遇到的一些问题进行阐述。具体操作操作请查阅《132之运维文档》 4 操作指南4.1 用户管理1、登录操作（1）在浏览器中输入http://{Your.HControl.Server.IP}:8080，进入管理员登录页面；（2）在登录表单中输入用户名和密码，默认均为admin，点击提交完成管理员登录，登录页面如下：2、修改用户权限（1）作为管理员进入管理员页面；（2）点击页面右侧Admin下拉菜单中的“管理HControl”；（3）选择左侧导航栏中“权限”，可看到页面有operator和read-only两种角色；（4） 若想对某个用户赋予某种权限只需要把该用户拉入该角色当中即可。3、管理用户（1）用管理员身份登录到HControl平台；（2）选择导航栏的Admin来查看HControl Web用户，点击Service Accounts查看用户及用户组信息。4、设置用户状态（1）进入管理员界面并点击“用户”；（2）点击需要修改的用户的用户名；（3）点击status状态下拉框，并选择想赋予该用户的状态；5、创建本地组（1）进入管理员界面并点击点击“群组”；（2）点击创建groups名；（3）输入一个唯一的groups名；（4）点击保存groups，操作页面如下图所示6、添加用户到组当中（1）进入管理页面，点击“群组”，进入群组管理页面；（2）点击需要修改的组名；（3）点击Local Members当中的输入框，并输入一个存在的用户名；（4）点击保存按钮，即完成了对用户的添加，页面如下图所示：7、编辑组当中的用户（1）进入管理页面，点击Groups接口进入Group管理页面；（2）点击需要修改的组名；（3）点击铅笔图片后，该组当中的用户都会变为可编辑状态，在输入框中完成编辑或完成对用户的删除操作；（4）点击保存，操作界面如下：8、删除本地组（1）进入管理页面，群组页面；（2）点击需要修改的组名；（3）点击“删除群组”按钮即可完成对当前组的删除。 4.2 仪表盘管理4.2.1 指标管理在首页选择菜单栏中“仪表盘”，可对仪表盘进行编辑。在“仪表盘”中，可以根据使用习惯对Widgets面板进行定制，通过鼠标拖动可以改变控件的布局，恢复“仪表盘默认布局”可恢复默认布局。 将鼠标指针移动到需要操作的Widget上，点击左上角删除标识，即可移除； 向“仪表盘”中添加Widget操作，可根据页面向导提示，添加即可。 4.2.1.1 查看历史指标信息1.点开需要查看的指标，点击左右箭头可看到最近1小时、4小时、1周、1月、1年的指标。 4.2.2 热力图热力图以另外一种热图的方式直观的展示出了Widgets中所监控的信息。在首页选择菜单栏中“仪表盘”，选择“热力图”选项，可对热力图进行一个查看和编辑。 4.2.3 配置历史在首页选择菜单栏中“仪表盘”，选择“历史配置”选项。历史配置中对每一个服务的配置文件修改信息作了一次汇总，用户可用查看服务之前的配置文件，点击跳转到服务的配置页面，提高用户体验。如图所示： 4.3 服务管理4.3.1 服务增删1、添加服务和删除组件（1）在首页点击“服务”，进入“服务”页面。添加服务若需要添加服务，则执行如下操作：在左侧服务栏目下方点击“操作”下拉框，并选中“添加服务”；点击“添加服务”按钮后会弹出导航框，弹出导航框如下图所示：在导航框中选择自己需要添加的服务，点击“下一步”；按照提示完成该服务的安装，完成安装后会提示安装成功。删除服务如需要删除服务，则执行如下步骤：选择需要移除的服务，在“服务操作”中，点击停止，等待服务停止后，点击删除，即可： 4.3.2 服务启停1、服务启停（1）在首页点击服务，进入服务页面；（2）在左侧服务栏中选择需要启停的服务，点击该服务名称；（3）点击“服务操作”下拉框，点击“停止”即可实现该服务的停止，若需要启动该服务则点击“启动”，即可实现启动该服务。 4.3.3 服务检测（1）在首页点击“服务”，进入“服务”页面：（2）在左侧服务栏中选择需要启停的服务，点击该服务名称：（3）点击“服务操作”下拉框，点击“运行服务检测”，会弹出一个提示框：（4）点击“确认”，即可实现对该服务的检查。 4.3.4 下载客户端配置（1）在首页点击“服务”，进入“服务”页面：（2）在左侧服务栏中选择对应的服务，点击该服务名称：（3）点击“服务操作”下拉框，点击“下载客户端配置”，会弹出一个提示框：（4）点击“确认”，即可完成对该服务的客户端配置的下载。 4.3.5 维修模式启停开启维护模式和关闭维护模式（1）在首页点击“服务”，进入“服务”页面：（2）在左侧服务栏中选择对应的服务，点击该服务名称：开启维护模式若开启维护模式，执行如下操作：（1）点击“服务操作”下拉框，点击“开启维护模式”后会弹出提示框：（2）点击“确认”；（3）等待几分钟后即可开启维护模式。关闭维护模式若关闭维护模式，则执行如下操作：（1）点击“服务操作”下拉框，点击“关闭维护模式”：（2）点击“确认”；即可关闭维护模式： 4.3.6 配置修改注意：【HControl当中，每个服务都有其各自需要的配置文件，因此修改配置是针对某个服务的配置信息进行修改】。（1）在首页点击“服务”，进入“服务”页面；（2）在左侧服务栏中选择需要修改配置的服务，点击该服务名称进入该服务的页面；（3）点击页面中间“配置”按键，进入该服务的配置页面；若修改原有配置则继续执行以下操作：（1）在页面当中找到需要修改的配置参数，寻找参数有两种方式：第一种方式是直接把所有的配置组展开，然后寻找，第二种方式则是在配置搜索栏当中输入需要修改的参数名称，完成配置参数的快速查找；（2）在需要修改的参数如数框中输入新的参数值；（3）点击“保存”，弹出提示框后点击“确认”；（4）再保存新的配置参数后，页面中间会有一个新的警告条，点击“重启”按钮，即可。若需要新增配置项则继续执行如下操作：（1）找到有“增加属性”操作的组；（2）点击“增加属性”后会弹出一个表单框，表单中包含Type、key、value三个字段，其中Type名称为配置保存的配置文件名，key为添加属性的键名，value为值名，在表单中输入相对应的属性值，弹出框如图所示：（3）点击“添加”：（4）点击“保存”，弹出提示框后点击“确认”：（5）保存新的配置参数后，页面中间会有一个新的警告条，点击“重启”按钮。创建配置组注意：【HControl一开始默认将集群中的所有主机都归为一个组，这样修改配置后会同步到安装服务的每一台主机上。管理配置组的功能就是可以管理集群中该组机器的配置，其他机器不受影响。设置过程中，选择Manage Config Groups，来建立新的分组，重新分配主机，可以改变组内机器的默认配置】。（1）在首页选中自己需要添加配置组的服务，然后在首页点击“服务”，进入“服务”页面；（2）点击“配置”按钮进入配置页面：（3）点击“管理配置组”，将弹出添加配置组的表单，在表单左侧点击+，弹出配置组添加框，操作框所示，在弹出框内输入组名和描述，点击“确认”：（4）选中左侧新添加的组名，点击主机框下方加号，将弹出主机选择框，选择框如图27所示，选中该配置组中需要添加的主机，点击“确认”即完成了配置组的添加： 4.3.7 快速链接注意：【在HControl平台当中所有服务都有与之对应的“快速链接”，通过该功能可以查看与相关组件的一些详细信息，如日志，组件详细信息页面等。】 在首页点击“服务”，进入“服务”页面； 在左侧服务栏中选择需要修改配置的服务，点击该服务名称进入该服务的页面； 点击Quik Links下拉框并选中需要查看的信息，页面即会跳转到该页面，值得注意的是，在使用该功能前需要配置本地主机名，实现主机名与IP地址的映射。 4.3.8 高可用管理一、HDFS中的NameNode和Yarn中Resource Manage的高可用功能，启动HA集群中至少需要3台主机】； 二、hbase、haproxy、historyserver、ldap、ranger、hive、slider、storm、ntp的高可用请参考如下步骤；（注意mr、ldap、slider以及ranger高可用建议先安装haproxy） 4.3.8.1 HDFS和YARN高可用操作步骤如下： 在首页点击“服务”，进入“服务”页面： 在左侧服务栏中选择“HDFS”或是“YARN”的服务，点击该服务名称： 点击”服务操作”下拉框，点击“启用NameNode(ResourceManager)高可用”，此时会弹出一个提示框： 点击“确认”后会弹出启动高可用的提示向导，点击“下一步”即可实现对高可用的启动： 当启动成功后会提示启动成功，若在启动过程中有遇到失败，可以参考日志提示解决后继续： 4.3.8.2 Hive高可用通过hc页面，在点击hive服务的服务操作中添加添加多个Hiveserver2和metastore，并遵循如下步骤： （1）HiveServer2高可用配置 参数名称 参数值 hive.server2.support.dynamic.service.discovery true hive.server2.zookeeper.namespace hiveserver2 hive.zookeeper.quorum host1:2181,host2:2181,host3:2181 hive.zookeeper.client.port 2181 通过beeline/JDBC连接串测试 12beeline!connect jdbc:hive2://hostt1:2181,host2:2181,host3:2181/default;serviceDiscoveryMode=zookeeper;zooKeeperNamespace=hiveserver2 输入用户名hive，密码是用户自定义配置。 说明：客户端连接的时候，会从zookeeper选择hiveserver2节点连接。 说明：hiveserver的数量由hive并发数决定，平均1个hiveserver并发不要超过100 （2）HiveMetastore高可用 通过hc页面添加多个hivemetastore（点击hive服务的服务操作中添加metastore即可），修改如下配置 1hive.metastore.uris=thrift://metahost1:9083,thrift://metahost2:9083 说明：Hivemetastore的“HA机制”比较简单，不需要Zookeeper参与，直接启动多个即可，HiveServer2在使用Hivemetastore时会有简单重试机制，依次访问这几个Hivemetastore实例 4.3.8.3 LDAP高可用（1）操作步骤如下： 在首页点击“服务”，进入“服务”页面； 在左侧服务栏中选择“LDAP”的服务，点击该服务名称； 点击右侧”服务操作”下拉框，点击“+添加 LDAP Server”,选择即将安装的主机名； 点击确认； 重启haproxy 注意：由于ranger需要同步ldap数据，故启动ldap高可用后，请注意检查和修改ranger如下属性： 12ranger.ldap.url=ldap://virtualip:390ranger.usersync.ldap.url=ldap://virtualip:390 ​ 如果hadoop需要同步ldap数据，注意启动ldap高可用后，请检查和修改core-site中如下属性： 1hadoop.security.group.mapping.ldap.url=ldap://virtualip:390 4.3.8.4 MR HistoryServer高可用（1）操作步骤如下： 在首页点击“服务”，进入“服务”页面； 在左侧服务栏中选择“BC-MAPREDUCE2”的服务，点击该服务名称； 点击右侧”服务操作”下拉框，点击“+添加 History Server”,选择即将安装的主机名； 点击确认； 重启haproxy 4.3.8.5 Ranger高可用（1）操作步骤如下： 在首页点击“服务”，进入“服务”页面： 在左侧服务栏中选择“Ranger”的服务，点击该服务名称： 点击”服务操作”下拉框，点击“启用Ranger高可用”,输入负载均衡（haproxy中虚拟ip）地址，例如http://ip:6081, 根据提示点击下一步选择备用ranger admin地址 根据提示点击下一步即可； 重启haproxy 4.3.8.6 Haproxy高可用页面添加多个haproxy（注意填写主机virtualaddress虚拟IP地址）。haproxy的用户和密码分别为haproxy和haproxy@admin。 注意： 1.同一网段内各组keepalived配置中的virtual_router_id [大小不能超过255]不可重复（在高级keepalived下virtaul_router_id） 4.3.8.7 Kerberos高可用操作步骤： 123456789101. 完成kerberos的开启（参考4.3.9中kerberos开启）；2. 修改kerberos配置中slave host为备用kerberos的主机名；3. 前往备kerberos所在节点，点击“+添加 Kerberos Server”，选择即将安装的主机名；4. 进入主kerberos server所在节点，点击重启;5. 进入备用kerberos server所在节点，点击重启;6. 页面上更改配置 is_dump 为true，并从kerberos server 备节点后台登录kadmin，密码为开启kerberos时页面配置的密码，执行ktadd host/&#123;slave_hostname&#125;， 重启master kdc ;7. 将主kerberos server 节点 的/var/kerberos/krb5kdc/.k5.BCHKDC 文件scp到备用kerberos server对应的目录 /var/kerberos/krb5kdc/目录下，启动备kerberos server；8. 后台主备节点登录kadmin，密码为开启kerberos时页面配置的密码，分别执行ktadd kiprop/&#123;master_hostname&#125; ktadd kiprop/&#123;slave_hostname&#125; 9. 页面更改配置 incr_db_prop为true krb.stopped.enabled为true，重启master kdc，启动 slave kdc10. 页面更改配置krb.stopped.enabled为false，保存。 4.3.8.8 Slider高可用（1）操作步骤如下： 在首页点击“服务”，进入“服务”页面： 在左侧服务栏中选择“BC-Slider”的服务，点击该服务名称： 点击“服务操作”下拉框，选择“+添加 SliderRouteNode”，选择节点安装该组件，点击确定； 根据页面提示重启slider; 重启haproxy 4.3.8.9 NTP高可用（1）操作步骤如下： 在首页点击“主机”，进入“主机”页面： 选择需要部署NTP的主机，点击进入对应主机名称，进入主机页面： 点击“+添加”下拉框，选择“NTP Server”，点击确定。 4.3.8.10 Storm高可用（1）nimbus高可用操作步骤如下： 在首页点击“主机”，进入“主机”页面： 选择需要部署Nimbus的主机，点击进入对应主机名称，进入主机页面： 点击“+添加”下拉框，选择“Nimbus”，点击确定。 （2）storm server ui 高可用操作步骤： 在首页点击“主机”，进入“主机”页面： 选择需要部署Nimbus的主机，点击进入对应主机名称，进入主机页面： 点击“+添加”下拉框，选择“Storm UI Server”，点击确定； 根据页面提示重启storm； 重启haproxy 当开启kerberos后，访问server ui目前有问题，已提jira给storm组相关同事http://10.254.0.20:8088/browse/STREAMING-1177。 4.3.8.11 数据库高可用Impala/hive/ranger使用的是mysql的数据库，在实际实施过程中，建议使用rdb数据库。开启数据库高可用方式如下： 1.rdb安装 请参考&lt;其他说明之rdb部署&gt;一节 2.服务驱动包替换 （1）hive驱动包替换方式 12341.把最新的jar包命名为mysql-connector-java.jar放到hc server的/var/lib/ambari-server/resources/下2.把最新的jar包命名为mysql-connector-java.jar放到hive server2/metastore所在节点的/usr/share/java/目录下3.重启下hive服务4.检测hive server2/metastore的/cmss/bch/bc1.3.5/hive/lib/下是否有最新的mysql-connector-java.jar,若有即生效。 （2）impala驱动包替换方式 1将要替换的驱动包替换到impala目录/cmss/bch/bc1.3.5/impala/fe/target/dependency/下，并重启impala生效。 （3）ranger驱动包替换方式 121.在ambari-server节点执行ambari-server setup --jdbc-dirver=新包地址 --jdbc-db=mysql;2.重启ranger即可 3.元数据数据库创建和登录用户 （1）hive操作步骤 12345678910/usr/lib/bcrdb/bin/mysql -pbcrdbCREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;password&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;hive&apos;@&apos;%&apos;;FLUSH PRIVILEGES;create database hive;其中password是hive的数据库密码修改HC上hive的mysql的连接串Database URL为：jdbc:mysql://host1:3306,host2:3306,host3:3306/hive?autoReconnect=true&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;failOverReadOnly=false&amp;useSSL=false其中host1/host2/host3指安装rdb的主机名称，请根据实际修改 （2）impala操作步骤 12345678910/usr/lib/bcrdb/bin/mysql -pbcrdbCREATE USER &apos;impala&apos;@&apos;%&apos; IDENTIFIED BY &apos;password&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;impala&apos;@&apos;%&apos;;FLUSH PRIVILEGES;create database impala;其中password是impala的数据库密码修改HC上Impala的连接串Database URL为：jdbc:mysql://host1:3306,host2:3306,host3:3306/impala?autoReconnect=true&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;failOverReadOnly=false&amp;useSSL=false其中host1/host2/host3指安装rdb的主机名称，请根据实际修改 （3）ranger操作步骤 12345678910/usr/lib/bcrdb/bin/mysql -pbcrdbCREATE USER &apos;rangerdba&apos;@&apos;%&apos; IDENTIFIED BY &apos;rangerdba&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;rangerdba&apos;@&apos;%&apos; WITH GRANT OPTION;FLUSH PRIVILEGES;修改HC上ranger的连接串JDBC connect string for a Ranger database为jdbc:mysql://host1:3306,host2:3306,host3:3306/ranger?autoReconnect=true&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;failOverReadOnly=false&amp;useSSL=false连接串ranger.jpa.audit.jdbc.url为：jdbc:mysql://host1:3306,host2:3306,host3:3306/ranger_audit?autoReconnect=true&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;failOverReadOnly=false&amp;useSSL=false其中host1/host2/host3指安装rdb的主机名称，请根据实际修改 4.3.9 kerberos启停（1）开启kerberos 开启HC界面后，选择菜单栏“管理”–&gt;”Kerberos”–&gt;”启用kerberos”; 选择 未安装 MIT KDC，勾选 集群未安装KDC Server 多选框，（如果是已经开通过一次kerberos，停用之后再次开启，则选择 已存在MIT KDC，并勾选所有多选框）； 修改如下四个配置 12345例如：其中master为安装kdc所在的节点的主机名； BCHKDC是域名，qwe是密码KDC host = masterRealm name = BCHKDCKadmin host = masterAdmin password = qwe123 其他步骤默认即可。 （2）停用kerberos 选择菜单栏—&gt;管理–&gt;kerberos–&gt;停用kerberos，根据向导选择默认步骤即可 备注：开启kerberos后，所有服务走kerberos认证，如kerberos挂，则所有服务不能使用，故默认开启kerbeors后禁止停止kerberos服务。若测试页面kerberos服务的停止操作，需修改配置krb.stopped.enabled =true，方可停止。 4.3.9.1 开启kerberos后注意事项（1）hive的service check若出现检测超时，请手动修改执行check的主机下的该文件 12/cmss/bch/bc1.3.5/hive/hcatalog/etc/hcatalog/proto-hive-site.xml配置里面的hive.metastore.uris为真实配置 4.3.10 bcid配置4.3.10.1 hadoop的bcid认证开启配置 属性 值 文件 hadoop.security.enable.bdoc.authentication true core-site 4.3.10.2 hive的bcid认证开启配置hive的bcid认证依赖hadoop的bcid认证开启，请先配置hadoop的bcid后再配置hive的bcid 属性 值 文件 hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.BdocUserAuthenticator hiveserver2-site 开启bcid认证后，连接方式需加入id和key，例如： 123hive2://hiveserver2:10000/default?hadoop.security.bdoc.access.id=0aca8fd72c0e57dad87f;hadoop.security.bdoc.access.key=hive或者jdbc:hive2://hostt1:2181,host2:2181,host3:2181/default;serviceDiscoveryMode=zookeeper;zooKeeperNamespace=hiveserver2;hadoop.security.bdoc.access.id=0aca8fd72c0e57dad87f;hadoop.security.bdoc.access.key=hive 4.3.10.3 spark thrift server配置bcid（1）在yarn的配置fair-scheduler中增加配置如下 1234&lt;queue name=&quot;spark&quot;&gt;&lt;aclAdministerApps&gt;spark&lt;/aclAdministerApps&gt;&lt;aclSubmitApps&gt;spark&lt;/aclSubmitApps&gt;&lt;/queue&gt; （2）在spark配置spark-defaults中增加属性 12spark.hadoop.hadoop.security.bdoc.access.key=adminspark.hadoop.hadoop.security.bdoc.access.id=admin 在Thrift Startup Parameters中增加指定提交队列： 123--queue root.spark完整内容如下：--master yarn --deploy-mode client --driver-memory 1g --driver-cores 1 --num-executors 1 --executor-cores 1 --executor-memory 1g --queue root.spark 4.3.10.4 开启bcid认证后注意事项（1）service check报错admin未配置或者不能为空 1231.检查resourcemanager下是否有/cmss/bch/bc1.3.5/hadoop/etc/hadoop/bdoc-password.txt，2.若无，拷贝namenode所在节点的文件到resoucemanager的目录/cmss/bch/bc1.3.5/hadoop/etc/hadoop/下，页面重启resourcemanager即可；3.若有，页面重启resourcemanager即可 4.3.11 hadoop混合模式配置混合认证开启的前提：kerberos和bcid已经开启。 混合认证配置属性如下： hadoop.security.authentication=compatible 在hc页面删除如下服务的相关属性并重启： hdfs中的 dfs.datanode.kerberos.principal 和 dfs.datanode.keytab.file 并重启hdfs mapreduce yarn中的yarn.nodemanager.principal 和 yarn.nodemanager.keytab并重启yarn 重启其他服务 4.3.12 agent心跳管理agent心跳丢失默认不开启。文件/etc/ambari-server/conf/ambari.properties，涉及参数说明如下 属性 默认值 auto.shutdown.datanode.if.lost.hearbeat false auto.shutdown.datanode.check.interval 180000 auto.shutdown.datanode.check.delay 300000 说明如下： (1)auto.shutdown.datanode.if.lost.hearbeat——-发现了丢失心跳的datanode，是否主动通知namenode下线DataNode，默认不开启此功能 (2)auto.shutdown.datanode.check.interval——默认每3分钟检测一次，是否有丢失心跳的host以及该host上是否有DataNode.(3)auto.shutdown.datanode.check.delay——测心跳丢失host信息的初始延迟时间，默认是web页面打开5分钟后开始检测。 4.3.13 server超时配置页面超时配置： 属性 默认值 user.inactivity.timeout.default 1800 单位秒，由于网络环境质量无法保证，故建议初始安装阶段可以延长改该值。 4.4 主机管理4.4.1 主机增删注意：【在部署完集群后，HControl还可以在原来部署集群的基础上去添加和删除主机，以提高操作的灵活性。】 在首页点击Hosts，即可进入主机列表页面。 增加主机 若需要添加主机，则继续执行如下操作： （1）点击左上角“操作”下拉框。（2）选中“添加新主机”，此时会弹出导航框，弹框如下图所示：（3）添加主机的操作步骤与部署时相似，参考HControl部署文档。 删除主机 若需要在集群上删除一个主机，则继续执行如下操作：（1）点击“主机”列表页中需要删除的主机名称，进入主机组件详情页；（2）点击“主机操作”下拉框，选择“删除主机”，操作页面如下图；（3）弹出框后点击“确认”，进入删除程序，根据提示完成删除操作。 4.4.2 组件启停​ 在首页选择菜单栏中“主机”，进入“主机”页面，选择需要操作的主机，点击进入。在主机页面，右侧主机操作中选择在“启动所有组件”或是“停止所有组件”。 在首页点击Hosts，即可进入主机列表页面。 选中组件所在主机，并点击该主机名即可进入该主机的概览页面。 在主机概览页面左栏有改主机中所有的组件及状态。若想启动某个组件，则执行如下操作：（1）选中一个状态为stop的组件，并点击该组件的下拉框。（2）在下拉框中选择start，弹出确认框后点击ok。（3）等待1分钟左右即可实现该组件的启动，执行页面所示。若想停止某个组件，则执行如下操作：（1）选中一个状态为started的组件，并点击该组件的下拉框;（2）在下拉框中选择stop，弹出确认框后点击”确认”；（3）等待1分钟左右即可实现该组件的停止： 4.4.3 维护模式启停​ 在首页选择菜单栏中“主机”，进入“主机”页面，选择需要操作的主机，点击进入。在主机页面，右侧主机操作中选择在“开启维护模式”或是“关闭维护模式” 4.4.4 机架设置​ 在首页选择菜单栏中“主机”，进入“主机”页面，选择需要操作的主机，点击进入。在主机页面，右侧主机操作中选择在“设置机架” 4.4.5 主机删选注意：【在多主机集群当中主机筛选功能可以快速找到特定的主机。在筛选主机时可以通过主机的状态特性来查找，也可以通过主机的物理特性来定位，当主机存在多网卡的情况，主机列表当中所显示的IP地址为与hostname相应的第一个IP。】 1、在首页点击Hosts，即可进入主机列表页面。若想通过主机状态来筛选主机，则继续执行如下操作：(1)点击左上方Filter下拉框。(2)在该下拉框中有Healthy、Master Down、Slave Down、Restart、Lost Heartbeat、Alert等状态，选中你想筛选的主机状态，其中Healthy表示运良好的主机，Master Down表示有Master挂掉的主机，Slave Down表示有Slave挂掉的主机，Restart表示可执行Restart的主机，Lost Heartbeat表示失去心跳的主机，Alert表示有警告的主机。(3)在列表中将展现出你所想筛选的主机。若想通过主机的物理信息筛选主机，则继续执行如下操作：(1)在列表头部有一些筛选输入框，包括主机名、IP、Cores等信息，在这些输入框中输入你想筛选主机的物理特性，操作页面所示。(2)在列表中将展现出你所想筛选的主机: 4.5 告警管理4.5.1 告警组操作​ 选择操作“管理告警组”，进入“管理告警组”页面，可对告警组进行编辑。 4.5.2 告警组管理​ 选择下拉菜单中对应的组，可看到对应组件的告警定义、告警状态、告警最后修改时间以及告警的状态。 4.6 审计日志管理4.6.1 hc-server审计日志​ 修改ambari-server所在节点上 12/etc/ambari-server/conf/ambari.properties中auditlog.enabled=true ​ 重启ambari-server 1ambari-server restart 4.6.2 yarn审计日志​ 在首页点击左侧的服务YARN，选择配置，在“Advanced”下选择“高级yarn-log4j”，在文本框中加入如下代码： 1234567891011121314151617rm.audit.logger=INFO,console,RMAUDITlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=$&#123;rm.audit.logger&#125;log4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=falselog4j.appender.RMAUDIT=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.RMAUDIT.File=$&#123;hadoop.log.dir&#125;/rm-audit.loglog4j.appender.RMAUDIT.layout=org.apache.log4j.PatternLayoutlog4j.appender.RMAUDIT.layout.ConversionPattern=%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.RMAUDIT.DatePattern=.yyyy-MM-ddnm.audit.logger=INFO,console,NMAUDITlog4j.logger.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=$&#123;nm.audit.logger&#125;log4j.additivity.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=falselog4j.appender.NMAUDIT=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.NMAUDIT.File=$&#123;hadoop.log.dir&#125;/nm-audit.loglog4j.appender.NMAUDIT.layout=org.apache.log4j.PatternLayoutlog4j.appender.NMAUDIT.layout.ConversionPattern=%d&#123;ISO8601&#125; %p %c&#123;2&#125;: %m%nlog4j.appender.NMAUDIT.DatePattern=.yyyy-MM-dd 根据页面提示，重启yarn等服务 4.6.3 hive审计日志​ 在首页点击左侧的服务Hive，选择配置，在“Advanced”下选择“高级hive-log4j”，在文本框中加入如下代码： 123456789101112131415rm.audit.logger=INFO,console,RMAUDITlog4j.logger.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer=INFO ,auditaplog4j.logger.org.apache.hadoop.hive.ql.Driver=INFO ,auditaplog4j.logger.org.apache.hadoop.hive.ql.exec.mr.ExecDriver=INFO ,auditaplog4j.logger.org.apache.hadoop.hive.ql.exec.mr.MapRedTask=INFO ,auditaplog4j.logger.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask=INFO ,auditaplog4j.logger.org.apache.hadoop.hive.ql.exec.Task=INFO ,auditaplog4j.logger.org.apache.hadoop.hive.ql.session.SessionState=INFO ,auditaplog4j.logger.org.apache.hadoop.hive.metastore.HiveMetaStore.audit=INFO,auditaplog4j.appender.auditap=org.apache.log4j.FileAppenderlog4j.appender.auditap.File=/var/log/cmss/hive/hive-audit.loglog4j.appender.auditap.Append=false log4j.appender.auditap.layout=org.apache.log4j.PatternLayoutlog4j.appender.auditap.layout.ConversionPattern=[%d&#123;HH:mm:ss:SSS&#125;][%C-%M] -%m%n 根据页面提示，重启hive等服务。 4.7 其他说明4.7.1 Ranger服务安装（1）ranger服务添加 通过hc的”添加服务“添加ranger;（注意执行ambari-server setup –jdbc-db=mysql –jdbc-driver=/usr/share/java/mysql-connector-java.jar;ranger需要rangerdba用户通过密码访问ranger的数据库，请确认在安装时选择的数据库的节点上执行如下命令) 1234567CREATE USER &apos;rangerdba&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;rangerdba&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;rangerdba&apos;@&apos;localhost&apos;;CREATE USER &apos;rangerdba&apos;@&apos;%&apos; IDENTIFIED BY &apos;rangerdba&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;rangerdba&apos;@&apos;%&apos;;GRANT ALL PRIVILEGES ON *.* TO &apos;rangerdba&apos;@&apos;localhost&apos; WITH GRANT OPTION;GRANT ALL PRIVILEGES ON *.* TO &apos;rangerdba&apos;@&apos;%&apos; WITH GRANT OPTION;FLUSH PRIVILEGES; 完成服务添加（如出现rangerdba用户被数据库拒绝，请检查密码是否正确；如出现driver not in path,请先跳过，在ranger页面配置的Ranger-Admin-Properties中）修改SQL_CONNECTOR_JAR=真实路径，然后安装即可； （2）ranger_hive_plugin使用 修改hive如下属性 Settings下Choose Authorization的认证方式为Ranger; 在hiveserver2-site下，修改如下属性 | 属性 | 属性值 || ———————————– | —————————————- || hive.security.authorization.enabled | true || hive.security.authenticator.manager | org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator || hive.security.authorization.manager | org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizerFactory | 在hive-site中修改如下属性： | 属性 | 属性值 || ———————————– | —————————————- || hive.security.authorization.enabled | true || hive.server2.enable.doAs | false || hive.conf.restricted.list | hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role,hive.security.authorization.enabled | ​ 重启hive （3）ranger_hdfs_plugin使用 修改hdfs属性如下： 1勾选Enable Ranger for HDFS 备注：如需完成hadoop的ldap同步，请修改如下属性 123修改hadoop.group.enable.ldap属性为true;修改hadoop.security.group.mapping.ldap.url属性为localhost为ldap_server所在主机地址--当启动ldap高可用时，请注意修改为虚拟ldap://ip:390重启hdfs （4）ranger_hbase_plugin使用 修改hbase属性如下： 1勾选Enable Ranger for HBASE 根据页面提示重启相关服务 （5）ranger_kafka_plugin使用 修改kafka属性如下： 1勾选Enable Ranger for KAFKA 根据页面提示重启相关服务 （6）启动hive的ranger插件后，hive的service-check可能不过，原因是因为amabri-qa并没有被授权，解决方法如下： 增加ambari-qa用户到并同步到ldap中，登录ranger:6080，点击策略授权（所有权限）即可。 初始化ldap数据，初始化脚本： 123456789101112131415161718192021222324252627282930dn: dc=hadoop,dc=apache,dc=orgobjectClass: topobjectClass: domaindn: ou=users,dc=hadoop,dc=apache,dc=orgobjectClass: organizationalUnitobjectClass: topou: usersdn: ou=groups,dc=hadoop,dc=apache,dc=orgobjectClass: organizationalUnitobjectClass: topou: groupsdn: ou=orgs,dc=hadoop,dc=apache,dc=orgobjectClass: organizationalUnitobjectClass: topou: orgsdn: cn=ambari-qa,ou=users,dc=hadoop,dc=apache,dc=orgobjectClass: personobjectClass: topcn: ambari-qasn: ambari-qadn: cn= hdfs,ou=groups,dc=hadoop,dc=apache,dc=orgobjectClass: groupofnamesobjectClass: topcn: hdfsmember:cn=ambari-qa,ou=users,dc=hadoop,dc=apache,dc=org （7）启动ldap高可用后，请注意修改如下服务的对应配置为ldap:virtual_ip:390 服务 文件 属性 HDFS 高级 core-site hadoop.security.group.mapping.ldap.url Ranger LDAP Settings ranger.ldap.url 高级ranger-ugsync-site ranger.usersync.ldap.url 4.7.2 Ranger插件Test Connection配置(1)ranger-hdfs-plugin 当启动ha后，测试TestConnection需要在页面增加如下属性值的配置（属性值请从hc页面查看获取） 属性 dfs.ha.namenodes. dfs.namenode.rpc-address..nn1 dfs.namenode.rpc-address..nn2 dfs.nameservices dfs.client.failover.proxy.provider. 其中nsid是指nameservice的id (2)ranger-hive-plugin 当启动ha后，测试TestConnection需要在页面增加如下属性值的配置(属性值请从hc页面查看) 属性 属性值 jdbc.url jdbc:hive2://host1:2181,host2:2181,host3:2181/default;serviceDiscoveryMode=zookeeper;zooKeeperNamespace=hiveserver2 hive.server2.zookeeper.namespace hiveserver2 hive.metastore.uris thrift://host1:9083,thrift://host2:9083,thrift://host3:9083 hive.zookeeper.quorum host1:2181,host2:2181,host3:2181 hive.zookeeper.client.port 2181 说明：其中host1/host2/host3是指安装了zookeeper的主机名称。 4.7.3 Ldap日志配置CentOS上通过yum安装的ldap默认是日志不开启，如需开启，请参考如下配置： 执行service rsyslog status或者service syslog status,查看是否拥有系统日志功能 如果系统log是rsyslog，则修改/etc/rsyslog.conf 配置，增加 1local4.* /var/log/ldap.log 如果系统log是syslog，则修改/etc/syslog.conf 配置，增加 1local4.* /var/log/ldap.log 后台执行 12service rsyslog restart或者service syslog restartservice slapd restart 在/var/log/ldap.log可看到日志。 4.7.4 RDB部署选择3台机器，假设是host1/host2/host3部署rbd步骤如下： 1.安装rdb在三台机器 1yum install bcrdb socat percona-xtrabackup -y 2.修改配置/etc/bcrdb/my.cnf 12345678主要修改如下内容：各种内存参数根据服务器硬件配置进行调整，主要修改innodb_buffer_pool_size参数，单独RDB服务器可配置为服务器总内存的60%-80%。bind_address 监听IP。配置为服务器业务网IP。wsrep_node_address 心跳IP。配置为服务器心跳网地址，如不区分业务、心跳网，可以配置相同的IP，或注释掉该参数。wsrep_cluster_name 集群名。可以不修改，如果有多个集群，建议取不同的集群名。wsrep_node_name= node1 节点名。各节点必须取不同的节点名，比如node2、node3。wsrep_cluster_address= &quot;gcomm://10.133.17.171,10.133.17.107&quot; 指定除了本节点以外的其他节点的心跳网IP。log_bin_trust_function_creators = 1 参数加在wsrep_provider_options参数的下一行。（RANGER 元数据库使用该参数） 3.启动rdb。选择host1作为第一个节点，执行 1service bcrdbd bootstrap host2/host3执行 1service bcrdbd start 4.登录host1/host2/host3任一节点 12345/usr/lib/bcrdb/bin/mysql -pbcrdb 验证rdb是否正确安装：show status like &apos;wsrep%&apos;;如果wsrep_cluster_size==说明OK 6.其他说明 123456bcrdb的目录说明 程序主目录 /usr/lib/bcrdb/数据目录 /var/lib/bcrdb/日志目录 /var/log/bcrdb/pid、socket目录 /var/run/bcrdb/配置文件目录 /etc/bcrdb/ 5如何获得帮助可访问HControl官方网站http://10.254.2.95/bdoc/index.html寻找解决办法，或联系相关运维人员取得帮助。 6本软件系统售后服务的做法与原则本软件在使用过程中若存在问题或者疑问，可以联系相关运维人员。 7常见问题解答 未配置yum源，导致no packager错误；解决方法：参考部署文档。 注册失败解决办法：（1）检查防火墙是否关闭；（2）检查openssl版本是否大于等于30；（3）检查主机名称是否正确；（4）检查ambari-agent.ini是否配正确server的主机名称；（5）重启agent。 主kerberos崩溃后，采用如下方法恢复： 123假设节点hostname为master 和 slave，现场环境master崩溃 1.修改页面krb5-conf 中admin_server=&#123;&#123;slave&#125;&#125; ，并刷新配置到每一个kdc client （在主机页面的client端后有该按钮） 2.登陆slave节点，将/var/kerberos/krb5kdc/kpropd.acl mv到其他任意目录，后执行service kadmin start; 由于ranger默认使用数据库为mysql，不同版本的mysql在字段定义存在区别。如果使用mysql5.7系列，在安装ranger过程中会报错： 1234562017-04-21 17:08:21,556 [JISQL] /usr/jdk64/jdk1.7.0_67/bin/java -cp /usr/share/java/mysql-connector-java-5.1.17.jar:/cmss/bch/bc1.3.3/ranger/ranger-admin/jisql/lib/* org.apache.util.sql.Jisql -driver mysqlconj -cstring jdbc:mysql://docker1.cmss.com/ranger -u &apos;rangerdba&apos; -p &apos;********&apos; -noheader -trim -c \; -input /cmss/bch/bc1.3.3/ranger/ranger-admin/db/mysql/create_dbversion_catalog.sqlError executing: create table if not exists x_db_version_h ( id bigint not null auto_increment primary key, version varchar(64) not null, inst_at timestamp not null default current_timestamp, inst_by varchar(256) not null, updated_at timestamp not null, updated_by varchar(256) not null, active ENUM(&apos;Y&apos;, &apos;N&apos;) default &apos;Y&apos; ) ; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Invalid default value for &apos;updated_at&apos;SQLException : SQL state: 42000 com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Invalid default value for &apos;updated_at&apos; ErrorCode: 10672017-04-21 17:08:22,192 [E] create_dbversion_catalog.sql DB schema import failed!2017-04-21 17:08:22,204 [E] DB schema setup failed! Please contact Administrator 后台修改如下： 12/cmss/bch/bc&#123;version&#125;/ranger/ranger-admin/db/mysql/create_dbversion_catalog.sql中updated_at timestamp not null default current_timestamp，即可 原因：5.7的时间戳需要设置默认值，已提官网patch。 5.使用namenode的高可用之后，impala service check会出现“operation category read is not supported in state standby”,原因是因为impala的元数据中关于hdfs的配置并未修改，解决方法如下： 123（1）登录impala数据库所在节点的mysql或者rdb:mysql或者/usr/lib/bcrpd/bin/mysql（2）进入impala数据库：use impala;（3）修改元数据SDS中LOCATION，设置其hdfs地址为启动nn高可用之后的地址即可]]></content>
  </entry>
  <entry>
    <title><![CDATA[storm常见操作]]></title>
    <url>%2F2017%2F09%2F18%2Fstorm%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[storm安装首先安装配置zookeeperPython安装(官网要求2.6.6以上)，CentOS7系统自带2.7.x版本。配置/opt/apache-storm-1.0.3/conf/storm.yaml123456789101112storm.zookeeper.servers: - "CentOS7-1" - "CentOS7-2" - "CentOS7-3" nimbus.seeds: ["CentOS7-1"] supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 storm.local.dir: "/storm/localdir/" ui.port: 10080 1）storm.zookeeper.serversZookeeper集群的host列表，大致为：yaml storm.zookeeper.servers: - “111.222.333.444” - “555.666.777.888”假如Zookeeper集群使用了自定义的端口（默认为2181），需要同时设置storm.zookeeper.port。 2）storm.local.dirNimbus和Supervisor守护进程需要目录存放数据，例如jar包，conf配置文件等。需要用户新建一个目录，给予用户正确的权限，最后路径参数填入。例如：yaml storm.local.dir: “/mnt/storm” 3）nimbus.seedsWorker节点需要知道从哪些机器上下载拓扑的jar包与配置文件。例如：yaml nimbus.seeds:[“111.222.333.44”] 4） supervisor.slots.ports每个worker节点都需要用户定义最多多少个worker能同时运行。每个worker使用一个单独的端口接收消息，这个配置项就是定义哪些端口可以被使用。如果用户定义了5个端口，那么storm则最多能配置5个worker在这个节点上运行。默认为4个端口：6700,6701,6702和6703。例如：123456yamlsupervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 同步以上配置到其他服务器。在各个节点上增加storm集群本地存储文件，该目录被Nimbus和Supervisor进程用于存储少量状态，如jars、confs等的本地磁盘目录，需要提前创建该目录并给以足够的访问权限。然后在storm.yaml中配置该目录1sudo mkdir -p /storm/localdir/ 启动Storm各个后台进程在hostname=master节点（CentOS7-1）启动Nimbus进程服务，放置于后台运行 1nohup bin/storm nimbus &gt;/dev/null 2&gt;&amp;1 &amp; 在各个hostname-slavery节点(CentOS7-2，CentOS7-3,CentOS7-4)启动Supervisor 1nohup bin/storm supervisor &gt;/dev/null 2&gt;&amp;1 &amp; 在hostname=master(CentOS7-1）节点启动UI进程服务，放置于后台运行 1nohup bin/storm ui &gt;/dev/null 2&gt;&amp;1 &amp; 拓扑操作启动拓扑首先，将编译完成拓扑jar包放入bin目录(或指定的任意目录)，然后输入如下指令启动拓扑：1bin/storm jar &#123;topojar.name&#125;.jar &#123;main.name&#125; 其中，topojar.name即为jar包的名字，main.name为jar包中需要运行的主方法的名称（需要全路径）。 停止拓扑如果已经启动UI进程，则可通过界面进行停止拓扑等操作。当然，也可以通过命令停止拓扑： 1bin/storm kill &#123;topo.name&#125; 其中，topo.name为拓扑的名称，在主方法中指定，可以通过UI查看获取，或者在启动拓扑时返回结果查看可知。]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm体系结构]]></title>
    <url>%2F2017%2F09%2F18%2Fstorm%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[storm的流式处理计算模式保证了任务能够只进行一次初始化,就能够持续计算,同时使用了ZeroMQ（Netty）作为底层消息队列,有效地提高了整体架构的数据处理效率,避免了Hadoop的瓶颈。Storm的适用场景: 流数据处理，Storm可以用来处理源源不断流进来的消息,处理之后将结果写入到某个存储中去。 分布式rpc，由于storm的处理组件是分布式的,而且处理延迟极低,所以可以作为一个通用的分布式rpc框架来使用。 持续计算,任务一次初始化,一直运行,除非你手动kill它。 storm架构的设计与Hadoop主从架构一样,Storm也采用Master/Slave体系结构,分布式计算由Nimbus和Supervisor两类服务进程实现,Nimbus进程运行在集群的主节点,负责任务的指派和分发,Supervisor运行在集群的从节点,负责执行任务的具体部分。如图所示： Nimbus:负责资源分配和任务调度。 Supervisor:负责接受nimbus分配的任务,启动和停止属于自己管理的worker进程。 Worker:运行具体处理组件逻辑的进程。 Task:worker中每一个spout/bolt的线程称为一个task｡同一个spout/bolt的task可能会共享一个物理线程,该线程称为executor。 storm架构中使用Spout/Bolt编程模型来对消息进行流式处理｡消息流是storm中对数据的基本抽象,一个消息流是对一条输入数据的封装,源源不断输入的消息流以分布式的方式被处理，Spout组件是消息生产者，是storm架构中的数据输入源头，它可以从多种异构数据源读取数据，并发射消息流Bolt组件负责接收Spout组件发射的信息流，并完成具体的处理逻辑｡在复杂的业务逻辑中可以串联多个Bolt组件，在每个Bolt组件中编写各自不同的功能，从而实现整体的处理逻辑。如图 storm基本概念在Storm的集群里面有两种节点: 控制节点(master node)和工作节点(worker node)。控制节点上面运行一个叫Nimbus后台程序,它的作用类似Hadoop里面的JobTracker，Nimbus负责在集群里面分发代码，分配计算任务给机器，并且监控状态。每一个工作节点上面运行一个叫做Supervisor的进程。Supervisor会监听分配给它那台机器的工作，根据需要启动/关闭工作进程worker。每一个工作进程执行一个topology的一个子集；一个运行的topology由运行在很多机器上的很多工作进程worker组成。(一个supervisor里面有多个workder，一个worker是一个JVM。可以配置worker的数量，对应的是conf/storm.yaml中的supervisor.slot的数量） Nimbus和Supervisor之间的所有协调工作都是通过Zookeeper集群完成。另外，Nimbus进程和Supervisor进程都是快速失败(fail-fast)和无状态的｡所有的状态要么在zookeeper里面, 要么在本地磁盘上。这也就意味着你可以用kill -9来杀死Nimbus和Supervisor进程,然后再重启它们，就好像什么都没有发生过，这个设计使得Storm异常的稳定。 Topology在Storm中,一个实时应用的计算任务被打包作为Topology发布，这同Hadoop的MapReduce任务相似。但是有一点不同的是:在Hadoop中，MapReduce任务最终会执行完成后结束；而在Storm中，Topology任务一旦提交后永远不会结束，除非你显示去停止任务。计算任务Topology是由不同的Spouts和Bolts，通过数据流（Stream）连接起来的图｡下面是一个Topology的结构示意图： 其中包含有：Spout：Storm中的消息源,就跟水龙头一样，用于为Topology生产消息(数据),一般是从外部数据源(如Message Queue、RDBMS、NoSQL、Realtime Log ）不间断地读取数据并发送给Topology消息(tuple元组)。Bolt：Storm中的消息处理者，用于为Topology进行消息的处理，Bolt可以执行过滤，聚合， 查询数据库等操作，而且可以一级一级的进行处理。 数据模型Turplestorm使用tuple来作为它的数据模型。每个tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型，在我的理解里面一个tuple可以看作一个java对象。总体来看，storm支持所有的基本类型：字符串以及字节数组作为tuple的值类型。你也可以使用你自己定义的类型来作为值类型，只要你实现对应的序列化器(serializer)。 一个Tuple代表数据流中的一个基本的处理单元，它可以包含多个Field，每个Field表示一个属性。比如举例一个，三个字段（taskID：int； StreamID：String； ValueList： List）： Tuple是一个Key-Value的Map，由于各个组件间传递的tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List。一个没有边界的，源源不断的，连续的Tuple序列就组成了Stream。 worker（进程）一个topology可能会在一个或者多个worker(工作进程)里面执行，每个worker是一个物理JVM并且执行整个topology的一部分。比如,对于并行度是300的topology来说，如果我们使用50个工作进程worker来执行，那么每个工作进程会处理其中的6个tasks。Storm会尽量均匀的工作分配给所有的worker，setBolt 的最后一个参数是你想为bolts的并行量。 Spouts消息源spout是Storm里面一个topology里面的消息生产者｡一般来说消息源会从一个外部源读取数据并且向topology里面发出消息:tuple。Spout可以是可靠的也可以是不可靠的，如果这个tuple没有被storm成功处理,可靠的消息源spouts可以重新发射一个tuple，但是不可靠的消息源spouts一旦发出一个tuple就不能重发了。 消息源可以发射多条消息流stream｡使用OutputFieldsDeclarer。declareStream来定义多个stream,然后使用SpoutOutputCollector来发射指定的stream。代码上是这样的:collector.emit(new Values(str)); Spout类里面最重要的方法是nextTuple。要么发射一个新的tuple到topology里面或者简单的返回如果已经没有新的tuple。要注意的是nextTuple方法不能阻塞，因为storm在同一个线程上面调用所有消息源spout的方法。另外两个比较重要的spout方法是ack和fail。storm在检测到一个tuple被整个topology成功处理的时候调用ack，否则调用fail。storm只对可靠的spout调用ack和fail。 Bolts所有的消息处理逻辑被封装在bolts里面。Bolts可以做很多事情：过滤，聚合，查询数据库等等。 Bolts可以简单的做消息流的传递(来一个元组,调用一次execute)。复杂的消息流处理往往需要很多步骤，从而也就需要经过很多bolts。比如算出一堆图片里面被转发最多的图片就至少需要两步:第一步算出每个图片的转发数量，第二步找出转发最多的前10个图片。(如果要把这个过程做得更具有扩展性那么可能需要更多的步骤)。 Bolts可以发射多条消息流, 使用OutputFieldsDeclarer.declareStream定义stream,使用OutputCollector.emit来选择要发射的stream。 Bolts的主要方法是execute，它以一个tuple作为输入，bolts使用OutputCollector来发射tuple(spout使用SpoutOutputCollector来发射指定的stream)，bolts必须要为它处理的每一个tuple调用OutputCollector的ack方法，以通知Storm这个tuple被处理完成了，从而通知这个tuple的发射者spouts。一般的流程是: bolts处理一个输入tuple, 发射0个或者多个tuple, 然后调用ack通知storm自己已经处理过这个tuple了。storm提供了一个IBasicBolt会自动调用ack。 ReliabilityStorm保证每个tuple会被topology完整的执行。Storm会追踪由每个spout tuple所产生的tuple树(一个bolt处理一个tuple之后可能会发射别的tuple从而形成树状结构)，并且跟踪这棵tuple树什么时候成功处理完。每个topology都有一个消息超时的设置，如果storm在这个超时的时间内检测不到某个tuple树到底有没有执行成功，那么topology会把这个tuple标记为执行失败，并且过一会儿重新发射这个tuple（超时的时间在storm0.9.0.1版本中是可以设置的,默认是30s）。 Tasks每一个spout和bolt会被当作很多task在整个集群里执行。每一个executor对应到一个线程,在这个线程上运行多个task，而stream grouping则是定义怎么从一堆task发射tuple到另外一堆task。你可以调用TopologyBuilder类的setSpout和setBolt来设置并行度。SetSpout里面的并行度参数含义：parallelism_hint the number of tasks that should be assigned to execute this spout. Each task will run on a thread in a process somwehere around the cluster。（执行这个spout安排了N个tasks｡每个task是一个线程，他们都在同一个进程中。）setBolt的参数含义也是一样的。 Storm数据流模型数据流(Stream)是Storm中对数据进行的抽象，它是时间上无界的tuple元组序列｡在Topology中，Spout是Stream的源头。负责为Topology从特定数据源发射Stream；Bolt可以接收任意多个Stream作为输入，然后进行数据的加工处理过程,如果需要，Bolt还可以发射出新的Stream给下级Bolt进行处理。Topology中每一个计算组件(Spout和Bolt)都有一个并行执行度，在创建Topology时可以进行指定，Storm会在集群内分配对应并行度个数的线程来同时执行这一组件。那么,有一个问题：既然对于一个Spout或Bolt,都会有多个task线程来运行,那么如何在两个组件(Spout和Bolt)之间发送tuple元组呢？Storm提供了若干种数据流分发(Stream Grouping)策略用来解决这一问题。在Topology定义时，需要为每个Bolt指定接收什么样的Stream作为其输入(注:Spout并不需要接收Stream,只会发射Stream)。目前Storm中提供了7种Stream Grouping策略 Shuffle Grouping: 随机分组, 随机派发stream里面的tuple，保证每个bolt接收到的tuple数目大致相同。 Fields Grouping:按字段分组，比如按userid来分组，具有同样userid的tuple会被分到相同的Bolts里的一个task。而不同的userid则会被分配到不同的bolts里的task。 All Grouping:广播发送，对于每一个tuple，所有的bolts都会收到。 Global Grouping:全局分组, 这个tuple被分配到storm中的一个bolt的其中一个task，再具体一点就是分配给id值最低的那个task。 Non Grouping:不分组，这个分组的意思是说stream不关心到底谁会收到它的tuple。目前这种分组和Shuffle grouping是一样的效果。有一点不同的是storm会把这个bolt放到这个bolt的订阅者同一个线程里面去执行。 Direct Grouping: 直接分组, 这是一种比较特别的分组方法，用这种分组意味着消息的发送者指定由消息接收者的哪个task处理这个消息｡。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息tuple必须使用emitDirect方法来发射｡消息处理者可以通过TopologyContext来获取处理它的消息的task的id (OutputCollector.emit方法也会返回task的id)。 Local or shuffle grouping：如果目标bolt有一个或者多个task在同一个工作进程worker中，tuple将会被随机发生给这些tasks。否则,和普通的Shuffle Grouping行为一致。]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven阿里云仓库]]></title>
    <url>%2F2017%2F09%2F18%2Fmaven%E9%98%BF%E9%87%8C%E4%BA%91%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[修改maven根目录下的conf文件夹中的setting.xml文件，内容如下：12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt;]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka关闭脚本]]></title>
    <url>%2F2017%2F09%2F06%2Fkafka%E5%85%B3%E9%97%AD%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[1234567891011#!/bin/bashCOUNT=`cat /root/kafka.list |wc -l`passwd=123456USER=kafkafor((i=1;i&lt;=$COUNT;i++));doIP=`cat /root/kafka.list |sed -n "$i p" `echo $IP./kill.sh $IPecho "Kafka successfull stoped"done 1234567891011#!/usr/bin/expectset timeout 5 set ip [lindex $argv 0]set password 123456spawn ssh kafka@$ipexpect &#123; "password" &#123;send "$password\r";exp_continue&#125; "Last login" &#123;send " ps -ef |grep kafka |grep -v grep |cut -c 9-15 |xargs kill -15 \n"&#125;&#125;expect eofexit kafka.list1234567891011121314151610.11.94.5410.11.94.6210.11.94.7010.11.94.7710.11.94.8410.11.94.9210.11.94.10010.11.94.10710.11.94.15610.11.94.17710.11.94.17910.11.94.18310.11.94.18510.11.94.18610.11.94.19010.11.94.192]]></content>
      <categories>
        <category>Kafka</category>
        <category>scripts</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka中partition和consumer关系]]></title>
    <url>%2F2017%2F09%2F05%2Fkafka%E4%B8%ADpartition%E5%92%8Cconsumer%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka常见操作]]></title>
    <url>%2F2017%2F08%2F31%2Fkafka%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Kafka脚本常用配置参数kafka-console-consumer.sh该脚本用于消费消息 –from-beginning : 设定消费的方式 –topic : 创建的topic的名称 –zookeeper REQUIRED: 指定连接到的zookeeper的节点信息，可以设置多个zk节点以便进行failover –group : 指定一个消费者组在consumer端，不需要指定broke-list，而是通过zookeeper和topic找到所有的持有topic消息的broker kafka-console-producer.sh该脚本用于创建消息 –topic REQUIRED: 指定产生消息的topic –broker-list REQUIRED: 指定往哪些broker推送消费消息 kafka-topic.sh –create: 创建一个topic –describe: 列出topic的细节信息 –list: 列出所有的topic –partitions : 列出指定分区的数量 –replication-factor : 指定复制的副本的数量 –zookeeper REQUIRED: 指定连接到的zookeeper的节点信息，可以设置多个zk节点以便进行failover –topic : 创建topic的名称 Kafka的简单流程创建topic12[kafka@hc1 ~]$ kafka-topics.sh --create --zookeeper hc1:2181 --replication-factor 2 --partitions 3 --topic kiwiCreated topic "kiwi". 配置producer产生消息12345[kafka@hc1 ~]$ kafka-console-producer.sh --broker-list hc1:6777 --topic kiwi123hello world 配置consumer消费消息123456[kafka@hc1 ~]$ kafka-console-consumer.sh --zookeeper hc1:2181 --topic kiwi --from-beginning Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].123hello world Kafka topic 详解12345[kafka@hc1 ~]$ kafka-topics.sh --zookeeper hc1:2181 --describe --topic kiwiTopic:kiwi PartitionCount:3 ReplicationFactor:2 Configs: Topic: kiwi Partition: 0 Leader: 2 Replicas: 2,1 Isr: 2,1 Topic: kiwi Partition: 1 Leader: 0 Replicas: 0,2 Isr: 0,2 Topic: kiwi Partition: 2 Leader: 1 Replicas: 1,0 Isr: 1,0 每个partition有不同的leader，Leader所在的broker同时也是Replicas所在的broker(ID号一样) 每个partition副本集都有一个leader leader指的是partition副本集中的leader，它负责读写，然后负责将数据复制到其它的broker上 一个Topic的所有partition会比较均匀的分布到多个broker上 “isr” 全称 “in-sync replicas”. 代表这现在包含这个partion副本的broker节点.]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper使用kerberos认证连接]]></title>
    <url>%2F2017%2F08%2F31%2Fzookeeper%E4%BD%BF%E7%94%A8kerberos%E8%AE%A4%E8%AF%81%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[zookeeper开启kerberos后，需要使用jass的方式，进行连接 设定，修改zookeeper-env.sh1export CLIENT_JVMFLAGS="-Djava.security.auth.login.config=$JAAS_PATH" 其中$JAAS_PATH 为JAAS文件的绝对路径 配置jaas文件查看jaas.conf文件，如下所示： 12345678Client &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/security/keytabs/zk.service.keytab" storeKey=true useTicketCache=false principal="zookeeper/hc1@BCHKDC";&#125; 重启zk集群，使用zkClis.sh -server hostname:port 方式访问 1/cmss/bch/bc1.3.4/zookeeper/bin/zkCli.sh -server hc1:2181]]></content>
      <categories>
        <category>zookeeper</category>
        <category>kerberos</category>
      </categories>
      <tags>
        <tag>kerberos</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka在zk中的信息详解]]></title>
    <url>%2F2017%2F08%2F30%2FKafka%E5%9C%A8zk%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[问题： Kafka的哪些信息记录在Zookeeper中zk中consumers、config、brokers、controller_epoch都是Kafka相关的znode Consumer Group消费的每个Partition的Offset信息存放在什么位置 Topic的每个Partition存放在哪个Broker上的信息存放在哪里 Producer跟Zookeeper究竟有没有关系？]]></content>
      <categories>
        <category>Kafka</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka宕机演练]]></title>
    <url>%2F2017%2F08%2F30%2FKafka%E5%AE%95%E6%9C%BA%E6%BC%94%E7%BB%83%2F</url>
    <content type="text"><![CDATA[备注：kafka全部停机的情况下，故障处理须严格按照以下步骤在hcontrol界面停止kafka，模拟故障过程1.在stdp节点停止全部的sdtp服务，列表如下： IP 地址 监测端口 用途 10.11.94.23 9100/9200/9300/9400/9500/9600 2/3G数据接收 10.11.94.31 9100/9200/9300/9400/9500/9600 LTE数据接收 10.11.94.39 9100/9200/9300/9400/9500/9600 LTE数据接收 10.11.94.46 9100/9200/9300/9400/9500/9600 LTE数据接收 10.11.94.53 9100/9200/9300/9400/9500/9600 2/3G数据接收 10.11.94.61 9100/9200/9300/9400/9500/9600 LTE数据接收 登录sdtp服务器执行如下命令停止sdtp：1supervisorctl stop all 2.在hcontrol界面启动kafka3.选举kafka leader节点123export JAVA_HOME=/usr/jdk64/jdk1.7.0_67cd /cmss/bch/bc1.3.4/kafka/./kafka-preferred-replica-election.sh --zookeeper 10.11.94.38:2181/kafka1 &gt; 1.txt 4.检查kafka采集源有没有恢复分别执行如下命令，看看有没有数据输出1234./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic r_gn_http | more./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic r_lte_s1_mme | more./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic nokia_wsnotify1 | more./kafka-console-consumer.sh --zookeeper 10.11.94.38:2181/kafka1 --topic esb_wsnotify --from-beginnin |more 5.重启启动crossdata并确认ws+mq的拓扑crossdata部署在10.11.94.195与10.11.94.188两个节点上，我们需要先停止10.11.94.188节点上停止crossdata123su - crossdata cd scheduler-1.3.0-SNAPSHOT-dist/bin ./stop_scheduler.sh 10.11.94.195节点，重启crossdata1234su - crossdata cd scheduler-1.3.0-SNAPSHOT-dist/bin ./stop_scheduler.sh./start_scheduler.sh 出现这样的如图所示字样代表启动成功然后需要逐一的检查每个拓扑的状态，确保全部无异常报错，拓扑如下表格 MQ+FTP WS+FTP cmnet_asia nokia_webService cmnet_haohan MRO_common 3rd_all aaa_all data_greenet 检查方法见运维手册 6.检查storm的所有的拓扑，确保所有的拓扑都有数据，如果拓扑有问题，重新提交该拓扑 东信 亚信 csfb-topology BUSI_ENGINE drpc-topology LOCATION_INFORMATION ott-topology ott-topology volte-delay-topology volte-topology 7.启动停止的stdp服务登录sdtp服务器执行如下命令停止sdtp：1supervisorctl start all]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka基本概念]]></title>
    <url>%2F2017%2F08%2F30%2FKafka%E5%9F%BA%E6%9C%AC%E8%AF%A5%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[简介Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消费。Kafka作为一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 支持通过Kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。 Kafka系统角色如图所示，kafka是由如下的组件所组成： Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。一个Broker上可以有一个Topic的多个Partition，每个Partition的Lead随机存在于某一个Broker，这样实现了Topic的读写的负载均衡 topic： 可以理解为一个MQ消息队列的名字 Partition：为了实现扩展性，一个非常大的topic可以分布到多个 broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息 都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体 （多个partition间）的顺序。也就是说，一个topic在集群中可以有多个partition，那么分区的策略是什么？(消息发送到哪个分区上，有两种基本的策略，一是采用Key Hash算法，一是采用Round Robin算法) Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。 Producer ：消息生产者，就是向kafka broker发消息的客户端（Push）。 Consumer ：消息消费者，向kafka broker取消息的客户端(Pull) Consumer Group(CG）：消息系统有两类，一是广播，二是订阅发布。广播是把消息发送给所有的消费者；发布订阅是把消息只发送给一个订阅者。Kafka通过Consumer Group组合实现了这两种机制： 实现一个topic消息广播（发给所有的consumer）和单播（发给任意一个consumer）。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个CG只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还 可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。典型的应用场景是，多个Consumer来读取一个Topic(理想情况下是一个Consumer读取Topic的一个Partition）,那么可以让这些Consumer属于同一个Consumer Group即可实现消息的多Consumer并行处理，原理是Kafka将一个消息发布出去后，ConsumerGroup中的Consumers可以通过Round Robin的方式进行消费(Consumers之间的负载均衡使用Zookeeper来实现) 消息多播的实现：为一个Topic指定多个Consumer Group，每个Consumer Group指定一个Consumer，那么由于消息会发送给所有的Consumer Group，那么所有的Consumer都会消费这个消息 消息单播的实现：为一个Topic指定一个Consumer Group，这个Consumer Group指定多个Consumer，那么由于消息发送给这个Consumer Group时只有一个Consumer消费，这就实现了一个消息只被一个Consumer消费的效果 #总结：Topic、Partition和Replica的关系：]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Yarn-cluster与Yarn-client]]></title>
    <url>%2F2017%2F08%2F29%2FSpark-Yarn-cluster%E4%B8%8EYarn-client%2F</url>
    <content type="text"><![CDATA[摘要在Spark中，有Yarn-Client和Yarn-Cluster两种模式可以运行在Yarn上，通常Yarn-cluster适用于生产环境，而Yarn-client更适用于交互，调试模式，以下是它们的区别 Spark插拨式资源管理Spark支持Yarn,Mesos,Standalone三种集群部署模式，它们的共同点：Master服务(Yarn ResourceManager,Mesos master,Spark standalone)来决定哪些应用可以运行以及在哪什么时候运行，Slave服务(Yarn NodeManger)运行在每个节点上，节点上实际运行着Executor进程，此外还监控着它们的运行状态以及资源的消耗 Spark On Yarn的优势 Spark支持资源动态共享，运行于Yarn的框架都共享一个集中配置好的资源池 可以很方便的利用Yarn的资源调度特性来做分类·，隔离以及优先级控制负载，拥有更灵活的调度策略 Yarn可以自由地选择executor数量 Yarn是唯一支持Spark安全的集群管理器，使用Yarn，Spark可以运行于Kerberized Hadoop之上，在它们进程之间进行安全认证 Yarn-cluster VS Yarn-client当在Spark On Yarn模式下，每个Spark Executor作为一个Yarn container在运行，同时支持多个任务在同一个container中运行，极大地节省了任务的启动时间 Appliaction Master为了更好的理解这两种模式的区别先了解下Yarn的Application Master概念，在Yarn中，每个application都有一个Application Master进程，它是Appliaction启动的第一个容器，它负责从ResourceManager中申请资源，分配资源，同时通知NodeManager来为Application启动container，Application Master避免了需要一个活动的client来维持，启动Applicatin的client可以随时退出，而由Yarn管理的进程继续在集群中运行 Yarn-cluster在Yarn-cluster模式下，driver运行在Appliaction Master上，Appliaction Master进程同时负责驱动Application和从Yarn中申请资源，该进程运行在Yarn container内，所以启动Application Master的client可以立即关闭而不必持续到Application的生命周期，下图是yarn-cluster模式 Yarn-cluster模式下作业执行流程：客户端 根据yarnConf来初始化yarnClient，并启动yarnClient 创建客户端Application，并获取Application的ID，进一步判断集群中的资源是否满足executor和ApplicationMaster申请的资源，如果不满足则抛出IllegalArgumentException； 设置资源、环境变量：其中包括了设置Application的Staging目录、准备本地资源（jar文件、log4j.properties）、设置Application其中的环境变量、创建Container启动的Context等； 设置Application提交的Context，包括设置应用的名字、队列、AM的申请的Container、标记该作业的类型为Spark； 申请Memory，并最终通过yarnClient.submitApplication向ResourceManager提交该Application。当作业提交到YARN上之后，客户端就没事了，甚至在终端关掉那个进程也没事，因为整个作业运行在YARN集群上进行，运行的结果将会保存到HDFS或者日志中。 YARN集群 运行ApplicationMaster的run方法； 设置好相关的环境变量。 创建amClient，并启动； 在Spark UI启动之前设置Spark UI的AmIpFilter； 在startUserClass函数专门启动了一个线程（名称为Driver的线程）来启动用户提交的Application，也就是启动了Driver。在Driver中将会初始化SparkContext； 等待SparkContext初始化完成，最多等待spark.yarn.applicationMaster.waitTries次数（默认为10），如果等待了的次数超过了配置的，程序将会退出；否则用SparkContext初始化yarnAllocator； 当SparkContext、Driver初始化完成的时候，通过amClient向ResourceManager注册ApplicationMaster 分配并启动Executeors。在启动Executeors之前，先要通过yarnAllocator获取到numExecutors个Container，然后在Container中启动Executeors。如果在启动Executeors的过程中失败的次数达到了maxNumExecutorFailures的次数，那么这个Application将失败，将Application Status标明为FAILED，并将关闭SparkContext。其实，启动Executeors是通过ExecutorRunnable实现的，而ExecutorRunnable内部是启动CoarseGrainedExecutorBackend的。 最后，Task将在CoarseGrainedExecutorBackend里面运行，然后运行状况会通过Akka通知CoarseGrainedScheduler，直到作业运行完成。 Yarn-client在Yarn-client中，Application Master仅仅从Yarn中申请资源给Executor，之后client会跟container通信进行作业的调度，下图是Yarn-client模式 Yarn-client模式下作业执行流程： 客户端生成作业信息提交给ResourceManager(RM) RM在本地NodeManager启动container并将Application Master(AM)分配给该NodeManager(NM) NM接收到RM的分配，启动Application Master并初始化作业，此时这个NM就称为Driver Application向RM申请资源，分配资源同时通知其他NodeManager启动相应的Executor Executor向本地启动的Application Master注册汇报并完成相应的任务 下表是Spark Standalone与Spark On Yarn模式下的比较 从深层次的含义讲，yarn-cluster和yarn-client模式的区别其实就是Application Master进程的区别，yarn-cluster模式下，driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。然而yarn-cluster模式不适合运行交互类型的作业。而yarn-client模式下，Application Master仅仅向YARN请求executor，client会和请求的container通信来调度他们工作，也就是说Client不能离开。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[map 和 flatMap]]></title>
    <url>%2F2017%2F08%2F29%2Fmap-%E5%92%8C-flatMap%2F</url>
    <content type="text"><![CDATA[#基本概念map 和 flatMap 这两兄弟转文负责将 “一个数组转化为另外一个数组”。1let testArray = ["test1","test1234","","test56"] Mapmap函数能够被数组调用，它接受一个闭包作为参数，作用于数组中的每个元素，闭包返回一个变换后的元素，接着将所有这些变换后的元素组成一个新的数组。 12345678let anotherArray = testArray.map &#123; (string:String) -&gt; Int? in let length = string.characters.count guard length &gt; 0 else &#123; return nil &#125; return string.characters.count&#125; print(anotherArray) //[Optional(5), Optional(8), nil, Optional(6)] FlatMapflatMap很像map函数，但是它摒弃了那些值为nil的元素。另外一个与map函数不同之处在于：倘若元素值不为nil情况下，flapMap能够将可选类型(optional)转换为非可选类型(non-optionals)。 123456789let anotherArray2 = testArray.flatMap &#123; (string:String) -&gt; Int? in let length = string.characters.count guard length &gt; 0 else &#123; return nil &#125; return string.characters.count&#125; print(anotherArray2) //[5, 8, 6]]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark概念]]></title>
    <url>%2F2017%2F08%2F28%2FSpark%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[什么是SparkApache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架，Spark，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。 Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。 Spark的架构Spark架构采用了分布式计算中的Master-Slave模型。 Master是对应集群中的含有Master进程的节点，Slave是集群中含有Worker进程的节点。 Master作为整个集群的控制器，负责整个集群的正常运行；Worker相当于是计算节点，接收主节点命令与进行状态汇报；Executor负责任务的执行；Client作为用户的客户端负责提交应用，Driver负责控制一个应用的执行，如图1-4所示Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进行控制。 在一个Spark应用的执行过程中，Driver和Worker是两个重要角色。 Driver程序是应用逻辑执行的起点，负责作业的调度，即Task任务的分发，而多个Worker用来管理计算节点和创建Executor并行处理任务。 在执行阶段，Driver会将Task和Task所依赖的file和jar序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进行处理。 Spark的整体流程为：Client提交应用，Master找到一个Worker启动Driver，Driver向Master或者资源管理器申请资源，之后将应用转化为RDD Graph，再由DAGScheduler将RDD Graph转化为Stage的有向无环图提交给TaskScheduler，由TaskScheduler提交任务给Executor执行。 在任务执行的过程中，其他组件协同工作，确保整个应用顺利执行。 下面详细介绍Spark的架构中的基本组件。 ClusterManager：在Standalone模式中即为Master（主节点），控制整个集群，监控Worker。 在YARN模式中为资源管理器。 Worker：从节点，负责控制计算节点，启动Executor或Driver。 在YARN模式中为NodeManager，负责计算节点的控制。 Driver：运行Application的main（）函数并创建SparkContext。 Executor：执行器，在worker node上执行任务的组件、 用于启动线程池运行任务。 每个Application拥有独立的一组Executors。 SparkContext：整个应用的上下文，控制应用的生命周期。 RDD：Spark的基本计算单元，一组RDD可形成执行的有向无环图RDD Graph。 DAG Scheduler：根据作业（Job）构建基于Stage的DAG，并提交Stage给TaskScheduler。 TaskScheduler：将任务（Task）分发给Executor执行。 SparkEnv：线程级别的上下文，存储运行时的重要组件的引用。SparkEnv内创建并包含如下一些重要组件的引用。 MapOutPutTracker：负责Shuffle元信息的存储。 BroadcastManager：负责广播变量的控制与元信息的存储。 BlockManager：负责存储管理、 创建和查找块。 MetricsSystem：监控运行时性能指标信息。 SparkConf：负责存储配置信息。 Spark的运行逻辑如图所示，在Spark应用中，整个执行流程在逻辑上会形成有向无环图（DAG）。Action算子触发之后，将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。 Spark的调度方式与MapReduce有所不同。 Spark根据RDD之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数执行流水线。图中的A、 B、C、 D、 E、 F分别代表不同的RDD，RDD内的方框代表分区。 数据从HDFS输入Spark，形成RDD A和RDD C， RDD A上执行map与flat map操作，转换为RDD B RDD C上执行map操作，转换为RDD D RDD D在执行reduce by key的操作转换为RDD E RDD B和RDD E执行join操作，转换为F，而在B和E连接转化为F的过程中又会执行Shuffle， 最后RDD F通过函数saveAsSequenceFile输出并保存到HDFS中。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop操作手册]]></title>
    <url>%2F2017%2F08%2F21%2Fhadoop%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[HDFS命令用户命令classpath命令说明：打印需要得到Hadoop的jar和所需要的lib包路径。命令使用：hdfs classpath dfs命令说明：在支持Hadoop文件系统上运行文件命令。命令使用：1hdfs dfs [COMMAND [COMMAND_OPTIONS]] appendToFile 描述：把本地文件系统中的单个src或是多个srcs复制到目标文件系统中。并且从stdin中读入并且附加至目标文件系统。使用：1hadoop fs -appendToFile &lt;localsrc&gt;...&lt;dst&gt; 举例：12hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile cat描述：将参数所指示的文件的内容输出到stdout。使用：1hadoop fs -cat URI [URI ...] 举例：1hadoop fs -cat /file1 hdfs://nn2.example.com/file2 2）hadoop fs -cat file:///file3 /user/hadoop/file4 checksum描述：返回文件的校验信息。使用：1hadoop fs -checksum URI 举例：11）hadoop fs -checksum hdfs://nn1.example.com/file1 2）hadoop fs -checksum file:///etc/hosts chgrp描述：改变文件所属的用户组。使用这一命令的用户必须是文件的所属用户，或者是超级用户。使用：1hadoop fs -chgrp [-R] GROUP URI [URI ...] -R 这一操作对整个目录结构递归执行 chmod描述：改变文件的权限。使用这一命令的用户必须是文件的所属用户，或者是超级用户。使用：1hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...] -R，这一操作对整个目录结构递归执行 chown描述：改变文件的所属用户。使用这一命令的用户必须是文件在命令变更之前的所属用户，或者是超级用户。使用：1hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] -R，这一操作对整个目录结构递归执行 copyFromLocal描述：与-put命令相似，但是要限定源文件路径为本地文件系统。使用：1hadoop fs -copyFromLocal &lt;localsrc&gt; URI -f，将覆盖目标，若是此文件已经存在的话。 copyToLocal描述：与-get命令相似，但是要限定目标文件路径为本地文件系统。使用：1hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt; count描述：统计匹配对应路径下的目录数，文件数，字节数（文件大小）。使用： 12345678910hadoop fs -count [-q] [-h] [-v] &lt;paths&gt;``` -count的输出格式为DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME &gt; -q的输出格式为QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME &gt; -h，选项显示可读的格式大小。 &gt; -v，选项显示标题行。 &gt; 举例 1）hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file22）hadoop fs -count -q hdfs://nn1.example.com/file13）hadoop fs -count -q -h hdfs://nn1.example.com/file14）hdfs dfs -count -q -h -v hdfs://nn1.example.com/file1123456### cp描述：文件复制从源到目的地。该命令允许多个来源，但这种情况下，目标必须是一个目录。使用：``` bashhadoop fs -cp [-f] [-p | -p[topax]] URI [URI ...] &lt;dest&gt; -f，将覆盖目标，若是此文件已经存在的话。-p，保留文件属性（timestamps, ownership, permission，ACL，XATTR），如果-p没有arg，则保留（timestamps, ownership, permission) 举例： 12hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir createSnapshot描述：创建一个snapshot。使用： 1hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt; renameSnapshot描述：重命名一个snapshot。使用：1hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt; deleteSnapshot描述：删除一个snapshot。使用：1hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; df描述：显示HDFS可用空间。使用：1hadoop fs -df [-h] URI [URI ...] 1） -h,会将文件大小的数值用方便阅读的形式表示，比如用64.0M 代替67108864。 2）hadoop dfs -df /user/hadoop/dir1du描述：如果参数为目录，显示该目录下所有目录+文件的大小；如果参数为单个文件，则显示文件大小。使用：hadoop fs -du [-s] [-h] URI [URI …]1）-s,指输出所有文件大小的累加和，而不是每个文件的大小。 2）-h,会将文件大小的数值用方便阅读的形式表示，比如用64.0M 代替67108864。举例：1）hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1dus描述：显示文件的大小。这个命令等价于hadoop dfs -du -s。使用：hadoop fs -dus expunge描述：清空回收站。使用：hadoop fs -expungeget描述：将文件或目录从HDFS中的src拷贝到本地文件系统localdst。CRC 校验失败的文件可通过-ignorecrc 选项拷贝。文件和CRC 校验和可以通过-crc 选项拷贝。使用：hadoop fs -get [-ignorecrc] [-crc] 举例：1）hadoop fs -get /user/hadoop/file localfile 2）hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfilegetfacl描述：显示文件和目录的访问控制列表（ACL）。如果一个目录有一个默认的ACL，然后setfacl的也显示默认的ACL。1）-R，列出所有文件及子目录的访问控制列表（ACL）。 2）path，列出文件或目录。使用：hadoop fs -getfacl [-R] 举例：1）hadoop fs -getfacl /file 2）hadoop fs -getfacl -R /dirgetfattr描述：显示一个文件或目录的扩展属性名和值（如果有的话）。使用：hadoop fs -getfattr [-R] -n name | -d [-e en] 1）-R，这一操作对整个目录结构递归执行。 2）–n name,以名字匹配的扩展属性值。 3） -d：以路径匹配。 4）-encode：检索后进行编码，有效的编码为“text”, “hex”和 “base64”，编码为text括在双引号double quotes (“)，编码为“hex”和 “base64”其前缀分别为 0x和0s。 5）path:文件或目录。举例：1）hadoop fs -getfattr -d /file 2）hadoop fs -getfattr -R -n user.myAttr /dirgetmerge描述：取一个源目录和目标文件作为输入，并会连接src的文件到目标本地文件。1）-nl，可以设置为允许在每个文件的末尾添加一个换行符（LF）。使用：hadoop fs -getmerge [-nl] 举例：1）hadoop fs -getmerge -nl /src /opt/output.txt 2）hadoop fs -getmerge -nl /src/file1.txt /src/file2.txt /output.txthelp描述：返回命令的用法介绍。使用：hadoop fs -helpls描述：类似于Linux的ls命令。1）对于一个文件，返回文件状态以如下格式列出：文件权限，副本数，用户ID，组ID，文件大小，最近修改日期，最近修改时间，文件名 2）对于目录，返回目录下的第一层子目录和文件，与Unix 中ls 命令的结果类似；结果以如下状态列出：文件权限，用户ID，组ID，最近修改日期，最近修改时间，文件名。使用：hadoop fs -ls [-d] [-h] [-R] 1） -R，这一操作对整个目录结构递归执行。 2） -d：目录被列为纯文本文件。 3） -h,会将文件大小的数值用方便阅读的形式表示，比如用64.0M 代替67108864。举例：1）hadoop fs -ls /user/hadoop/file1lsr描述：递归版本的ls。类似于命令hadoop fs -ls -R。使用：hadoop fs -lsr mkdir描述：需要的URI路径作为参数，并创建目录。使用：hadoop fs -mkdir [-p] 1）-p, 该命令的行为与Unix 中mkdir -p 的行为十分相似。这一路径上的父目录如果不存在，则创建该父目录。举例：1）hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 2）hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dirmoveFromLocal描述：与put 命令类似，将本地文件或目录localsrc上传到HDFS中的dst路径。但是源文件localsrc 拷贝之后自身被删除。使用：hadoop fs -moveFromLocal moveToLocal描述：“尚未实现”。使用：hadoop fs -moveToLocal [-crc] mv描述：将文件从源路径移动到目标路径（移动之后源文件删除）。目标路径为目录的情况下，源路径可以有多个。跨文件系统的移动（本地到HDFS 或者反过来）是不允许的。使用：hadoop fs -mv URI [URI …] 举例：1）hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 2）hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1put描述：将单个src或是多个srcs从本地文件系统复制到目标文件系统。从stdin中读取输入，然后写入目标文件系统。使用：hadoop fs -put … 举例：1）hadoop fs -put localfile /user/hadoop/hadoopfile 2）hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir 3）hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile 4）hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.rm描述：删除指定的文件，只删除非空目录和文件。使用：hadoop fs -rm [-f] [-r |-R] [-skipTrash] URI [URI …]1）-f，如果这个文件不存在将不会显示诊断信息或修改退出状态以反映一个错误。 2）-R，此操作对整个目录结构递归执行。-r选项等同于-R。 3）-skipTrash，删除指定文件而不是放入垃圾回收站。举例：1）hadoop fs -rm hdfs://nn.example.com/file /user/hadoop/emptydirrmdir描述：删除一个目录。使用：hadoop fs -rmdir [–ignore-fail-on-non-empty] URI [URI …]–ignore-fail-on-non-empty，即使用通配符时，如果目录仍包含的文件不会失败。举例：1）hadoop fs -rmdir /user/hadoop/emptydirrmr描述：递归版本的删除。与命令hadoop fs -rm -r类似。使用：hadoop fs -rmr [-skipTrash] URI [URI …]setfacl描述：设置文件和目录的访问控制列表（ACL）。使用：hadoop fs -setfacl [-R] [-b |-k -m |-x ] |[–set ]1）-b,基本ACL条目全部删除。对于用户，组和其他的条目将保留与权限位的兼容性。 2）-k, 删除默认的ACL。 3）-R,此操作对整个目录结构递归执行。 4）-m，修改ACL。新条目将添加到ACL，并且现有条目将被保留。 5）-x，删除指定的ACL条目。其他ACL条目将被保留。 6）–set, 完全取代ACL，丢弃所有现有条目。该acl_spec必须包括用户，组条目，以及其他与权限位的兼容性。 7）acl_spec，逗号分隔的ACL条目列表。 8）path，文件或目录进行修改。举例：1）hadoop fs -setfacl -m user:hadoop:rw- /file 2）hadoop fs -setfacl -x user:hadoop /file 3）hadoop fs -setfacl -b /file 4）hadoop fs -setfacl -k /dir 5）hadoop fs -setfacl –set user::rw-,user:hadoop:rw-,group::r–,other::r– /file 6）hadoop fs -setfacl -R -m user:hadoop:r-x /dir 7）hadoop fs -setfacl -m default:user:hadoop:r-x /dirsetfattr描述：设置一个文件或目录的扩展属性名和值。使用：hadoop fs -setfattr -n name [-v value] | -x name 1）-b，基本ACL条目全部删除。对于用户，组和其他的条目将保留与权限位的兼容性。 2）-n name,扩展属性名称。 3）-v value：扩展属性值。该值有三种不同的编码方法。如果参数双引号括起来，则该值是引号内的字符串；如果参数是0x或0X前缀，那么它被当作一个十六进制数；如果参数是0或0s开头，那么它被视为一个base64编码。 4）-x name,删除扩展属性。 5）path,文件或目录。举例：1）hadoop fs -setfattr -n user.myAttr -v myValue /file 2）hadoop fs -setfattr -n user.noValue /file 3）hadoop fs -setfattr -x user.myAttr /filesetrep描述：更改文件的副本系数。如果路径为一个目录，则命令递归的修改目录树下的所有文件的副本系数。使用：hadoop fs -setrep [-R] [-w] 1）–w,标志请求命令等待复制完成。这可能潜在地需要很长的时间。 2）-R,标志被接受的向后兼容性。它没有任何效果。举例：hadoop fs -setrep -w 3 /user/hadoop/dir1stat描述：用于返回指定路径的统计信息。使用：hadoop fs -stat [format] …举例：hadoop fs -stat “%F %u:%g %b %y %n” /filetail描述：将文件尾部1KB的内容输出到stdout。使用：hadoop fs -tail [-f] URI-f,随着文件的增长，输出附加数据，和UNIX中一致。举例：hadoop fs -tail pathnametest描述：检查命令，可以检查文件是否存在、文件的大小等。使用：hadoop fs -test -[defsz] URI1）-e，检查文件是否存在，如果存在则返回0。 2）-z，检查文件是否是0字节，如果是则返回0。 3）-d，如果路径是一个目录，则返回1，否则返回0。 4）-f，如果路径是一个文件，返回0。 5）-s，如果路径不为空，则返回0。举例：hadoop fs -test -e filenametext描述：取一个源文件，并把源文件按照text的格式输出。允许的格式为zip和TextRecordInputStream。举例：hadoop fs -text touchz描述：创建0长度的文件。使用：hadoop fs -touchz URI [URI …]举例：hadoop fs -touchz pathnametruncate描述：截断匹配指定的文件模式到指定长度的所有文件。使用：hadoop fs -truncate [-w] 1）-w,标志请求块恢复命令等待完成。举例：1）hadoop fs -truncate 55 /user/hadoop/file1 /user/hadoop/file2 2）hadoop fs -truncate -w 127 hdfs://nn1.example.com/user/hadoop/file1usage描述：返回单个命令的帮助。使用：hadoop fs -usage command(3) fetchdt命令说明：从NameNode获取授权令牌。命令使用：hdfs fetchdt [–webservice ] 参数说明：]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume操作手册]]></title>
    <url>%2F2017%2F08%2F21%2Fflume%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[Flume操作指南主要从设置和配置两个方面说明，若要看Flume各个组件的更详细的配置，可参照Flume官网。 http://flume.apache.org/FlumeUserGuide.html 设置设置一个AgentFlume Agent配置存在本地的配置文件中。它是一个Java Properties文件格式的文本文件。一个配置文件中可以配置一个或多个Agent。配置文件包含了一个Agent中每一个Source、Sink和Channel的属性，以及它们如何连接起来形成数据流。 配置各自的组件每一个组件（Source、Sink和Channel）都有name、type以及跟特性type相关联的其他属性集。例如，一个Avro Source需要一个hostname或者是IP地址以及一个端口号去接收数据。一个Memory Channel的最大队列数（“capacity”），一个HDFS Sink需要知道文件系统的URI，创建文件的路径以及文件循环的频率（“hdfs.rollInternal”）等等。一个组件的所有这些属性都需要被设置在这个Flume Agent所以来的配置文件中。 把各个组件串起来Flume Agent需要知道加载什么样的组件，以及如何将这些组件按照循序串起来形成数据流。这就需要列出Agent中每一个Source、Sink和Channel的名字，然后为每一个Source和Sink指定连接的Channel。例如，一个Agent从一个叫avroWeb的Avro Source通过一个叫做file-channel的File Channel传输Event到一个叫hdfs-cluster1的HDFS Sink中。这个配置文件就需要包含这些组件的名字，并且file-channel作为avroWeb和hdfs-cluster1共享的Channel。 启动一个AgentAgent通过一个叫做flume-ng的shell脚本启动，flume-ng位于Flume分发包的bin目录下。你需要在命令行指定Agent的名字、配置文件目录以及对应的配置文件：1$ bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template 一个简单的例子， 这里我们举一个配置文件例子，描述单节点Flume部署。这个配置文件用户生成Event并且把他们顺序地打印到console上。123456789101112# example.conf: A single-node Flume configuration # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 这个配置文件定义了一个单独的叫做a1的Agent。a1有一个在44444端口上监听数据的Source，一个把Event暂存到Memory的Channel以及一个把Event数据打印到console的Sink。这个配置文件命名了各种各样的组件，然后描述了他们的类型和配置参数。一个给定的配置文件可能定义了多个命名的Agent；当一个给定的Flume进程要被启动时，一个标志会传进去来告诉它到底哪个命名的Agent要被启动。基于这个配置文件，我们可以通过以下方式启动Flume：1$ bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console 注意在一个完整的部署中，我们通常会多包含一个配置：–conf=。目录包含一个叫做flume-env.sh的shell脚本和一个log4j配置文件。这个例子中我们传入了一个Java选项强制Flume把日志记录在console上，并且不使用定制的环境脚本。从另一个shell终端，我们可以telnet 44444端口然后发送一个Event：1$ telnet localhost 44444 Trying 127.0.0.1... Connected to localhost.localdomain (127.0.0.1). Escape character is '^]'. Hello world! &lt;ENTER&gt; OK 之前的Flume启动终端上就会用日志形式输出传入的Event。116/06/19 15:32:19 INFO source.NetcatSource: Source starting 16/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444] 16/06/19 15:32:34 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D Hello world!. &#125; 祝贺你，你已经成功配置并部署了一个Flume Agent！下边的文章更详细的讲述了Agent 的配置。 数据采集Flume支持很多从不同数据源采集数据的机制 RPCFlume分发包中的一个Avro客户端可以通过Avro RPC机制将一个给定的文件发送给Flume Avro Source。1$ bin/flume-ng avro-client -H localhost -p 41414 -F /usr/logs/log.10 上边的命令会发送/usr/logs/log.10的内容发送到监听那个端口的Flume Source上。 执行命令Flume分发包中有一个exec Source可一个执行一个给定的命令，并且消费的输出。一个输出的一行。例如以’\r’或’\n’或两者结尾的文本。注：Flume不支持tail作为一个Source。不过你可以封装tail命令到一个exec Source中从而流化文件。 网络流Flume支持一下的机制去从常用的日志流中读取数据，例如: Avro Thrift Syslog Netcat 配置多Agent流为了使数据跨多个Agent和hop传输，前一个Agent的Sink和当前Agent的Source需要时Avro类型的，并且Sink指向Source的hostname和port。 合并日志收集中一个非常常见的场景是大量的日志生产客户端向少量的和存储系统关联的消费端Agent发送数据。例如，从成百上千个Web服务器收集日志然后发送到十几个写入HDFS集群的Agent。这可以通过配置多个第一层的带有Avro Sink的Agent，并且所Avro Sink都指向一个单的Agent的Avro Source上。第二层Agent的Source合并接收过来的数据到单独的Channel中，然后由Sink消费并发送到目的地。 拆分流（多路输出）Flume支持多路输出Event到一个或多个目的地。可以通过定义一个流多路输出器，它来复制或选择性路由一个Event到一个或多个Channel中。上边的例子显示了一个叫“foo”的Agent的一个Source扇出数据流到三个不同的Channel中。这个扇出可以是复制或者多路输出。在复制流的情况下，每一个Event被发送到三个Channel中。在多路输出的情况下，一个Event根据它的属性和预定义值的匹配情况被传送到可用Channel一个子集中，例如，如果一个Event的属性叫做txnType，并且被设置为“customer”，然后它应该被传送到channel1和channel3中，如果被设置为“vendor”，它应该被传送到channel2中，否则channel3中。这些映射都可以在配置文件中进行配置。 配置前边的文章已经介绍过了，Flume Agent配置是从一个具有分层属性的Java属性文件格式的文件中读取的。 定义数据流要在一个Flume Agent中定义数据流，你需要通过一个Channel将Source和Sink连接起来。你需要列出给定Agent的Source、Sink和Channel。一个Source可以指定多个Channel，但是一个Sink只能指定一个Channel。格式如下：1234# list the sources, sinks and channels for the agent &lt;Agent&gt;.sources = &lt;Source&gt; &lt;Agent&gt;.sinks = &lt;Sink&gt; &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; # set channel for source &lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ... # set channel for sink &lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt; 例如，一个叫做agent_foo的Agent从一个外部的Avro客户端读取数据，然后通过Memory Channel将数据发送到HDFS上。配置文件如下：1234# list the sources, sinks and channels for the agent agent_foo.sources = avro-appserver-src-1 agent_foo.sinks = hdfs-sink-1 agent_foo.channels = mem-channel-1 # set channel for source agent_foo.sources.avro-appserver-src-1.channels = mem-channel-1 # set channel for sink agent_foo.sinks.hdfs-sink-1.channel = mem-channel-1 这个配置将会使Event通过一个叫mem-channel-1的Memory Channel从avro-AppSrv-source流向hdfs-Cluster1-sink。当Agent使用此配置文件启动的时候，它就会实例化这个数据流。 配置各自的组件定义数据流之后，你需要设置每一个Source、Sink和Channel的属性。属性位于每个组件类型配置的层次命名空间下。1234# properties for sources &lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt; # properties for channels &lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt; # properties for sinks &lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt; Flume的每一个组件都需要设置“type”属性，以便理解到底需要的是那种组件对象。每一个Source、Sink和Channel类型都有它自己的属性集。这些属性都需要根据需要设置。就像前边的一个通过mem-channel-1的Memory Channel从avro-AppSrv-source流向hdfs-Cluster1-sink的数据流的例子，下边是这些组件配置的例子：123456789agent_foo.sources = avro-AppSrv-source agent_foo.sinks = hdfs-Cluster1-sink agent_foo.channels = mem-channel-1 # set channel for sources, sinks # properties of avro-AppSrv-source agent_foo.sources.avro-AppSrv-source.type = avro agent_foo.sources.avro-AppSrv-source.bind = localhost agent_foo.sources.avro-AppSrv-source.port = 10000 # properties of mem-channel-1 agent_foo.channels.mem-channel-1.type = memory agent_foo.channels.mem-channel-1.capacity = 1000 agent_foo.channels.mem-channel-1.transactionCapacity = 100 # properties of hdfs-Cluster1-sink agent_foo.sinks.hdfs-Cluster1-sink.type = hdfs agent_foo.sinks.hdfs-Cluster1-sink.hdfs.path = hdfs://namenode/flume/webdata 3.一个Agent中添加多个数据流一个简单的Flume Agent可以包含多个独立的数据流配置。你可以在配置中列出多个Source、Sink和Channel。这些组件能够互联起来形成数据流： 1234# list the sources, sinks and channels for the agent &lt;Agent&gt;.sources = &lt;Source1&gt; &lt;Source2&gt; &lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink2&gt; &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; 之后你就可以连接Source和Sink到它们各自的Channel上，从而形成不同的数据流。例如，如果你需要在一个Agent中设置两个数据流，一个从外部Avro客户端流向外部HDFS，另一个从tail的输出流向Avro Sink，下边就是实现这么个数据流的配置：123456# list the sources, sinks and channels in the agent agent_foo.sources = avro-AppSrv-source1 exec-tail-source2 agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2 agent_foo.channels = mem-channel-1 file-channel-2 # flow #1 configuration agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1 # flow #2 configuration agent_foo.sources.exec-tail-source2.channels = file-channel-2 agent_foo.sinks.avro-forward-sink2.channel = file-channel-2 4.配置一个多Agent数据流为设置一个多层的数据流，你需要在第一跳上使用一个Avro/Thrift Sink指向下一跳的Avro/Thrift Source。这就会使第一个Flume Agent传输Event到下一个Flume Agent。例如，如果你在使用Avro客户端周期性的发送文件（每个Event包含一个文件）到本地Flume Agent，然后本地Agent又把Event传输到另一个和最终存储系统挂载的Flume Agent上。看下边的例子Weblog Agent配置：12345# list sources, sinks and channels in the agent agent_foo.sources = avro-AppSrv-source agent_foo.sinks = avro-forward-sink agent_foo.channels = file-channel # define the flow agent_foo.sources.avro-AppSrv-source.channels = file-channel agent_foo.sinks.avro-forward-sink.channel = file-channel # avro sink properties agent_foo.sources.avro-forward-sink.type = avro agent_foo.sources.avro-forward-sink.hostname = 10.1.1.100 agent_foo.sources.avro-forward-sink.port = 10000 # configure other pieces HDFS Agent配置：1234# list sources, sinks and channels in the agent agent_foo.sources = avro-collection-source agent_foo.sinks = hdfs-sink agent_foo.channels = mem-channel # define the flow agent_foo.sources.avro-collection-source.channels = mem-channel agent_foo.sinks.hdfs-sink.channel = mem-channel # avro sink properties agent_foo.sources.avro-collection-source.type = avro agent_foo.sources.avro-collection-source.bind = 10.1.1.100 agent_foo.sources.avro-collection-source.port = 10000 # configure other pieces 这里我们连接Weblog Agent的avro-forward-sink到HDFS Agent的avro-collection-source。这就会使来自外部appserver Source的Event最终存储到HDFS中。 扇出数据流前边的章节已经讨论过，Flume支持从一个Source扇出数据到多个Channel中。有两种扇出模式：复制（replicating）和多路（multiplexing）。在复制流中，Event被发送到所有配置Channel中。在多路流中，Event被发送到一个特定的子集。为扇出数据流，你需要为这个Source指定一个Channel列表以及扇出的策略。这是通过添加一个Channel “selector”（可以是复制或者多路）实现的。然后如果多路的模式，还需进一步指定选择规则。如果不指定“selector”，默认是复制。123456# List the sources, sinks and channels for the agent &lt;Agent&gt;.sources = &lt;Source1&gt; &lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink2&gt; &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; # set list of channels for source (separated by space) &lt;Agent&gt;.sources.&lt;Source1&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; # set channel for sinks &lt;Agent&gt;.sinks.&lt;Sink1&gt;.channel = &lt;Channel1&gt; &lt;Agent&gt;.sinks.&lt;Sink2&gt;.channel = &lt;Channel2&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = replicating 多路选择还有一些其他的配置来使数据流分叉。这就需要指定一个Event属性到一个Channel集合的映射。选择器会检查Event Header上配置的每一个属性。如果它匹配了指定的值，那么这个Event就会被发送到跟这个值映射的所有Channel上。如果没有任何匹配，这个Event就会被发送到发送到默认配置的Channel集合上：12# Mapping for multiplexing selector &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.type = multiplexing &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.header = &lt;someHeader&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value1&gt; = &lt;Channel1&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value2&gt; = &lt;Channel1&gt; &lt;Channel2&gt; &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.mapping.&lt;Value3&gt; = &lt;Channel2&gt; #... &lt;Agent&gt;.sources.&lt;Source1&gt;.selector.default = &lt;Channel2&gt; 为每个值的映射的Channel集合允许重叠。下边的例子是一个单一流，它多路输出到两个路径上。名叫agent_foo的Agent有一个单一的Avro Source和两个Channel连接到两个Sink上：123456789# list the sources, sinks and channels in the agent agent_foo.sources = avro-AppSrv-source1 agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2 agent_foo.channels = mem-channel-1 file-channel-2 # set channels for source agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 file-channel-2 # set channel for sinks agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1 agent_foo.sinks.avro-forward-sink2.channel = file-channel-2 # channel selector configuration agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing agent_foo.sources.avro-AppSrv-source1.selector.header = State agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1 agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1 多路选择器检查Event中一个叫做“State”的Header属性。如果它的值是“CA”，这个Event就会被发送到mem-channel-1，如果它是“AZ”，这个Event就会被发送到file-channel-2，或者如果它是“NY”，那么这个Event就会被发送到这两个Channel上。如果“State” Header属性没有设置或者没有匹配上以上3个的任何一个，这个Event就被发送到mem-channel-1上，它是默认的。多路选择器也支持可选的Channel。为了为Header指定可选的Channel，“optional”配置参数需要像下边的方式一样使用：1234567# channel selector configuration agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing agent_foo.sources.avro-AppSrv-source1.selector.header = State agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1 agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.optional.CA = mem-channel-1 file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2 agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1 多路选择器将会首先尝试向必需的Channel上写入，即使这些必需的Channel中有任何一个没有成功消费Event，这整个事务将会失败。事务将会在所有的必需的Channel上重试，一旦多有必需的Channel都成功的消费了Event，多路选择器才会尝试向可选的Channel上写入。并且可选的Channel中有任何消费Event失败的，Flume也会简单忽略它并且不会重试。如果一个特殊Header的可选Channel集合和必选Channel集合有重叠的，那么这些Channel就被认为是必选的，那自然在这些Channel的失败会导致所有Channel的重试。例如，上边的例子中，“CA” Header对应的mem-channel-1就被认为是必选的Channel，尽管它同时被标记为必选和可选的，对这个Channel的写入失败将会导致跟这个选择器关联的所有Channel上重试。注意如果一个Header并没有配置任何必选的Channel，那么这个Event将会被写入默认的Channel上，并且将会尝试写入到跟这个Header关联的可选Channel上。如果指定了可选的Channel，而没有指定必选的Channel，依然会导致Event被发送到默认的Channel上。如果没有Channel被指定为默认的并且也没有必选的，选择器会尝试将Event写入到可选的Channel中。这种情况下，任何的失败都会被简单忽略并且不在重试。]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase权限控制]]></title>
    <url>%2F2017%2F08%2F21%2Fhbase%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[工作原理HBase 权限控制（简称ACL）,权限控制可对用户组以及用户进行授权，HBase 权限控制通过 AccessController 协处理器实现。 权限分类HBase 权限分为五种：READ(R),WRITE(R),EXECUTE(X),CREATE(C),ADMIN(A)。 R:READ权限，读取给定范围的数据，如scan,get等 W:WRITE权限，如put，delete等 X:EXECUTE, 如执行协处理器endpoint C:CREATE权限, 可以建表，删除表，BULKLOAD 需要CREATE权限 A：Admin 权限,可以执行集群操作，如Merge，split,balance,snapshot,assign/unassign/offline,move,list_procedures,abort_procedure 需要注意的是：desc/list/flush /split/disable/enable 需要CREATE或者ADMIN权限listnamespace/getnamespace 需要ADMIN权限 权限范围HBase 可对Global，namespace,table,ColumnFamily,Cell 级别进行权限控制。 Global，全局级别，表示所有命名空间，所有表。 Namespace，命名空间级别权限，可对某命名空间进行权限控制。 Table，表级别权限，可对某张表进行权限控制 ColumnFamily，列簇级别权限，可对某个表中的某个列簇进行权限控制 Cell ，Cell 级别权限，可对某个表的某个列簇中的某个Cell进行权限控制。 权限配置配置如下：1234567891011121314151617181920&lt;property&gt;&lt;name&gt;hbase.superuser&lt;/name&gt;&lt;value&gt;hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.security.authorization&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint,org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt; 配置好如上属性，重启HBase 生效。注意如上是针对权限的配置，如果存在其它协处理器，多个协处理器之间使用逗号分隔。 hbase权限命令HBase 权限控制命令有grant，revoke，user_permissiongrant 授予权限，revoke 收回权限，user_permission 查看权限 授予权限grant命令用于授予权限grant &lt;user&gt;, &lt;permissions&gt; [, &lt;@namespace&gt; [, &lt;table&gt; [, &lt;column family&gt; [, &lt;column qualifier&gt;]]]用户和用户组授权语法一直，不同的是用户组使用前缀@，namespace和table语法相同，不同的是namespace需要使用前缀@示例：12345hbase&gt; grant 'user1', 'RWXCA' //授予user1用户 READ,WRITE,EXECUTE,CREATE,ADMIN权限,global级别。hbase&gt; grant '@hadoop', 'RWXCA' //授予hadoop用户组READ,WRITE,EXECUTE,CREATE,ADMIN权限,global级别。hbase&gt; grant 'user1', 'RWXCA', '@ns1' //授予命名空间ns1的READ,WRITE,EXECUTE,CREATE,ADMIN权限给user1用户,namespace级别hbase&gt; grant 'user1', 'RW', 't1', 'F' //授予表t1列簇F READ,WRITE权限给user1用户，列簇级别hbase&gt; grant 'user1', 'RW', 'ns1:t1', 'F', 'col1' //授予命名空间ns1下表t1列簇F，column为col1 READ,WRITE权限给user1用户，Cell级别 收回权限revoke 命令用于收回权限，语法与grant 类似，语法如下revoke &lt;user&gt; [, &lt;@namespace&gt; [, &lt;table&gt; [, &lt;column family&gt; [, &lt;column qualifier&gt;]]]]示例：12345hbase&gt; revoke 'user1'hbase&gt; revoke '@hadoop'hbase&gt; revoke 'user1', '@ns1'hbase&gt; revoke 'user1', 't1', 'F', 'col1'hbase&gt; revoke 'user1', 'ns1:t1', 'F', 'col1' 查看权限user_permission 命令用于查看权限，语法如下：user_permission &lt;table&gt;注：namespace请使用@前缀示例：123456hbase&gt; user_permission //查看所有权限列表hbase&gt; user_permission '@ns1' //查看命名空间ns1权限hbase&gt; user_permission 'table1' //查看命名空间table1权限hbase&gt; user_permission 'namespace1:table1' //查看namespace1中的table1权限hbase&gt; user_permission '.*' //支持正则表达是查看权限hbase&gt; user_permission '^[A-C].*' //支持正则表达是查看]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[klist命令]]></title>
    <url>%2F2017%2F08%2F17%2Fklist%2F</url>
    <content type="text"><![CDATA[用途显示 Kerberos 凭证高速缓存或密钥表的内容。 语法klist [[ -c] [ -f] [ -e] [ -s] [ -a] [ -n]] [ -k [ -t] [ -K]] [ name] 描述klist 命令显示 Kerberos 凭证高速缓存或密钥表的内容。 选项 描述 -a 显示所有在凭证高速缓存中的票据，包括过期的票据。如果不指定该标志，不列出过期的票据。仅当列出凭证高速缓存时该标志有效。 -c 列出凭证高速缓存中票据。如果 -c 或 -k 标志都不指定，这个是缺省的。该标志和 -k 标志是互斥的。 -e 显示为会话密钥和票据的加密类型。 -f 用以下缩写显示票据的标志 name 指定凭证高速缓存或密钥表的名称。如果不指定一个文件名则用缺省的凭证高速缓存或密钥表。如果不指定表示高速缓存名称的文件名或密钥表名，klist 显示在缺省凭证高速缓存或密钥表文件的凭证。如果设置了 KRB5CCNAME 环境变量，它的值就用来命名缺省凭证（票据）高速缓存。 -k 列出在密钥表中的条目。该标志和 -c 标志互斥。 -K 为每个密钥表条目显示加密密钥值。仅当列出一个密钥表时该标志有效。 -n 显示数字因特网地址而不是主机名。没有 -n 的缺省情况是显示主机名。该命令与 -a 标志连用。 -s 禁止命令输出但是如果一个有效的票据授权票据（ticket-granting ticket）在凭证高速缓存中被发现，那么设置退出状态为 0。仅当列出凭证高速缓存时该标志有效。 -t 为密钥表条目显示时间戳。仅当列出一个密钥表时该标志有效。 示例 要列出在缺省凭证高速缓存中的所有条目，请输入：bash klist 要列出在 etc/krb5/my_keytab 密钥表中所有条目还有时间戳，请输入:klist -t -k etc/krb5/my_keytab]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
        <tag>klist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kinit命令]]></title>
    <url>%2F2017%2F08%2F17%2Fkinit%2F</url>
    <content type="text"><![CDATA[用途获得或更新 Kerberos 票据授权票据（ticket-granting ticket）。 语法1kinit [ -l lifetime ] [ -r renewable_life ] [ -f ] [ -p ] [ -A ] [ -s start_time ] [ -S target_service ] [ -k [ -t keytab_file ] ] [ -R ] [ -v ] [ -u ] [ -c cachename ] [ principal ] 描述kinit 命令获得或更新 Kerberos 票据授权票据。如果不在命令行上指定一个票据标志，那么使用由在 Kerberos 配置文件（kdc.conf ）中的 [kdcdefault] 和 [realms] 指定的密钥分发中心（KDC）选项.如果不更新一个存在的票据，该命令重新初始化凭证高速缓存并将包含从 KDC 接受的新的票据授权票据。如果不在命令行上指定 Principal 名并且指定 -s 标志，Principal 名从凭证高速缓存中获取。新的凭证高速缓存成为缺省的高速缓存，除非用 -c 标志指定高速缓存的名称。-l、-r 和 -s 标志的票据 Time 值被表达为 ndnhnmns 其中： n 代表一个数字 d 代表天 h 代表小时 m 代表分钟 s 代表秒,必须以这种顺序指定各个部分，但可省略任何部分，例如 4h5m 代表 4 小时 5 分钟，1d2s 代表 1 天 2 秒。 标志描述 选项 描述 -A 指定这个票据包含一个客户机地址的列表。如果不指定这个选项，这个票据将包含本地主机地址。当一个初始票据包含一个地址列表时，它仅可从地址列表中的一个地址中使用。 -c cachename 指定要用的凭证高速缓存的名称。如果该标志没被指定，应用缺省凭证高速缓存。如果 KRB5CCNAME 环境变量被设置，它的值被用来命名缺省票据高速缓存。高速缓存的任何存在的内容可由 kinit 破坏 -f 指定票据是可转发的。为转发票据，该标志必须被指定。 -k 指定从密钥表获得票据主体的密钥。如果不指定该标志，将提示您为票据主体输入密码。 -l lifetime 指定票据结束时间间隔。在到期失效后，不能再用此票据，除非票据被更新。这个间隔缺省时间是 10 小时。 -p 指定这个票据是可代理的。要使票据可代理，该标志必须被指定。principal 指定票据的主体。如果主体不在命令行中指定，那么主体从凭证高速缓存获得。 -r renewable_life 为可更新的票据指定更新时间间隔。在间隔到期后，票据不能被更新。更新时间必须大于结束时间。如果该标志不指定，那么这个票据是不可更新的，尽管如果请求的票据的生命期超出最大票据生命期仍能生成一个可更新的票据。 -R 指定更新一个存在的票据。当更新一个存在的票据时,可能没指定其他标志。 -s start_time 为一个迟后的票据指定一个请求，从 start_time 开始有效。 -S target_service 当得到初始票据时指定一个备用服务名来用。 -t keytab_file 指定密钥表名。如果没指定该标志并且 -k 标志被指定，用缺省的密钥表。-t 标志意味着 -k 标志。 -v 指定在高速缓存中的票据授权票据应被传到 kdc 来确认。如果票据在它的请求的时间范围内，高速缓存用确认过的票据替换。 -u 指定 kinit 命令创建进程的唯一凭证高速缓存文件。如果 kinit 命令成功，那么凭证高速缓存文件名将包含一个唯一编号（进程认证组或 PAG）。在 AIX® V5.3 和更高版本中，将从操作系统服务生成 PAG。KRB5CCNAME 环境变量设置为此凭证高速缓存文件，而 kinit 命令执行新的 shell。 示例 要获得一个生命期为 10 小时五天内可更新的票据授权票据，请输入：kinit -l 10h -r 5d my_principal 要更新一个存在票据，请输入：kinit -R]]></content>
      <categories>
        <category>Kerberos</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
        <tag>kinit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrips]]></title>
    <url>%2F2017%2F08%2F17%2Fscrips%2F</url>
    <content type="text"><![CDATA[1.123456789101112#! /bin/bashDAY=`/bin/date "+%Y%m%d%H" -d "-1 hours"`echo `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/cpu.logls /home/bcstream/monitor-storm/monitor*.log.$DAY* | xargs cat &gt; file.log a=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|head -1` b=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|tail -1` c=`cat file.log |sort -n|wc -l` d=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|awk '&#123;a+=$1&#125;'END'&#123;print a&#125;'`echo "max cpu_usage:"$b &gt;&gt; /root/statistics/cpu.logecho "min cpu_usage:"$a &gt;&gt; /root/statistics/cpu.logecho "average cpu_usage is:" `awk 'BEGIN&#123;printf "%.2f\n", '$d'/'$c'&#125;'` &gt;&gt; /root/statistics/cpu.logrm -rf file.log 2.12345678910111213141516#! /bin/bashDAY=`/bin/date "+%Y%m%d%H" -d "-1 hours"`echo `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/cpu.logls /home/bcstream/monitor-storm/monitor*.log.$DAY* | xargs cat &gt; file.log a=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|head -1` b=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|sort -n|tail -1` c=`cat file.log |sort -n|wc -l` d=`cat file.log |awk -F [\|] '&#123;print $2&#125;'|awk '&#123;a+=$1&#125;'END'&#123;print a&#125;'`echo "max cpu_usage:"$b &gt;&gt; /root/statistics/cpu.logecho "min cpu_usage:"$a &gt;&gt; /root/statistics/cpu.logecho "average cpu_usage is:" `awk 'BEGIN&#123;printf "%.2f\n", '$d'/'$c'&#125;'` &gt;&gt; /root/statistics/cpu.logrm -rf file.log[root@YP-TYHJ-APOLLO4200-5031 statistics]# cat kafka.sh#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/kafka.logcat /home/bcstream/srv/logs/writer/writer.log.`/bin/date "+%Y-%m-%d-%H"`-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;' &gt;&gt; /root/statistics/kafka.log 3.12345678910111213#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/kafka.logcat /home/bcstream/srv/logs/writer/writer.log.`/bin/date "+%Y-%m-%d-%H"`-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;' &gt;&gt; /root/statistics/kafka.log[root@YP-TYHJ-APOLLO4200-5031 statistics]# cat storm.sh#!/bin/bashecho `/bin/date "+%Y-%m-%d-%H"-00-00` &gt;&gt; /root/statistics/storm.logDAY=`/bin/date "+%Y-%m-%d-%H"`COUNT=`cat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |wc -l`for((i=1;i&lt;=$COUNT;i++)); do cat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |awk -F [\:] '&#123;print $2&#125;' | awk -F [\-] '&#123;print $3&#125;' |sed -n "$i p" &gt;&gt; /root/statistics/storm.logcat /home/bcstream/srv/logs/writer/writer.log.$DAY-00.complete |grep BC-02-03-012-01 |awk -F [\;] '&#123;print $4&#125;' |sed -n "$i p" &gt;&gt; /root/statistics/storm.logdone 4.12345#!/bin/bashLOG_DIR=/home/bcstream/monitorLOG_RETAIN_DAY=1find $LOG_DIR -type f -mtime +$LOG_RETAIN_DAY -name "*.complete" -exec rm &#123;&#125; \; &gt;/dev/null 5.12345678910#! /bin/bashDAY=$(date +%Y%m%d%H)HOST=$(hostname --fqdn)LOG_DIR=/home/bcstream/monitorif [ ! -d $LOG_DIR ]; then mkdir $LOG_DIRfiLOG=$LOG_DIR/monitor_$HOST.log.$DAYcpuUsage=`sar -C 1 1|grep "Average" | awk '&#123;print $3&#125;'`echo "$(date +"%Y%m%d %H:%M")|$cpuUsage"&gt;&gt; $LOG 6.12345678910111213141516171819#/bin/bashLOG_DIR=/home/bcstream/monitorCOORDINATE_HOST=10.11.94.52COORDINATE_DIR=/home/bcstream/monitor-stormpasswd=Tyhj@1118for file in `ls $LOG_DIR | grep -v complete`do echo $file expect -c " spawn scp $LOG_DIR/$file bcstream@$COORDINATE_HOST:$COORDINATE_DIR expect &#123; \"assword\" &#123;set timeout 300; send \"$passwd\r\";&#125; \"yes/no\" &#123;send \"yes\r\"; exp_continue;&#125; &#125;expect eof"mv $LOG_DIR/$file $LOG_DIR/$file.completedone 7.12345#! /bin/basha=`cat /home/bcstream/srv/logs/writer/writer.log.2017-08-17-14-00.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;'`b=`cat /home/bcstream/srv/logs/writer/writer.log.2017-08-17-14-10.complete |grep BC-02-04-009-01 |awk -F [\;] '&#123;print $4&#125;' | awk '&#123;sum+=$1&#125;END&#123;print sum&#125;'`c=$(($b-$a))echo "Kafka statistics:"$c]]></content>
      <categories>
        <category>Bash</category>
      </categories>
      <tags>
        <tag>Bash</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Dynamic Resource Allocation 动态资源分配]]></title>
    <url>%2F2017%2F08%2F16%2FSpark-Dynamic-Resource-Allocation%2F</url>
    <content type="text"><![CDATA[http://lxw1234.com/archives/2015/12/593.htm Spark中，所谓资源单位一般指的是executors，和Yarn中的Containers一样，在Spark On Yarn模式下，通常使用–num-executors来指定Application使用的executors数量，而–executor-memory和–executor-cores分别用来指定每个executor所使用的内存和虚拟CPU核数。相信很多朋友至今在提交Spark应用程序时候都使用该方式来指定资源。 假设有这样的场景，如果使用Hive，多个用户同时使用hive-cli做数据开发和分析，只有当用户提交执行了Hive SQL时候，才会向YARN申请资源，执行任务，如果不提交执行，无非就是停留在Hive-cli命令行，也就是个JVM而已，并不会浪费YARN的资源。现在想用Spark-SQL代替Hive来做数据开发和分析，也是多用户同时使用，如果按照之前的方式，以yarn-client模式运行spark-sql命令行，在启动时候指定–num-executors 10，那么每个用户启动时候都使用了10个YARN的资源（Container），这10个资源就会一直被占用着，只有当用户退出spark-sql命令行时才会释放。 spark-sql On Yarn，能不能像Hive一样，执行SQL的时候才去申请资源，不执行的时候就释放掉资源呢，其实从Spark1.2之后，对于On Yarn模式，已经支持动态资源分配（Dynamic Resource Allocation），这样，就可以根据Application的负载（Task情况），动态的增加和减少executors，这种策略非常适合在YARN上使用spark-sql做数据开发和分析，以及将spark-sql作为长服务来使用的场景。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop FairScheduler介绍]]></title>
    <url>%2F2017%2F08%2F15%2FHadoop-FairScheduler%2F</url>
    <content type="text"><![CDATA[翻译自：https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/FairScheduler.html 介绍在一个公司内部的Hadoop Yarn集群，肯定会被多个业务、多个用户同时使用，共享Yarn的资源，如果不做资源的管理与规划，那么整个Yarn的资源很容易被某一个用户提交的Application占满，其它任务只能等待，这种当然很不合理，我们希望每个业务都有属于自己的特定资源来运行MapReduce任务，Hadoop中提供的公平调度器–Fair Scheduler，就可以满足这种需求。Fair Scheduler将整个Yarn的可用资源划分成多个资源池，每个资源池中可以配置最小和最大的可用资源（内存和CPU）、最大可同时运行Application数量、权重、以及可以提交和管理Application的用户等。 FairScheduler是一个资源分配方式，在整个时间线上，所有的applications平均的获取资源。Hadoop NextGen能够调度多种类型的资源。默认情况下，FairScheduler只是对内存资源做公平的调度(分配)。当集群中只有一个application运行时，那么此application占用整个集群资源。当其他的applications提交后，那些释放的资源将会被分配给新的applications，所以每个applicaiton最终都能获取几乎一样多的资源。不像Hadoop默认的Scheduler(CapacityScheduler)，CapacityScheduler将applications以queue的方式组成，它可以让short applications在何时的时间内完成，而不会starving那些长期运行的applications，它也是一个合理的方式在多个用户之间共享集群。最终，Fair共享也可以与application priorities一起工作—–“priorities”作为权重来使用，以决定每个application需要获取资源的量。 Scheduler将applications以queues的方式组织，在这些queues之间公平的共享资源。默认，所有的users共享一个queue，名称为“default”。如果application在请求资源时指定了queue，那么请求将会被提交到指定的queue中；仍然可以通过配置，根据请求中包含的user名称来分配queue。在每个queue内部，调度策略是在运行中的applicaitons之间共享资源。默认是基于内存的公平共享，不过FIFO和multi-resource with Dominant Resource Fairness也能够配置。Queues可以分级来划分资源，配置权重以特定的比例共享集群资源。 FairScheduler允许为queues分配担保性的最小的共享资源量，这对保证某些用户、groups或者applications总能获取充足的资源是有帮助的。当一个queue中包含applications时，它至少能够获取最小量的共享资源，当queue不在需要时，那些过剩的资源将会被拆分给其他的运行中的application。这就让Scheduler在有效利用资源是，保证了queue的capacity。 为了了解queue之间如何共享资源，设想两个用户A和B，各自有自己的queue（图4-4）。A开启了一个job，由于B没有请求，所以它获得了所有的资源。然后在A的job还在运行时，B开启了一个job，一段时间后，每一个job使用了一半的资源，以我们早些时候看到的方式。现在，如果B开启了第二个job，它会同B的其它job一起分享资源，所以B的每一个job占有四分之一个资源，而A继续占有一半的资源。结果是资源的用户之间平分。 FairScheudler在默认情况下允许所有的application运行，但是这也可以通过配置文件来限制每个用户下和每个queue下运行applications的个数。这对限制一个用户一次提交大量applications是有用的，或者通过限制running applications个数来提升性能，因为大量的running applicaiton会导致创建大量的中间数据或者过多的上下文切换。限制applications不会导致随后的提交失败，只是在Scheduler queue中等待，直到先前的application结束。 Hierarchical queuesFairScheduler支持分层的queues。所有的queues继承自“root” queue。有效的资源在root子节点中，以典型的公平调度的方式分布；子节点再将分配给自己的资源以相同的方式分配给自己的子节点。applications只能在queue的叶子节点上调度。可以通过FairScheduler相关的配置文件，Queues可以被指定作为其他queues的子节点。 Queue的名字，以其父节点path作为开始，以“.”作为分割符。比如名称为“queue1”的queue作为root的子节点，那么应该表示为“root.queue1”，名称为“queue2”的queue为“parent1”的子节点，那么应该表示为“root.parent1.queue2”。当指明一个queue时，root部分是可选的，比如“queue1”就是指queue1，而queue2指“parent1.queue2”。 Installation为了使用FairScheduler，首先需要在yarn-site.xml配置： 1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt; 配置定制FairScheduler涉及到2个文件。首先，scheduler有关的选项可以在yarn-site.xml中配置。此外，多数情况，用户需要创建一个“allocation”文件来列举存在的queues和它们相应的weights和capacities。这个“allocation”文件每隔10秒钟加载一次，更新的配置可以更快的生效。 yarn-site.xml中的配置 yarn.scheduler.fair.allocation.file： “allocation”文件的位置，“allocation”文件是一个用来描述queue以及它们的属性的配置文件。这个文件必须为格式严格的xml文件。如果为相对路径，那么将会在classpath下查找此文件(conf目录下)。默认值为“fair-scheduler.xml”。 yarn.scheduler.fair.user-as-default-queue：是否将与allocation有关的username作为默认的queue name，当queue name没有指定的时候。如果设置成false(且没有指定queue name) 或者没有设定，所有的jobs将共享“default” queue。默认值为true。 yarn.scheduler.fair.preemption：是否使用“preemption”(优先权，抢占)，默认为fasle，在此版本中此功能为测试性的。 yarn.scheduler.fair.assignmultiple：是在允许在一个心跳中，发送多个container分配信息。默认值为false。 yarn.scheduler.fair.max.assign：如果assignmultuple为true，那么在一次心跳中，最多发送分配container的个数。默认为-1，无限制。 yarn.scheduler.fair.locality.threshold.node：一个float值，在0~1之间，表示在等待获取满足node-local条件的containers时，最多放弃不满足node-local的container的机会次数，放弃的nodes个数为集群的大小的比例。默认值为-1.0表示不放弃任何调度的机会。 yarn.scheduler.fair.locality.threashod.rack：同上，满足rack-local。 yarn.scheduler.fair.sizebaseweight：是否根据application的大小(job的个数)作为权重。默认为false，如果为true，那么复杂的application将获取更多的资源。 分配文件格式分配文件必须是XML格式。这个格式包含5类元素： 队列元素：表示队列。队列元素可以选择一个可选属性”type”，当设置为’parent’可以将其设置为父队列。这个当我们想创建一个父队列而不配置任何子队列时是有用的。每个队列元素可能包含下列属性： minResources:队列最小资源所有权，是以”X mb,Y vcores”形式。对于单一资源公平策略，vcores值被忽略。如果一个队列的最小共享不满足，则将在同一个父之前的任何其他队列中提供可用的资源。在单一资源公平策略下，如果一个队列的内存使用率低于其最小内存共享，则被认为是不满足的。在主导资源公平下，一个队列如果它的使用占主导地位的资源相对于集群容量是低于其最低份额的，那么其没认为是不满意的。如果在这个情况下，多个队列是不满足的，资源到队列是相关资源实例用和最小值中间最小的比例。注意这是可能的，一个队列低于其最小值，可能不会立即获得其最小值当其提交一个应用程序时，因为已经运行的作业可能会使用到这些资源。 maxResources：队列被允许的最大资源，以”X mb,Y vcores”的形式。对于单一资源的公平策略，Vcores值被忽略。如果其总使用量超过这个限制，一个队列将永远不会被分配到一个容器。 maxRunningApps：限制同时从队列中运行的应用程序数目。 maxAMShare：限制可用于运行应用程序主程序的队列的公平份额的分数。此属性只能用于子队列。例如，如果设置为1.0f，然后AMs在子队列可以达到100%的内存和CPU的公平分享。对-1.0f价值将禁用此功能，amShare不会被检查。默认值是0.5f weight：非比例的于其他队列共享集群，权重默认值是1，并且一个权重是2的队列应得到作为一个队列权重越2倍多的资源。 schedulingPolicy：为任何队列设置调度策略。这个允许的值是”fifo”,”fair”,”drf”或任何org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy.延伸类。默认是”fair”。FIFO：先按照优先级高低调度，如果优先级相同，则按照提交时间先后顺序调度，如果提交时间相同，则按照（队列或者应用程序）名称大小（字符串比较）调度；FAIR：按照内存资源使用量比率调度，即按照used_memory/minShare大小调度（核心思想是按照该调度算法决定调度顺序，但还需考虑一些边界情况）；DRF：借鉴了Mesos中的设计策略，按照主资源公平调度算法进行调度，具体已经在Apache Mesos调度器机制进行了介绍。 aclSubmitApps：可以向队列中提交应用程序的用户和/或组的列表。 aclAdministerApps：可以管理队列的用户和/或组的列表。目前唯一的能管理的动作是杀死一个应用程序。 minSharePreemptionTimeout：从其他队列试图强占容器资源之前，在最小共享下的秒数。如果没设置，队列将继承其父队列的值。 fairSharePreemptionTimeout：从其他队列试图强占容器资源之前，在公平共享阈值下的秒数。如果没设置，队列将继承其父队列的值。 fairSharePreemptionThreshold：队列的公平共享优先权阈值。如果队列等待fairSharePreemptionTimeout没有接受到fairSharePreemptionThreshold* fairShare相应的资源，其可以从其他队列强占容器资源。如果没设置，队列将继承其父队列的值。 用户元素:它代表了管理个人用户行为的设置。它们可以包含一个单一的属性：maxrunningapps，对于特定用户运行应用程序的一个限制。 userMaxAppsDefault元素：对于未指定其他指定的任何用户，设置默认运行应用程序限制。 defaultFairSharePreemption Timeout 元素：对root队列设置公平共享强占超时时间，被root队列中的fairSharePreemptionTimeout 元素重写 defaultMinSharePreemptionTimeout 元素：对root队列设置最小共享强占超时时间，被root队列中defaultMinSharePreemptionTimeout元素重写 defaultFairSharePreemptionThreshold 元素：对root队列设置最小共享强占阈值，被root队列中的defaultFairSharePreemptionThreshold元素重写。 queueMaxAppsDefault 元素：为队列设置默认运行应用程序限制;被每个队列中的maxRunningApps元素重写。 queueMaxAMShareDefault 元素：为队列设置默认AM资源；被每个队列的maxAMShare 元素重写。 defaultQueueSchedulingPolicy 元素：为队列设置默认的调度策略;如果被指定的话，被每个队列中的schedulingPolicy元素重写。默认是fair queuePlacementPolicy元素：其中包含一个规则元素列表，告诉调度如何放置传入的应用程序到队列。规则按照它们被列出的顺序被应用。规则可以获取参数。所有的规则接受”create”参数，这表示规则是够能创建一个新队列。”create”默认是true，如果设置为false并且该规则应该放置应用程序到队列，但是没有在分配文件中配置，我们会继续到下一个规则。最后一个规则必须是一个可能永远不发出继续的规则。有效的规则是： specified：该应用程序配放置到其去修的队列。如果应用程序需求无队列，例如，其指定”default”，我们继续。如果应用程序被需求一个队列名开始或结束是””.””，例如，命名例如”.q1”或”q1.”,将被拒绝。 user:应用程序被放置到名称为提交的用户的队列中。用户名的”.”将被”dot“替换，例如对于用户”first.last”的用户是”first_dot_last” primaryGroup:应用程序被放置到一个名称为提交用户的主组的队列中，组名中的”.”被替换为”dot“，例如组名为”one.two”是”one_dot_two” secondaryGroupExistingQueue: 应用程序被放置到一个名称为提交用户的匹配第二组名的队列中。第一个被匹配的第二组名将被选中。组名中的”.”将被替换为”dot”，例如一个用户的第二组名为”one.two”将被置换为“one_dot_two”，如果这个队列存在的话。 nestedUserQueue：嵌套规则。 default:该应用程序被放置在默认规则的属性queue指定的队列中。如果queue为设置，那么应用程序被放置到’root.default’队列。 reject:应用程序被拒绝 Example一个公平调度的例子：12345678910111213141516171819202122&lt;allocations&gt; &lt;queue name="sample_queue"&gt; &lt;minResources&gt;10000 mb,0vcores&lt;/minResources&gt; &lt;maxResources&gt;90000 mb,0vcores&lt;/maxResources&gt; &lt;maxRunningApps&gt;50&lt;/maxRunningApps&gt; &lt;weight&gt;2.0&lt;/weight&gt; &lt;schedulingPolicy&gt;fair&lt;/schedulingPolicy&gt; &lt;queue name="sample_sub_queue"&gt; &lt;aclSubmitApps&gt;charlie&lt;/aclSubmitApps&gt; &lt;minResources&gt;5000 mb,0vcores&lt;/minResources&gt; &lt;/queue&gt; &lt;/queue&gt; &lt;user name="sample_user"&gt; &lt;maxRunningApps&gt;30&lt;/maxRunningApps&gt; &lt;/user&gt; &lt;userMaxAppsDefault&gt;5&lt;/userMaxAppsDefault&gt; &lt;queuePlacementPolicy&gt; &lt;rule name="specified" /&gt; &lt;rule name="primaryGroup" create="false" /&gt; &lt;rule name="default" /&gt; &lt;/queuePlacementPolicy&gt;&lt;/allocations&gt; 通过Web UI 监控当前应用程序，队列，和公平共享可以通过资源管理器的web接口来检查，在http://*ResourceManager URL*/cluster/scheduler。在web界面上的每个队列都可以看到下列字段： userd Resources – 队列中分配给容器的资源总和。 Num Active Applications – 已接收至少一个容器的队列中的应用程序的数量。 Num Pending Applications –尚未接收任何容器的队列中的应用程序的数量。 Min Resources – 保证队列最小资源的配置。 Max Resources – 已被允许的队列的配置的最大资源。 Instantaneous Fair Share –队列的瞬时公平共享资源。这些共享只考虑活动的队列（那些运行的应用程序），和用于的调度策略。当其他队列没有使用它们时，队列可能在它们的共享之外分配资源。一个队列的资源消耗出来或低于其公平份额，而不会有瞬时容器争抢。 Steady Fair Share – 队列资源的稳定的公共共享。这些共享考虑所有的队列，不论它们是否是积极的（有运行应用程序）。当配置和容量变化时，这些都被计算较少的变更。它们的意思是提供可视性，在用户期望的资源上，并因此显示在WEB 界面上。]]></content>
      <categories>
        <category>Hadoop</category>
        <category>Yarn</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume详解]]></title>
    <url>%2F2017%2F08%2F11%2Fflume%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[什么是flumeflume 作为 cloudera 开发的实时日志收集系统，受到了业界的认可与广泛应用。Flume 初始的发行版本目前被统称为 Flume OG（original generation），属于 cloudera。但随着 FLume 功能的扩展，Flume OG 代码工程臃肿、核心组件设计不合理、核心配置不标准等缺点暴露出来，尤其是在 Flume OG 的最后一个发行版本 0.94.0 中，日志传输不稳定的现象尤为严重，为了解决这些问题，2011 年 10 月 22 号，cloudera 完成了 Flume-728，对 Flume 进行了里程碑式的改动：重构核心组件、核心配置以及代码架构，重构后的版本统称为 Flume NG（next generation）；改动的另一原因是将 Flume 纳入 apache 旗下，cloudera Flume 改名为 Apache Flume。官方用户手册：http://flume.apache.org/FlumeUserGuide.html flume的特点flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。支持在日志系统中定制各类数据发送方，用于收集数据;同时，Flume提供对数据进行简单处理，并写到各种数据接受方(比如文本、HDFS、Hbase等)的能力 。flume的数据流由事件(Event)贯穿始终。事件是Flume的基本数据单位，它携带日志数据(字节数组形式)并且携带有头信息，这些Event由Agent外部的Source生成，当Source捕获事件后会进行特定的格式化，然后Source会把事件推入(单个或多个)Channel中。你可以把Channel看作是一个缓冲区，它将保存事件直到Sink处理完该事件。Sink负责持久化日志或者把事件推向另一个Source。flume的可靠性当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。flume的可恢复性还是靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。 flume核心概念 Agent：使用JVM运行Flume。每台机器运行一个agent，但是可以在一个agent中包含多个sources和sinks。 Client：生产数据，运行在一个独立的线程。 Sources：从Client收集数据，传递给Channel。 Sink：从Channel收集数据，运行在一个独立线程。 Channel：连接 sources 和 sinks ，这个有点像一个队列。 Events：可以是日志记录、 avro 对象等。 flume 案例案例1：Avrovro可以发送一个给定的文件给Flume，Avro 源使用AVRO RPC机制 创建agent配置文件123456789101112131415161718192021a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 4141# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Describe the sinka1.sinks.k1.type = logger# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 创建指定文件1echo "hello world" &gt; /home/flume/logs/log.00 使用avro-client发送文件1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 15:11:16,718 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 hello world &#125; 案例2：SpoolSpool监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点： 拷贝到spool目录下的文件不可以再打开编辑。 spool目录下不可包含相应的子目录 创建agent配置文件123456789101112131415161718192021a2.sources = r1a2.sinks = k1a2.channels = c1#Describe/configure the sourcea2.sources.r1.type = spooldira2.sources.r1.channels = c1a2.sources.r1.spoolDir = /home/flume/logsa2.sources.r1.fileHeader = true#Describe the sinka2.sinks.k1.type = logger#Use a channel which buffers events in memorya2.channels.c1.type = memorya2.channels.c1.capacity = 1000a2.channels.c1.transactionCapacity = 100#Bind the source and sink to the channela2.sources.r1.channels = c1a2.sinks.k1.channel = c1 启动flume agent1flume-ng avro-client -c . -H kiwi02.novalocal -p 4141 -F /home/flume/logs/log.00 追加文件到/home/flume/logs目录1echo "spool test1" &gt; /home/flume/logs/1.txt 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 12345608 Aug 2017 16:18:46,736 INFO [lifecycleSupervisor-1-0] (org.apache.flume.source.SpoolDirectorySource.start:78) - SpoolDirectorySource source starting with directory: /home/flume/logs08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:120) - Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.08 Aug 2017 16:18:46,764 INFO [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:96) - Component type: SOURCE, name: r1 started08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.readEvents:258) - Last read took us just up to a file boundary. Rolling to the next file, if there is one.08 Aug 2017 16:20:40,596 INFO [pool-3-thread-1] (org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile:348) - Preparing to move file /home/flume/logs/2.txt to /home/flume/logs/2.txt.COMPLETED08 Aug 2017 16:20:44,747 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;file=/home/flume/logs/2.txt&#125; body: 73 70 6F 6F 6C 20 74 65 73 74 31 spool test1 &#125; 案例3：ExecEXEC执行一个给定的命令获得输出的源,如果要使用tail命令，必选使得file足够大才能看到输出内容 创建agent配置文件1234567891011121314151617181920a3.sources = r1a3.sinks = k1a3.channels = c1# Describe/configure the sourcea3.sources.r1.type = execa3.sources.r1.channels = c1a3.sources.r1.command = tail -F /var/log/date.log# Describe the sinka3.sinks.k1.type = logger# Use a channel which buffers events in memorya3.channels.c1.type = memorya3.channels.c1.capacity = 1000a3.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela3.sources.r1.channels = c1a3.sinks.k1.channel = c1 创建业务监控的日志文件1234567[root@kiwi02 ~]# cat 1.sh #!/bin/bashwhile true; do echo `/bin/date` &gt;&gt; /var/log/date.log sleep 1done 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 123456708 Aug 2017 16:46:58,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 36 3A Tue Aug 8 16:46: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:01,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,422 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125;08 Aug 2017 16:47:04,423 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;&#125; body: 54 75 65 20 41 75 67 20 38 20 31 36 3A 34 37 3A Tue Aug 8 16:47: &#125; 案例4：SyslogtcpSyslogtcp监听TCP的端口做为数据源 创建agent配置文件123456789101112131415161718192021a4.sources = r1a4.sinks = k1a4.channels = c1# Describe/configure the sourcea4.sources.r1.type = syslogtcpa4.sources.r1.port = 5140a4.sources.r1.host = localhosta4.sources.r1.channels = c1# Describe the sinka4.sinks.k1.type = logger# Use a channel which buffers events in memorya4.channels.c1.type = memorya4.channels.c1.capacity = 1000a4.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela4.sources.r1.channels = c1a4.sinks.k1.channel = c1 往5140端口发送信息1[root@kiwi02 ~]# echo "hello idoall.org syslog" | nc localhost 5140 在kiwi02.novalocal节点的日志文件中，可以看到以下信息： 108 Aug 2017 21:53:13,584 INFO [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.LoggerSink.process:94) - Event: &#123; headers:&#123;Severity=0, flume.syslog.status=Invalid, Facility=0&#125; body: 68 65 6C 6C 6F 20 69 64 6F 61 6C 6C 2E 6F 72 67 hello idoall.org &#125;]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop2使用实例]]></title>
    <url>%2F2017%2F08%2F11%2Fsqoop2%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Sqoop是一款开源的工具，主要用于在Hadoop和传统的数据库(MySQL、postgresql等)进行数据的传递，可以将一个关系型数据库（例如：mysql、Oracle、Postgres等）中的数据导进到hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop中一大亮点就是可以通过hadoop的mapreduce把数据从关系型数据库中导入数据到HDFS。Sqoop目前版本已经到了1.99.7，我们可以在其官网上看到所有的版本，Sqoop1.99.7是属于sqoop2，Sqoop1的最高版本为1.4.6sqoop2的官方文档：http://sqoop.apache.org/docs/1.99.7/index.html sqoop2的使用过程使用 show connector 查看sqoop的所有连接123456789sqoop:000&gt; show connector+----+------------------------+----------------+------------------------------------------------------+----------------------+| Id | Name | Version | Class | Supported Directions |+----+------------------------+----------------+------------------------------------------------------+----------------------+| 1 | kite-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kite.KiteConnector | FROM/TO || 2 | kafka-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.kafka.KafkaConnector | TO || 3 | hdfs-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.hdfs.HdfsConnector | FROM/TO || 4 | generic-jdbc-connector | 1.99.6-bc1.3.2 | org.apache.sqoop.connector.jdbc.GenericJdbcConnector | FROM/TO |+----+------------------------+----------------+------------------------------------------------------+----------------------+ 在向hdfs导入导出数据时，需要依赖以上四个连接创建link （在1.99.4版本之后 用户不需要再创建连接） 查看当前的所有link12345sqoop:000&gt; show link +----+------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------+--------------+----------------+---------++----+------+--------------+----------------+---------+ 查看当前的所有job12345sqoop:000&gt; show job+----+------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+------+----------------+--------------+---------++----+------+----------------+--------------+---------+ 创建link创建hdfs连接（根据connector选择一个合适的连接方式） 12345678sqoop:000&gt; create link -cid 3Creating link for connector with id 3Please fill following values to create new link objectName: hdfs_linkLink configurationHDFS URI: hdfs://kiwi01.novalocal:8020Hadoop conf directory: /cmss/bch/bc1.3.4/hadoop/etc/hadoopNew link was successfully created with validation status OK and persistent id 2 查看创建的连接 123456sqoop:000&gt; show link+----+-----------+--------------+----------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+-----------+--------------+----------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true |+----+-----------+--------------+----------------+---------+ 创建mysql的link 123456789101112131415161718sqoop:000&gt; create link -cid 4Creating link for connector with id 4Please fill following values to create new link objectName: mysql_linkLink configurationJDBC Driver Class: com.mysql.jdbc.DriverJDBC Connection String: jdbc:mysql://hcontrol1341/kiwiUsername: kiwiPassword: ****JDBC Connection Properties: There are currently 0 values in the map:entry# protocol=tcpThere are currently 1 values in the map:protocol = tcpentry# New link was successfully created with validation status OK and persistent id 5 查看创建的link 1234567sqoop:000&gt; show link+----+------------+--------------+------------------------+---------+| Id | Name | Connector Id | Connector Name | Enabled |+----+------------+--------------+------------------------+---------+| 2 | hdfs_link | 3 | hdfs-connector | true || 5 | mysql_link | 4 | generic-jdbc-connector | true |+----+------------+--------------+------------------------+---------+ 创建job123456789101112131415161718192021222324252627282930313233343536373839404142434445464748sqoop:000&gt; create job -f 5 -t 2Creating job for links with from id 5 and to id 2Please fill following values to create new job objectName: mysql_to_hdfsFrom database configurationSchema name: employeesTable name: dept_managerTable SQL statement: Table column names: Partition column name: Null value allowed for the partition column: Boundary query: Incremental readCheck column: Last value: To HDFS configurationOverride null value: Null value: Output format: 0 : TEXT_FILE 1 : SEQUENCE_FILEChoose: 0Compression format: 0 : NONE 1 : DEFAULT 2 : DEFLATE 3 : GZIP 4 : BZIP2 5 : LZO 6 : LZ4 7 : SNAPPY 8 : CUSTOMChoose: 0Custom compression format: Output directory: hdfs://kiwi01.novalocal:8020/sqoopAppend mode: Throttling resourcesExtractors: 2Loaders: 2New job was successfully created with validation status OK and persistent id 1 查看job的状态 123456sqoop:000&gt; show job+----+---------------+----------------+--------------+---------+| Id | Name | From Connector | To Connector | Enabled |+----+---------------+----------------+--------------+---------+| 1 | mysql_to_hdfs | 4 | 3 | true |+----+---------------+----------------+--------------+---------+ 启动job12345678910sqoop:000&gt; start job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:55:46 CST: BOOTING - Progress is not available 查看job的状态12345678910sqoop:000&gt; status job -j 1Submission detailsJob ID: 1Server URL: http://localhost:12000/sqoop/Created by: sqoop2Creation date: 2017-08-03 18:55:46 CSTLastly updated by: sqoop2External ID: job_1501745270549_0001 http://kiwi01.novalocal:8088/proxy/application_1501745270549_0001/2017-08-03 18:56:06 CST: RUNNING - 0.00 % End]]></content>
      <categories>
        <category>Sqoop2</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
        <tag>sqoop2</tag>
      </tags>
  </entry>
</search>
